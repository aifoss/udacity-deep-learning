{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language Translation RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def load_data(self, path):\n",
    "        \"\"\"\n",
    "        Load dataset from file.\n",
    "        \"\"\"\n",
    "        input_file = os.path.join(path)\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataLoader = DataLoader()\n",
    "\n",
    "source_text = dataLoader.load_data(source_path)\n",
    "target_text = dataLoader.load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataExplorer:\n",
    "    \n",
    "    def explore_data(self, source_text, target_text, sent_range):\n",
    "        \"\"\"\n",
    "        Explore sample sentences from dataset.\n",
    "        \"\"\"\n",
    "        print('Dataset Stats')\n",
    "        print('Roughly the number of unique words: {}'\\\n",
    "              .format(len({word: None for word in source_text.split()})))\n",
    "\n",
    "        sentences = source_text.split('\\n')\n",
    "        word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "        \n",
    "        print('Number of sentences: {}'.format(len(sentences)))\n",
    "        print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "        print()\n",
    "        print('English sentences {} to {}:'.format(*sent_range))\n",
    "        print('\\n'.join(source_text.split('\\n')[sent_range[0]:sent_range[1]]))\n",
    "        print()\n",
    "        print('French sentences {} to {}:'.format(*sent_range))\n",
    "        print('\\n'.join(target_text.split('\\n')[sent_range[0]:sent_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "view_sentence_range = (0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 227\n",
      "Number of sentences: 137861\n",
      "Average number of words in a sentence: 13.225277634719028\n",
      "\n",
      "English sentences 0 to 10:\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "the united states is usually chilly during july , and it is usually freezing in november .\n",
      "california is usually quiet during march , and it is usually hot in june .\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "your least liked fruit is the grape , but my least liked is the apple .\n",
      "his favorite fruit is the orange , but my favorite is the grape .\n",
      "paris is relaxing during december , but it is usually chilly in july .\n",
      "new jersey is busy during spring , and it is never hot in march .\n",
      "our least liked fruit is the lemon , but my least liked is the grape .\n",
      "the united states is sometimes busy during january , and it is sometimes warm in november .\n",
      "\n",
      "French sentences 0 to 10:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "son fruit préféré est l'orange , mais mon préféré est le raisin .\n",
      "paris est relaxant en décembre , mais il est généralement froid en juillet .\n",
      "new jersey est occupé au printemps , et il est jamais chaude en mars .\n",
      "notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n",
      "les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n"
     ]
    }
   ],
   "source": [
    "dataExplorer = DataExplorer()\n",
    "\n",
    "dataExplorer.explore_data(source_text, target_text, view_sentence_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \n",
    "    def preprocess_and_save_data(self, source_text, target_text):\n",
    "        \"\"\"\n",
    "        Preprocess text data and save to file.\n",
    "        \"\"\"\n",
    "        source_text = source_text.lower()\n",
    "        target_text = target_text.lower()\n",
    "\n",
    "        source_vocab_to_int, source_int_to_vocab = self.create_lookup_tables(source_text)\n",
    "        target_vocab_to_int, target_int_to_vocab = self.create_lookup_tables(target_text)\n",
    "\n",
    "        source_id_text, target_id_text = self.text_to_ids(source_text, \n",
    "                                                          target_text, \n",
    "                                                          source_vocab_to_int, \n",
    "                                                          target_vocab_to_int)\n",
    "\n",
    "        PickleHelper().save_preprocessed_data(((source_text, target_text),\n",
    "                                               (source_id_text, target_id_text),\n",
    "                                               (source_vocab_to_int, target_vocab_to_int),\n",
    "                                               (source_int_to_vocab, target_int_to_vocab)))\n",
    "                                              \n",
    "                                              \n",
    "    def create_lookup_tables(self, text):\n",
    "        \"\"\"\n",
    "        Create lookup tables for vocabulary.\n",
    "        \"\"\"\n",
    "        CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3}\n",
    "        \n",
    "        vocab = set(text.split())\n",
    "        vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "        for v_i, v in enumerate(vocab, len(CODES)):\n",
    "            vocab_to_int[v] = v_i\n",
    "\n",
    "        int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "        return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "    \n",
    "    def text_to_ids(self, source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "        \"\"\"\n",
    "        Convert source and target text to proper word ids.\n",
    "\n",
    "        :param source_text: String that contains all the source text.\n",
    "        :param target_text: String that contains all the target text.\n",
    "        :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "        :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "        :return: A tuple of lists (source_id_text, target_id_text)\n",
    "        \"\"\"\n",
    "        source_id_text = [[source_vocab_to_int[word] for word in line.split(' ') if word != ''] \\\n",
    "                          for line in source_text.split('\\n')]\n",
    "        \n",
    "        target_id_text = [[target_vocab_to_int[word] for word in line.split(' ') if word != ''] \\\n",
    "                              + [target_vocab_to_int['<EOS>']] \\\n",
    "                          for line in target_text.split('\\n')]\n",
    "\n",
    "        return (source_id_text, target_id_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PickleHelper:\n",
    "    \n",
    "    def save_preprocessed_data(self, data):\n",
    "        \"\"\"\n",
    "        Save preprocessed training data.\n",
    "        \"\"\"\n",
    "        pickle.dump(data, open('preprocess.p', 'wb'))\n",
    "        \n",
    "    def load_preprocessed_data(self):\n",
    "        \"\"\"\n",
    "        Load the Preprocessed training data and return them in batches of <batch_size> or less.\n",
    "        \"\"\"\n",
    "        return pickle.load(open('preprocess.p', mode='rb'))\n",
    "    \n",
    "    def save_params(self, params):\n",
    "        \"\"\"\n",
    "        Save parameters to file.\n",
    "        \"\"\"\n",
    "        pickle.dump(params, open('params.p', 'wb'))\n",
    "    \n",
    "    def load_params(self):\n",
    "        \"\"\"\n",
    "        Load parameters from file.\n",
    "        \"\"\"\n",
    "        return pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPreprocessor = DataPreprocessor()\n",
    "\n",
    "dataPreprocessor.preprocess_and_save_data(source_text, target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickleHelper = PickleHelper()\n",
    "\n",
    "((source_text, target_text),\n",
    " (source_int_text, target_int_text), \n",
    " (source_vocab_to_int, target_vocab_to_int),\n",
    " (source_int_to_vocab, target_int_to_vocab)) = pickleHelper.load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking TensorFlow Version and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorFlowGPUChecker:\n",
    "    \n",
    "    def check(self):\n",
    "        # Check TensorFlow Version\n",
    "        assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), \\\n",
    "            'Please use TensorFlow version 1.1 or newer'\n",
    "        print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "        # Check for a GPU\n",
    "        if not tf.test.gpu_device_name():\n",
    "            warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "        else:\n",
    "            print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "versionChecker = TensorFlowGPUChecker()\n",
    "\n",
    "versionChecker.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqRNN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.inputs = None\n",
    "        self.targets = None\n",
    "        \n",
    "        self.learning_rate = None\n",
    "        self.keep_prob = None\n",
    "        \n",
    "        self.source_seq_len = None\n",
    "        self.target_seq_len = None\n",
    "        self.max_target_len = None\n",
    "        \n",
    "        self.training_logits = None\n",
    "        self.inference_logits = None\n",
    "        \n",
    "        self.cost = None\n",
    "        self.train_op = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNBuilder:\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        \"\"\"\n",
    "        Create TF Placeholders for input, targets, learning rate, \n",
    "        and lengths of source and target sequences.\n",
    "\n",
    "        :return: Tuple (inputs, targets, learning_rate, keep_prob,\n",
    "                        source_seq_len, target_seq_len, max_target_len)\n",
    "        \"\"\"\n",
    "        inputs = tf.placeholder(tf.int32, (None, None), name='input')\n",
    "        targets = tf.placeholder(tf.int32, (None, None), name='targets')\n",
    "\n",
    "        learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        source_seq_len = tf.placeholder(tf.float32, (None,), name='source_seq_len')\n",
    "        target_seq_len = tf.placeholder(tf.int32, (None,), name='target_seq_len')\n",
    "        max_target_len = tf.reduce_max(target_seq_len, name='max_target_len')\n",
    "        \n",
    "        return (inputs, targets, learning_rate, keep_prob, \\\n",
    "                source_seq_len, target_seq_len, max_target_len)\n",
    "\n",
    "    \n",
    "    def build_encoding_layer(self, \n",
    "                             rnn_inputs, \n",
    "                             rnn_size, \n",
    "                             num_layers, \n",
    "                             keep_probability, \n",
    "                             source_seq_len,\n",
    "                             source_vocab_size,\n",
    "                             enc_embedding_size):\n",
    "        \"\"\"\n",
    "        Create encoding layer.\n",
    "\n",
    "        :param rnn_inputs: Inputs for the RNN\n",
    "        :param rnn_size: RNN Size\n",
    "        :param num_layers: Number of layers\n",
    "        :param keep_prob: Dropout keep probability\n",
    "        :param source_seq_len: List of the lengths of each sequence in the batch\n",
    "        :param source_vocab_size: Vocabulary size of source data\n",
    "        :param enc_embedding_size: Embedding size of source data\n",
    "        :return: Tuple (enc_output, enc_state)\n",
    "        \"\"\"\n",
    "        # Encodder embedding\n",
    "        enc_embed = tf.contrib.layers.embed_sequence(rnn_inputs,\n",
    "                                                     source_vocab_size,\n",
    "                                                     enc_embedding_size)\n",
    "\n",
    "        # Encoder cell\n",
    "        def make_cell(rnn_size):\n",
    "            initializer = tf.random_uniform_initializer(-0.1, 0.1, seed=2)\n",
    "            cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, keep_probability)\n",
    "            return cell\n",
    "\n",
    "        enc_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "            [make_cell(rnn_size) for _ in range(num_layers)])\n",
    "\n",
    "        enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, \n",
    "                                                  enc_embed, \n",
    "                                                  source_seq_len,\n",
    "                                                  dtype=tf.float32)\n",
    "\n",
    "        return (enc_output, enc_state)\n",
    "    \n",
    "    \n",
    "    def process_decoder_input(self, targets, target_vocab_to_int, batch_size):\n",
    "        \"\"\"\n",
    "        Preprocess target data for encoding.\n",
    "\n",
    "        :param targets: Target Placehoder\n",
    "        :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "        :param batch_size: Batch Size\n",
    "        :return: Preprocessed target data\n",
    "        \"\"\"\n",
    "        ending = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "        dec_input = tf.concat([tf.fill([batch_size, 1], target_vocab_to_int['<GO>']), \n",
    "                               ending], 1)        \n",
    "        return dec_input\n",
    "    \n",
    "    \n",
    "    def build_decoding_layer(self, \n",
    "                             enc_state,\n",
    "                             dec_input, \n",
    "                             target_seq_len, \n",
    "                             max_target_len,\n",
    "                             rnn_size,\n",
    "                             num_layers,\n",
    "                             target_vocab_to_int,\n",
    "                             target_vocab_size,\n",
    "                             batch_size,\n",
    "                             keep_probability,\n",
    "                             dec_embedding_size):\n",
    "        \"\"\"\n",
    "        Create decoding layer.\n",
    "\n",
    "        :param enc_state: Encoder state\n",
    "        :param dec_input: Decoder input\n",
    "        :param target_seq_len: The lengths of each sequence in the target batch\n",
    "        :param max_target_len: Maximum length of target sequences\n",
    "        :param rnn_size: RNN Size\n",
    "        :param num_layers: Number of layers\n",
    "        :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "        :param target_vocab_size: Size of target vocabulary\n",
    "        :param batch_size: The size of the batch\n",
    "        :param keep_prob: Dropout keep probability\n",
    "        :param dec_embedding_size: Decoding embedding size\n",
    "        :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "        \"\"\"\n",
    "        # Decoder embedding\n",
    "        dec_embedding = tf.Variable(tf.random_uniform([target_vocab_size, dec_embedding_size]))\n",
    "        dec_embed = tf.nn.embedding_lookup(dec_embedding, dec_input)\n",
    "\n",
    "        # Decoder cell\n",
    "        def make_cell(rnn_size):\n",
    "            initializer = tf.random_uniform_initializer(-0.1, 0.1, seed=2)\n",
    "            cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, keep_probability)\n",
    "            return cell\n",
    "\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "\n",
    "        # Dense layer to translate the decoder's output at each time step\n",
    "        # into a chocie from the target vocabulary\n",
    "        initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.1)\n",
    "        output_layer = Dense(target_vocab_size,\n",
    "                             kernel_initializer=initializer)\n",
    "\n",
    "        with tf.variable_scope(\"decode\") as decoding_scope:\n",
    "            \n",
    "            # Training decoder\n",
    "            training_decoder_output = self.get_training_decoding(enc_state, \n",
    "                                                                 dec_cell, \n",
    "                                                                 dec_embed, \n",
    "                                                                 target_seq_len, \n",
    "                                                                 max_target_len, \n",
    "                                                                 output_layer)\n",
    "\n",
    "            decoding_scope.reuse_variables()    \n",
    "            \n",
    "            # Inference decoder\n",
    "            inference_decoder_output = self.get_inference_decoding(enc_state, \n",
    "                                                                   dec_cell, \n",
    "                                                                   dec_embedding, \n",
    "                                                                   target_vocab_to_int['<GO>'],\n",
    "                                                                   target_vocab_to_int['<EOS>'],\n",
    "                                                                   max_target_len,\n",
    "                                                                   target_vocab_size,\n",
    "                                                                   output_layer,\n",
    "                                                                   batch_size)\n",
    "\n",
    "        return (training_decoder_output, inference_decoder_output)\n",
    "\n",
    "    \n",
    "    def get_training_decoding(self,\n",
    "                              enc_state, \n",
    "                              dec_cell, \n",
    "                              dec_embed_input, \n",
    "                              target_seq_len,\n",
    "                              max_target_len,\n",
    "                              output_layer):\n",
    "        \"\"\"\n",
    "        Create a decoding layer for training.\n",
    "\n",
    "        :param enc_state: Encoder state\n",
    "        :param dec_cell: Decoder RNN cell\n",
    "        :param dec_embed_input: Decoder embedded input\n",
    "        :param target_seq_len: The lengths of each sequence in the target batch\n",
    "        :param max_target_len: Maximum length of target sequences\n",
    "        :param output_layer: Function to apply the output layer\n",
    "        :return: BasicDecoderOutput containing training logits and sample_id\n",
    "        \"\"\"\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input,\n",
    "                                                   target_seq_len,\n",
    "                                                   time_major=False)\n",
    "\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, enc_state, output_layer)\n",
    "\n",
    "        return tf.contrib.seq2seq.dynamic_decode(decoder,\n",
    "                                                 impute_finished=True,\n",
    "                                                 maximum_iterations=max_target_len)[0]\n",
    "    \n",
    "    \n",
    "    def get_inference_decoding(self,\n",
    "                               enc_state, \n",
    "                               dec_cell, \n",
    "                               dec_embeddings, \n",
    "                               start_of_seq_id,\n",
    "                               end_of_seq_id,\n",
    "                               max_target_len,\n",
    "                               vocab_size,\n",
    "                               output_layer,\n",
    "                               bacth_size):\n",
    "        \"\"\"\n",
    "        Create a decoding layer for inference.\n",
    "\n",
    "        :param enc_state: Encoder state\n",
    "        :param dec_cell: Decoder RNN cell\n",
    "        :param dec_embeddings: Decoder embeddings\n",
    "        :param start_of_seq_id: <GO> ID\n",
    "        :param end_of_seq_id: <EOS> ID\n",
    "        :param max_target_len: Maximum length of target sequences\n",
    "        :param vocab_size: Size of decoder/target vocabulary\n",
    "        :param output_layer: Function to apply the output layer\n",
    "        :param batch_size: Batch size\n",
    "        :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "        \"\"\"\n",
    "        start_tokens = tf.tile(tf.constant([start_of_seq_id], dtype=tf.int32),\n",
    "                               [batch_size],\n",
    "                               name='start_tokens')\n",
    "\n",
    "        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,\n",
    "                                                          start_tokens,\n",
    "                                                          end_of_seq_id)\n",
    "\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, enc_state, output_layer)\n",
    "\n",
    "        return tf.contrib.seq2seq.dynamic_decode(decoder,\n",
    "                                                 impute_finished=True,\n",
    "                                                 maximum_iterations=max_target_len)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqGraphBuilder:\n",
    "    \n",
    "    def build_train_graph(self,\n",
    "                          batch_size,\n",
    "                          rnn_size,\n",
    "                          num_layers,\n",
    "                          enc_embedding_size,\n",
    "                          dec_embedding_size,\n",
    "                          keep_probability,\n",
    "                          source_vocab_size,\n",
    "                          target_vocab_size,\n",
    "                          target_vocab_to_int):\n",
    "                          \n",
    "        \"\"\"\n",
    "        Build the training graph with the Seq2Seq RNN.\n",
    "\n",
    "        :param batch_size: Batch Size\n",
    "        :param rnn_size: RNN Size\n",
    "        :param num_layers: Number of layers\n",
    "        :param enc_embedding_size: Decoder embedding size\n",
    "        :param dec_embedding_size: Encoder embedding size\n",
    "        :param keep_probability: Dropout keep probability\n",
    "        :param source_vocab_size: Source vocabulary size\n",
    "        :param target_vocab_size: Target vocabulary size\n",
    "        :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "        :return: RNN and training graph\n",
    "        \"\"\"\n",
    "        \n",
    "        train_graph = tf.Graph()\n",
    "        \n",
    "        rnn = Seq2SeqRNN()\n",
    "        rnnBuilder = RNNBuilder()\n",
    "        optimizerTuner = OptimizerTuner()\n",
    "  \n",
    "        with train_graph.as_default():\n",
    "        \n",
    "            # Create placeholders\n",
    "            inputs, targets, lr, keep_prob, source_seq_len, target_seq_len, max_target_len = \\\n",
    "                rnnBuilder.create_placeholders()\n",
    "            \n",
    "            rnn.inputs, rnn.targets = inputs, targets\n",
    "            rnn.learning_rate, rnn.keep_prob = lr, keep_prob\n",
    "            rnn.source_seq_len, rnn.target_seq_len, rnn.max_target_len = \\\n",
    "                source_seq_len, target_seq_len, max_target_len\n",
    "        \n",
    "            # Build encoding layer\n",
    "            enc_output, enc_state = rnnBuilder.build_encoding_layer(tf.reverse(inputs, [-1]),\n",
    "                                                                    rnn_size,\n",
    "                                                                    num_layers,\n",
    "                                                                    keep_probability,\n",
    "                                                                    source_seq_len,\n",
    "                                                                    source_vocab_size,\n",
    "                                                                    enc_embedding_size)\n",
    "            \n",
    "            # Process decoder input\n",
    "            dec_input = rnnBuilder.process_decoder_input(targets, \n",
    "                                                         target_vocab_to_int, \n",
    "                                                         batch_size)             \n",
    "                                     \n",
    "            # Build decoding layer\n",
    "            training_decoder_output, inference_decoder_output = \\\n",
    "                rnnBuilder.build_decoding_layer(enc_state,\n",
    "                                                dec_input, \n",
    "                                                target_seq_len, \n",
    "                                                max_target_len,\n",
    "                                                rnn_size,\n",
    "                                                num_layers,\n",
    "                                                target_vocab_to_int,\n",
    "                                                target_vocab_size,\n",
    "                                                batch_size,\n",
    "                                                keep_probability,\n",
    "                                                dec_embedding_size)\n",
    "                \n",
    "            training_logits = tf.identity(training_decoder_output.rnn_output, name='logits')\n",
    "            inference_logits = tf.identity(inference_decoder_output.sample_id, name='predictions')\n",
    "\n",
    "            rnn.training_logits, rnn.inference_logits = training_logits, inference_logits\n",
    "            \n",
    "            masks = tf.sequence_mask(target_seq_len, max_target_len, \n",
    "                                     dtype=tf.float32, \n",
    "                                     name='masks')\n",
    "            \n",
    "            with tf.name_scope(\"optimization\"):\n",
    "                # Loss function\n",
    "                cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    "                \n",
    "                # Optimizer\n",
    "                optimizer = tf.train.AdamOptimizer(lr)\n",
    "                train_op = optimizerTuner.get_gradient_clipped_optimizer(optimizer, cost)\n",
    "            \n",
    "                rnn.cost, rnn.train_op = cost, train_op\n",
    "            \n",
    "        return (rnn, train_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OptimizerTuner:\n",
    "    \n",
    "    def get_gradient_clipped_optimizer(self, optimizer, cost):\n",
    "        \"\"\"\n",
    "        Apply gradient clipping to optimizer.\n",
    "        \n",
    "        :param optimizer: Optimizer to apply gradient clipping to\n",
    "        :param cost: Loss function\n",
    "        :return: Optimizer with gradient clipping\n",
    "        \"\"\"\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) \\\n",
    "                            for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "rnn_size = 256\n",
    "num_layers = 3\n",
    "encoding_embedding_size = 128\n",
    "decoding_embedding_size = 128\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.75\n",
    "display_step = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_vocab_size = len(source_vocab_to_int)\n",
    "target_vocab_size = len(target_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell object at 0x7f6475180ba8>: The input_size parameter is deprecated.\n",
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell object at 0x7f647e6478d0>: The input_size parameter is deprecated.\n",
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell object at 0x7f6441aac908>: The input_size parameter is deprecated.\n",
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell object at 0x7f6440e28be0>: The input_size parameter is deprecated.\n",
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell object at 0x7f6475180ba8>: The input_size parameter is deprecated.\n",
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell object at 0x7f6440d79dd8>: The input_size parameter is deprecated.\n"
     ]
    }
   ],
   "source": [
    "graphBuilder = Seq2SeqGraphBuilder()\n",
    "\n",
    "rnn, train_graph = graphBuilder.build_train_graph(batch_size,\n",
    "                                                  rnn_size,\n",
    "                                                  num_layers,\n",
    "                                                  encoding_embedding_size,\n",
    "                                                  decoding_embedding_size,\n",
    "                                                  keep_probability,\n",
    "                                                  source_vocab_size,\n",
    "                                                  target_vocab_size,\n",
    "                                                  target_vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \n",
    "    def train_seq2seq_model(self, rnn, train_graph):\n",
    "        \"\"\"\n",
    "        Train and save the Seq2Seq model.\n",
    "        \n",
    "        :param rnn: Seq2Seq RNN model\n",
    "        :param train_graph: TensorFlow graph\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.Session(graph=train_graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            batchGenerator = DataBatchGenerator()\n",
    "            accuracyCalculator = AccuracyCalculator()\n",
    "            \n",
    "            for epoch_i in range(epochs):\n",
    "                batches = batchGenerator.get_batches(batch_size,\n",
    "                                                     train_source,\n",
    "                                                     train_target,\n",
    "                                                     source_pad_int,\n",
    "                                                     target_pad_int)\n",
    "                \n",
    "                for batch_i, (source_batch, target_batch, source_lengths, target_lengths) \\\n",
    "                        in enumerate(batches):\n",
    "                        \n",
    "                    # Training step\n",
    "                    feed = {rnn.inputs: source_batch,\n",
    "                            rnn.targets: target_batch,\n",
    "                            rnn.learning_rate: learning_rate,\n",
    "                            rnn.keep_prob: keep_probability,\n",
    "                            rnn.source_seq_len: source_lengths,\n",
    "                            rnn.target_seq_len: target_lengths}\n",
    "                    \n",
    "                    loss, _ = sess.run([rnn.cost, rnn.train_op],\n",
    "                                       feed_dict=feed)\n",
    "\n",
    "                    if batch_i % display_step == 0 and batch_i > 0:\n",
    "                        \n",
    "                        train_feed = {rnn.inputs: source_batch,\n",
    "                                      rnn.keep_prob: 1.0,\n",
    "                                      rnn.source_seq_len: source_lengths,\n",
    "                                      rnn.target_seq_len: target_lengths}\n",
    "                        train_logits = sess.run(rnn.inference_logits,\n",
    "                                                feed_dict=train_feed)\n",
    "                        \n",
    "                        valid_feed = {rnn.inputs: valid_source_batch,\n",
    "                                      rnn.keep_prob: 1.0,\n",
    "                                      rnn.source_seq_len: valid_source_lengths,\n",
    "                                      rnn.target_seq_len: valid_target_lengths}\n",
    "                        valid_logits = sess.run(rnn.inference_logits,\n",
    "                                                feed_dict=valid_feed)\n",
    "                        \n",
    "                        train_acc = accuracyCalculator.get_accuracy(target_batch, \n",
    "                                                                    train_logits)\n",
    "                        valid_acc = accuracyCalculator.get_accuracy(valid_target_batch,\n",
    "                                                                    valid_logits)\n",
    "                        \n",
    "                        print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\\\n",
    "                              .format(epoch_i+1, batch_i, len(source_int_text)//batch_size, \n",
    "                                      train_acc, valid_acc, loss))\n",
    "             \n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, save_path)\n",
    "        \n",
    "            print('\\nModel Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingValidationSetCreator:\n",
    "    \n",
    "    def create_train_val_sets(self, batch_size, source_letter_ids, target_letter_ids):\n",
    "        \"\"\"\n",
    "        Create training and validation sets.\n",
    "        \n",
    "        :param batch_size: Batch size\n",
    "        :param source_letter_ids: Mapping of source text letters to ints\n",
    "        :param target_letter_ids: Mapping of target text letters to ints\n",
    "        :return Tuple (train_source, train_target, valid_source, valid_target)\n",
    "        \"\"\"\n",
    "        \n",
    "        train_source = source_letter_ids[batch_size:]\n",
    "        train_target = target_letter_ids[batch_size:]\n",
    "        valid_source = source_letter_ids[:batch_size]\n",
    "        valid_target = target_letter_ids[:batch_size]\n",
    "\n",
    "        return (train_source, train_target, valid_source, valid_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValidationSetBatchCreator:\n",
    "    \n",
    "    def get_val_set_batches(self, \n",
    "                            batch_size, \n",
    "                            valid_source, \n",
    "                            valid_target, \n",
    "                            source_pad_int, \n",
    "                            target_pad_int):\n",
    "        \"\"\"\n",
    "        Get batches from validation datasets.\n",
    "\n",
    "        :param batch_size: Batch size\n",
    "        :param valid_source: Validation source dataset\n",
    "        :param valid_target: Validation target dataset\n",
    "        :param source_pad_int: Int ID for <PAD> in source\n",
    "        :param target_pad_int: Int ID for <PAD> in target\n",
    "        :return: Tuple (valid_source_batch, valid_target_batch, \\\n",
    "                        valid_source_lengths, valid_target_lengths)\n",
    "        \"\"\"\n",
    "\n",
    "        dataBatchGenerator = DataBatchGenerator()\n",
    "        \n",
    "        (valid_source_batch, valid_target_batch, \\\n",
    "         valid_source_lengths, valid_target_lengths) = \\\n",
    "            next(dataBatchGenerator.get_batches(batch_size, \n",
    "                                                valid_source, \n",
    "                                                valid_target,\n",
    "                                                source_pad_int,\n",
    "                                                target_pad_int))\n",
    "        \n",
    "        return (valid_source_batch, valid_target_batch, \n",
    "                valid_source_lengths, valid_target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataBatchGenerator:\n",
    "    \n",
    "    def get_batches(self, \n",
    "                    batch_size, \n",
    "                    sources, \n",
    "                    targets, \n",
    "                    source_pad_int, \n",
    "                    target_pad_int):\n",
    "        \"\"\"\n",
    "        Batch targets, sources, and the lengths of their sentences together.\n",
    "        \n",
    "        :param batch_size: Batch size\n",
    "        :param sources: Source dataset\n",
    "        :param targets: Target datasest\n",
    "        :param source_pad_int: Int ID for <PAD> in source\n",
    "        :param target_pad_int: Int ID for <PAD> in target\n",
    "        :return: Batch generator to yield (pad_source_batch, pad_target_batch, \\\n",
    "                                           pad_source_lengths, pad_target_lengths)\n",
    "        \"\"\"\n",
    "        \n",
    "        for batch_i in range(0, len(sources)//batch_size):\n",
    "            start_i = batch_i * batch_size\n",
    "            \n",
    "            source_batch = sources[start_i:start_i + batch_size]\n",
    "            target_batch = targets[start_i:start_i + batch_size]\n",
    "            \n",
    "            pad_source_batch = np.array(\n",
    "                self.pad_sentence_batch(source_batch, source_pad_int))\n",
    "            pad_target_batch = np.array(\n",
    "                self.pad_sentence_batch(target_batch, target_pad_int))\n",
    "\n",
    "            # Need the lengths for the _lengths parameters\n",
    "            pad_source_lengths = []\n",
    "            for source in pad_source_batch:\n",
    "                pad_source_lengths.append(len(source))\n",
    "                \n",
    "            pad_target_lengths = []\n",
    "            for target in pad_target_batch:\n",
    "                pad_target_lengths.append(len(target))\n",
    "\n",
    "            yield pad_source_batch, pad_target_batch, \\\n",
    "                  pad_source_lengths, pad_target_lengths\n",
    "  \n",
    "            \n",
    "    def pad_sentence_batch(self, sentence_batch, pad_int):\n",
    "        \"\"\"\n",
    "        Pad sentences with <PAD> so that each sentence of a batch has the same length.\n",
    "        \n",
    "        :param sentence_batch: Batch of sentences\n",
    "        :param pad_int: Int ID for <PAD>\n",
    "        :return: Batch of sentences padded with <PAD>\n",
    "        \"\"\"\n",
    "        max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "        return [sentence + [pad_int] * (max_sentence - len(sentence)) \\\n",
    "                    for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AccuracyCalculator:\n",
    "    \n",
    "    def get_accuracy(self, target, logits):\n",
    "        \"\"\"\n",
    "        Calculate accuracy.\n",
    "        \n",
    "        :param target: Target batch\n",
    "        :param logits: Logits\n",
    "        \"\"\"\n",
    "        max_seq = max(target.shape[1], logits.shape[1])\n",
    "        \n",
    "        if max_seq - target.shape[1]:\n",
    "            target = np.pad(\n",
    "                target,\n",
    "                [(0,0),(0,max_seq - target.shape[1])],\n",
    "                'constant')\n",
    "        if max_seq - logits.shape[1]:\n",
    "            logits = np.pad(\n",
    "                logits,\n",
    "                [(0,0),(0,max_seq - logits.shape[1])],\n",
    "                'constant')\n",
    "\n",
    "        return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingValidationSetCreator = TrainingValidationSetCreator()\n",
    "\n",
    "train_source, train_target, valid_source, valid_target = \\\n",
    "    trainingValidationSetCreator.create_train_val_sets(batch_size, \n",
    "                                                       source_int_text, \n",
    "                                                       target_int_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_pad_int = source_vocab_to_int['<PAD>']\n",
    "target_pad_int = target_vocab_to_int['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validationSetBatchCreator = ValidationSetBatchCreator()\n",
    "\n",
    "(valid_source_batch, valid_target_batch, \\\n",
    " valid_source_lengths, valid_target_lengths) = \\\n",
    "    validationSetBatchCreator.get_val_set_batches(\n",
    "        batch_size, valid_source, valid_target, source_pad_int, target_pad_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch   20/1077 - Train Accuracy: 0.3574, Validation Accuracy: 0.4137, Loss: 2.9427\n",
      "Epoch   1 Batch   40/1077 - Train Accuracy: 0.4047, Validation Accuracy: 0.4645, Loss: 2.5631\n",
      "Epoch   1 Batch   60/1077 - Train Accuracy: 0.4528, Validation Accuracy: 0.4901, Loss: 2.2917\n",
      "Epoch   1 Batch   80/1077 - Train Accuracy: 0.4273, Validation Accuracy: 0.4933, Loss: 2.1586\n",
      "Epoch   1 Batch  100/1077 - Train Accuracy: 0.4316, Validation Accuracy: 0.4943, Loss: 2.0426\n",
      "Epoch   1 Batch  120/1077 - Train Accuracy: 0.4379, Validation Accuracy: 0.5004, Loss: 1.9353\n",
      "Epoch   1 Batch  140/1077 - Train Accuracy: 0.4169, Validation Accuracy: 0.4904, Loss: 1.9039\n",
      "Epoch   1 Batch  160/1077 - Train Accuracy: 0.4297, Validation Accuracy: 0.4698, Loss: 1.6326\n",
      "Epoch   1 Batch  180/1077 - Train Accuracy: 0.4328, Validation Accuracy: 0.5036, Loss: 1.5546\n",
      "Epoch   1 Batch  200/1077 - Train Accuracy: 0.4605, Validation Accuracy: 0.5185, Loss: 1.5073\n",
      "Epoch   1 Batch  220/1077 - Train Accuracy: 0.4618, Validation Accuracy: 0.5217, Loss: 1.5039\n",
      "Epoch   1 Batch  240/1077 - Train Accuracy: 0.4617, Validation Accuracy: 0.5217, Loss: 1.4100\n",
      "Epoch   1 Batch  260/1077 - Train Accuracy: 0.4632, Validation Accuracy: 0.5153, Loss: 1.3042\n",
      "Epoch   1 Batch  280/1077 - Train Accuracy: 0.4641, Validation Accuracy: 0.5007, Loss: 1.3981\n",
      "Epoch   1 Batch  300/1077 - Train Accuracy: 0.4367, Validation Accuracy: 0.5082, Loss: 1.3095\n",
      "Epoch   1 Batch  320/1077 - Train Accuracy: 0.4898, Validation Accuracy: 0.5263, Loss: 1.2472\n",
      "Epoch   1 Batch  340/1077 - Train Accuracy: 0.4564, Validation Accuracy: 0.5515, Loss: 1.1793\n",
      "Epoch   1 Batch  360/1077 - Train Accuracy: 0.4715, Validation Accuracy: 0.5224, Loss: 1.1023\n",
      "Epoch   1 Batch  380/1077 - Train Accuracy: 0.5180, Validation Accuracy: 0.5376, Loss: 1.0158\n",
      "Epoch   1 Batch  400/1077 - Train Accuracy: 0.5160, Validation Accuracy: 0.5561, Loss: 1.0376\n",
      "Epoch   1 Batch  420/1077 - Train Accuracy: 0.5164, Validation Accuracy: 0.5607, Loss: 0.9453\n",
      "Epoch   1 Batch  440/1077 - Train Accuracy: 0.5109, Validation Accuracy: 0.5614, Loss: 0.9909\n",
      "Epoch   1 Batch  460/1077 - Train Accuracy: 0.4926, Validation Accuracy: 0.5579, Loss: 0.9150\n",
      "Epoch   1 Batch  480/1077 - Train Accuracy: 0.4901, Validation Accuracy: 0.5529, Loss: 0.9047\n",
      "Epoch   1 Batch  500/1077 - Train Accuracy: 0.5547, Validation Accuracy: 0.5661, Loss: 0.8639\n",
      "Epoch   1 Batch  520/1077 - Train Accuracy: 0.5655, Validation Accuracy: 0.5767, Loss: 0.7970\n",
      "Epoch   1 Batch  540/1077 - Train Accuracy: 0.5578, Validation Accuracy: 0.5625, Loss: 0.8067\n",
      "Epoch   1 Batch  560/1077 - Train Accuracy: 0.5492, Validation Accuracy: 0.5938, Loss: 0.7942\n",
      "Epoch   1 Batch  580/1077 - Train Accuracy: 0.6135, Validation Accuracy: 0.5962, Loss: 0.7186\n",
      "Epoch   1 Batch  600/1077 - Train Accuracy: 0.5826, Validation Accuracy: 0.5849, Loss: 0.7327\n",
      "Epoch   1 Batch  620/1077 - Train Accuracy: 0.5453, Validation Accuracy: 0.6005, Loss: 0.7606\n",
      "Epoch   1 Batch  640/1077 - Train Accuracy: 0.5272, Validation Accuracy: 0.5781, Loss: 0.7727\n",
      "Epoch   1 Batch  660/1077 - Train Accuracy: 0.5512, Validation Accuracy: 0.5909, Loss: 0.7659\n",
      "Epoch   1 Batch  680/1077 - Train Accuracy: 0.5692, Validation Accuracy: 0.5930, Loss: 0.7149\n",
      "Epoch   1 Batch  700/1077 - Train Accuracy: 0.5410, Validation Accuracy: 0.5856, Loss: 0.7067\n",
      "Epoch   1 Batch  720/1077 - Train Accuracy: 0.5580, Validation Accuracy: 0.5703, Loss: 0.7734\n",
      "Epoch   1 Batch  740/1077 - Train Accuracy: 0.5855, Validation Accuracy: 0.5827, Loss: 0.6981\n",
      "Epoch   1 Batch  760/1077 - Train Accuracy: 0.5754, Validation Accuracy: 0.5948, Loss: 0.6984\n",
      "Epoch   1 Batch  780/1077 - Train Accuracy: 0.5949, Validation Accuracy: 0.6080, Loss: 0.6998\n",
      "Epoch   1 Batch  800/1077 - Train Accuracy: 0.5500, Validation Accuracy: 0.6055, Loss: 0.6740\n",
      "Epoch   1 Batch  820/1077 - Train Accuracy: 0.5727, Validation Accuracy: 0.5973, Loss: 0.6895\n",
      "Epoch   1 Batch  840/1077 - Train Accuracy: 0.5965, Validation Accuracy: 0.6161, Loss: 0.6278\n",
      "Epoch   1 Batch  860/1077 - Train Accuracy: 0.5751, Validation Accuracy: 0.6143, Loss: 0.6453\n",
      "Epoch   1 Batch  880/1077 - Train Accuracy: 0.6312, Validation Accuracy: 0.6133, Loss: 0.6428\n",
      "Epoch   1 Batch  900/1077 - Train Accuracy: 0.6129, Validation Accuracy: 0.6200, Loss: 0.6460\n",
      "Epoch   1 Batch  920/1077 - Train Accuracy: 0.5980, Validation Accuracy: 0.6214, Loss: 0.6365\n",
      "Epoch   1 Batch  940/1077 - Train Accuracy: 0.5773, Validation Accuracy: 0.6246, Loss: 0.6161\n",
      "Epoch   1 Batch  960/1077 - Train Accuracy: 0.6306, Validation Accuracy: 0.6339, Loss: 0.5950\n",
      "Epoch   1 Batch  980/1077 - Train Accuracy: 0.6289, Validation Accuracy: 0.6293, Loss: 0.6023\n",
      "Epoch   1 Batch 1000/1077 - Train Accuracy: 0.6488, Validation Accuracy: 0.6278, Loss: 0.5474\n",
      "Epoch   1 Batch 1020/1077 - Train Accuracy: 0.5930, Validation Accuracy: 0.6200, Loss: 0.5758\n",
      "Epoch   1 Batch 1040/1077 - Train Accuracy: 0.5950, Validation Accuracy: 0.6211, Loss: 0.6215\n",
      "Epoch   1 Batch 1060/1077 - Train Accuracy: 0.6094, Validation Accuracy: 0.6183, Loss: 0.5785\n",
      "Epoch   2 Batch   20/1077 - Train Accuracy: 0.6039, Validation Accuracy: 0.6403, Loss: 0.5650\n",
      "Epoch   2 Batch   40/1077 - Train Accuracy: 0.6035, Validation Accuracy: 0.6190, Loss: 0.5749\n",
      "Epoch   2 Batch   60/1077 - Train Accuracy: 0.6075, Validation Accuracy: 0.6491, Loss: 0.5495\n",
      "Epoch   2 Batch   80/1077 - Train Accuracy: 0.6109, Validation Accuracy: 0.6328, Loss: 0.5837\n",
      "Epoch   2 Batch  100/1077 - Train Accuracy: 0.6395, Validation Accuracy: 0.6232, Loss: 0.5559\n",
      "Epoch   2 Batch  120/1077 - Train Accuracy: 0.6168, Validation Accuracy: 0.6388, Loss: 0.5700\n",
      "Epoch   2 Batch  140/1077 - Train Accuracy: 0.5880, Validation Accuracy: 0.6371, Loss: 0.5767\n",
      "Epoch   2 Batch  160/1077 - Train Accuracy: 0.6477, Validation Accuracy: 0.6442, Loss: 0.5456\n",
      "Epoch   2 Batch  180/1077 - Train Accuracy: 0.6238, Validation Accuracy: 0.6378, Loss: 0.5400\n",
      "Epoch   2 Batch  200/1077 - Train Accuracy: 0.6199, Validation Accuracy: 0.6328, Loss: 0.5463\n",
      "Epoch   2 Batch  220/1077 - Train Accuracy: 0.6044, Validation Accuracy: 0.6484, Loss: 0.5473\n",
      "Epoch   2 Batch  240/1077 - Train Accuracy: 0.6625, Validation Accuracy: 0.6470, Loss: 0.5112\n",
      "Epoch   2 Batch  260/1077 - Train Accuracy: 0.6164, Validation Accuracy: 0.6357, Loss: 0.5026\n",
      "Epoch   2 Batch  280/1077 - Train Accuracy: 0.6551, Validation Accuracy: 0.6502, Loss: 0.5273\n",
      "Epoch   2 Batch  300/1077 - Train Accuracy: 0.6246, Validation Accuracy: 0.6428, Loss: 0.5299\n",
      "Epoch   2 Batch  320/1077 - Train Accuracy: 0.6180, Validation Accuracy: 0.6417, Loss: 0.5129\n",
      "Epoch   2 Batch  340/1077 - Train Accuracy: 0.6110, Validation Accuracy: 0.6335, Loss: 0.5260\n",
      "Epoch   2 Batch  360/1077 - Train Accuracy: 0.6418, Validation Accuracy: 0.6463, Loss: 0.5120\n",
      "Epoch   2 Batch  380/1077 - Train Accuracy: 0.6484, Validation Accuracy: 0.6179, Loss: 0.4864\n",
      "Epoch   2 Batch  400/1077 - Train Accuracy: 0.6547, Validation Accuracy: 0.6474, Loss: 0.5117\n",
      "Epoch   2 Batch  420/1077 - Train Accuracy: 0.6398, Validation Accuracy: 0.6413, Loss: 0.4708\n",
      "Epoch   2 Batch  440/1077 - Train Accuracy: 0.6211, Validation Accuracy: 0.6449, Loss: 0.5106\n",
      "Epoch   2 Batch  460/1077 - Train Accuracy: 0.6301, Validation Accuracy: 0.6491, Loss: 0.4937\n",
      "Epoch   2 Batch  480/1077 - Train Accuracy: 0.6579, Validation Accuracy: 0.6403, Loss: 0.4837\n",
      "Epoch   2 Batch  500/1077 - Train Accuracy: 0.6590, Validation Accuracy: 0.6403, Loss: 0.4623\n",
      "Epoch   2 Batch  520/1077 - Train Accuracy: 0.6708, Validation Accuracy: 0.6808, Loss: 0.4326\n",
      "Epoch   2 Batch  540/1077 - Train Accuracy: 0.6504, Validation Accuracy: 0.6570, Loss: 0.4259\n",
      "Epoch   2 Batch  560/1077 - Train Accuracy: 0.6305, Validation Accuracy: 0.6449, Loss: 0.4337\n",
      "Epoch   2 Batch  580/1077 - Train Accuracy: 0.7057, Validation Accuracy: 0.6655, Loss: 0.4140\n",
      "Epoch   2 Batch  600/1077 - Train Accuracy: 0.6696, Validation Accuracy: 0.6761, Loss: 0.4089\n",
      "Epoch   2 Batch  620/1077 - Train Accuracy: 0.6543, Validation Accuracy: 0.6690, Loss: 0.4179\n",
      "Epoch   2 Batch  640/1077 - Train Accuracy: 0.6730, Validation Accuracy: 0.6722, Loss: 0.4155\n",
      "Epoch   2 Batch  660/1077 - Train Accuracy: 0.6422, Validation Accuracy: 0.6673, Loss: 0.4272\n",
      "Epoch   2 Batch  680/1077 - Train Accuracy: 0.6462, Validation Accuracy: 0.6598, Loss: 0.4216\n",
      "Epoch   2 Batch  700/1077 - Train Accuracy: 0.5930, Validation Accuracy: 0.6719, Loss: 0.4038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2 Batch  720/1077 - Train Accuracy: 0.6382, Validation Accuracy: 0.6591, Loss: 0.4379\n",
      "Epoch   2 Batch  740/1077 - Train Accuracy: 0.6883, Validation Accuracy: 0.6854, Loss: 0.3872\n",
      "Epoch   2 Batch  760/1077 - Train Accuracy: 0.6664, Validation Accuracy: 0.6609, Loss: 0.4037\n",
      "Epoch   2 Batch  780/1077 - Train Accuracy: 0.6656, Validation Accuracy: 0.6541, Loss: 0.4022\n",
      "Epoch   2 Batch  800/1077 - Train Accuracy: 0.6434, Validation Accuracy: 0.6797, Loss: 0.3877\n",
      "Epoch   2 Batch  820/1077 - Train Accuracy: 0.6867, Validation Accuracy: 0.6953, Loss: 0.4158\n",
      "Epoch   2 Batch  840/1077 - Train Accuracy: 0.6691, Validation Accuracy: 0.6570, Loss: 0.3579\n",
      "Epoch   2 Batch  860/1077 - Train Accuracy: 0.6533, Validation Accuracy: 0.6825, Loss: 0.3744\n",
      "Epoch   2 Batch  880/1077 - Train Accuracy: 0.7066, Validation Accuracy: 0.6854, Loss: 0.3589\n",
      "Epoch   2 Batch  900/1077 - Train Accuracy: 0.6957, Validation Accuracy: 0.6964, Loss: 0.3618\n",
      "Epoch   2 Batch  920/1077 - Train Accuracy: 0.6855, Validation Accuracy: 0.6893, Loss: 0.3511\n",
      "Epoch   2 Batch  940/1077 - Train Accuracy: 0.7164, Validation Accuracy: 0.6690, Loss: 0.3346\n",
      "Epoch   2 Batch  960/1077 - Train Accuracy: 0.7024, Validation Accuracy: 0.6811, Loss: 0.3278\n",
      "Epoch   2 Batch  980/1077 - Train Accuracy: 0.7379, Validation Accuracy: 0.7035, Loss: 0.3338\n",
      "Epoch   2 Batch 1000/1077 - Train Accuracy: 0.7463, Validation Accuracy: 0.7109, Loss: 0.2964\n",
      "Epoch   2 Batch 1020/1077 - Train Accuracy: 0.7055, Validation Accuracy: 0.7031, Loss: 0.3077\n",
      "Epoch   2 Batch 1040/1077 - Train Accuracy: 0.6900, Validation Accuracy: 0.7067, Loss: 0.3259\n",
      "Epoch   2 Batch 1060/1077 - Train Accuracy: 0.7285, Validation Accuracy: 0.7163, Loss: 0.2938\n",
      "Epoch   3 Batch   20/1077 - Train Accuracy: 0.7270, Validation Accuracy: 0.7280, Loss: 0.3071\n",
      "Epoch   3 Batch   40/1077 - Train Accuracy: 0.7180, Validation Accuracy: 0.7085, Loss: 0.3144\n",
      "Epoch   3 Batch   60/1077 - Train Accuracy: 0.7526, Validation Accuracy: 0.7269, Loss: 0.2930\n",
      "Epoch   3 Batch   80/1077 - Train Accuracy: 0.7324, Validation Accuracy: 0.7116, Loss: 0.3024\n",
      "Epoch   3 Batch  100/1077 - Train Accuracy: 0.7441, Validation Accuracy: 0.7560, Loss: 0.2839\n",
      "Epoch   3 Batch  120/1077 - Train Accuracy: 0.7512, Validation Accuracy: 0.7305, Loss: 0.2899\n",
      "Epoch   3 Batch  140/1077 - Train Accuracy: 0.7722, Validation Accuracy: 0.7308, Loss: 0.2794\n",
      "Epoch   3 Batch  160/1077 - Train Accuracy: 0.7738, Validation Accuracy: 0.7351, Loss: 0.2821\n",
      "Epoch   3 Batch  180/1077 - Train Accuracy: 0.7469, Validation Accuracy: 0.7511, Loss: 0.2552\n",
      "Epoch   3 Batch  200/1077 - Train Accuracy: 0.7652, Validation Accuracy: 0.7496, Loss: 0.2764\n",
      "Epoch   3 Batch  220/1077 - Train Accuracy: 0.7595, Validation Accuracy: 0.7805, Loss: 0.2493\n",
      "Epoch   3 Batch  240/1077 - Train Accuracy: 0.8262, Validation Accuracy: 0.7536, Loss: 0.2342\n",
      "Epoch   3 Batch  260/1077 - Train Accuracy: 0.7898, Validation Accuracy: 0.7514, Loss: 0.2163\n",
      "Epoch   3 Batch  280/1077 - Train Accuracy: 0.7207, Validation Accuracy: 0.7898, Loss: 0.2496\n",
      "Epoch   3 Batch  300/1077 - Train Accuracy: 0.8072, Validation Accuracy: 0.7919, Loss: 0.2204\n",
      "Epoch   3 Batch  320/1077 - Train Accuracy: 0.7848, Validation Accuracy: 0.7528, Loss: 0.2337\n",
      "Epoch   3 Batch  340/1077 - Train Accuracy: 0.7595, Validation Accuracy: 0.7685, Loss: 0.2322\n",
      "Epoch   3 Batch  360/1077 - Train Accuracy: 0.7598, Validation Accuracy: 0.7830, Loss: 0.2156\n",
      "Epoch   3 Batch  380/1077 - Train Accuracy: 0.7855, Validation Accuracy: 0.7830, Loss: 0.2021\n",
      "Epoch   3 Batch  400/1077 - Train Accuracy: 0.8395, Validation Accuracy: 0.7979, Loss: 0.2219\n",
      "Epoch   3 Batch  420/1077 - Train Accuracy: 0.8184, Validation Accuracy: 0.7727, Loss: 0.2021\n",
      "Epoch   3 Batch  440/1077 - Train Accuracy: 0.7531, Validation Accuracy: 0.7706, Loss: 0.2276\n",
      "Epoch   3 Batch  460/1077 - Train Accuracy: 0.7918, Validation Accuracy: 0.7933, Loss: 0.2037\n",
      "Epoch   3 Batch  480/1077 - Train Accuracy: 0.7936, Validation Accuracy: 0.8008, Loss: 0.1932\n",
      "Epoch   3 Batch  500/1077 - Train Accuracy: 0.8258, Validation Accuracy: 0.8015, Loss: 0.1768\n",
      "Epoch   3 Batch  520/1077 - Train Accuracy: 0.8270, Validation Accuracy: 0.8178, Loss: 0.1699\n",
      "Epoch   3 Batch  540/1077 - Train Accuracy: 0.7937, Validation Accuracy: 0.7464, Loss: 0.1773\n",
      "Epoch   3 Batch  560/1077 - Train Accuracy: 0.8074, Validation Accuracy: 0.7756, Loss: 0.1809\n",
      "Epoch   3 Batch  580/1077 - Train Accuracy: 0.8300, Validation Accuracy: 0.7990, Loss: 0.1681\n",
      "Epoch   3 Batch  600/1077 - Train Accuracy: 0.7984, Validation Accuracy: 0.8040, Loss: 0.1644\n",
      "Epoch   3 Batch  620/1077 - Train Accuracy: 0.7918, Validation Accuracy: 0.8168, Loss: 0.1650\n",
      "Epoch   3 Batch  640/1077 - Train Accuracy: 0.8043, Validation Accuracy: 0.7912, Loss: 0.1645\n",
      "Epoch   3 Batch  660/1077 - Train Accuracy: 0.8313, Validation Accuracy: 0.7869, Loss: 0.1762\n",
      "Epoch   3 Batch  680/1077 - Train Accuracy: 0.7984, Validation Accuracy: 0.8001, Loss: 0.1754\n",
      "Epoch   3 Batch  700/1077 - Train Accuracy: 0.8207, Validation Accuracy: 0.7994, Loss: 0.1459\n",
      "Epoch   3 Batch  720/1077 - Train Accuracy: 0.8203, Validation Accuracy: 0.7894, Loss: 0.1732\n",
      "Epoch   3 Batch  740/1077 - Train Accuracy: 0.8242, Validation Accuracy: 0.7887, Loss: 0.1504\n",
      "Epoch   3 Batch  760/1077 - Train Accuracy: 0.8195, Validation Accuracy: 0.8079, Loss: 0.1645\n",
      "Epoch   3 Batch  780/1077 - Train Accuracy: 0.7910, Validation Accuracy: 0.8175, Loss: 0.1677\n",
      "Epoch   3 Batch  800/1077 - Train Accuracy: 0.8297, Validation Accuracy: 0.8058, Loss: 0.1488\n",
      "Epoch   3 Batch  820/1077 - Train Accuracy: 0.7922, Validation Accuracy: 0.8484, Loss: 0.1668\n",
      "Epoch   3 Batch  840/1077 - Train Accuracy: 0.8441, Validation Accuracy: 0.8189, Loss: 0.1405\n",
      "Epoch   3 Batch  860/1077 - Train Accuracy: 0.8397, Validation Accuracy: 0.8391, Loss: 0.1610\n",
      "Epoch   3 Batch  880/1077 - Train Accuracy: 0.8285, Validation Accuracy: 0.8278, Loss: 0.1552\n",
      "Epoch   3 Batch  900/1077 - Train Accuracy: 0.7930, Validation Accuracy: 0.7582, Loss: 0.1873\n",
      "Epoch   3 Batch  920/1077 - Train Accuracy: 0.8285, Validation Accuracy: 0.8047, Loss: 0.1669\n",
      "Epoch   3 Batch  940/1077 - Train Accuracy: 0.8621, Validation Accuracy: 0.8402, Loss: 0.1330\n",
      "Epoch   3 Batch  960/1077 - Train Accuracy: 0.8289, Validation Accuracy: 0.8271, Loss: 0.1401\n",
      "Epoch   3 Batch  980/1077 - Train Accuracy: 0.8504, Validation Accuracy: 0.8587, Loss: 0.1468\n",
      "Epoch   3 Batch 1000/1077 - Train Accuracy: 0.8627, Validation Accuracy: 0.8597, Loss: 0.1201\n",
      "Epoch   3 Batch 1020/1077 - Train Accuracy: 0.8508, Validation Accuracy: 0.8604, Loss: 0.1248\n",
      "Epoch   3 Batch 1040/1077 - Train Accuracy: 0.8512, Validation Accuracy: 0.8413, Loss: 0.1378\n",
      "Epoch   3 Batch 1060/1077 - Train Accuracy: 0.8680, Validation Accuracy: 0.8438, Loss: 0.1126\n",
      "Epoch   4 Batch   20/1077 - Train Accuracy: 0.8297, Validation Accuracy: 0.8722, Loss: 0.1220\n",
      "Epoch   4 Batch   40/1077 - Train Accuracy: 0.8629, Validation Accuracy: 0.8608, Loss: 0.1196\n",
      "Epoch   4 Batch   60/1077 - Train Accuracy: 0.8531, Validation Accuracy: 0.8452, Loss: 0.1157\n",
      "Epoch   4 Batch   80/1077 - Train Accuracy: 0.8461, Validation Accuracy: 0.8477, Loss: 0.1230\n",
      "Epoch   4 Batch  100/1077 - Train Accuracy: 0.8434, Validation Accuracy: 0.8459, Loss: 0.1084\n",
      "Epoch   4 Batch  120/1077 - Train Accuracy: 0.8559, Validation Accuracy: 0.8583, Loss: 0.1213\n",
      "Epoch   4 Batch  140/1077 - Train Accuracy: 0.8396, Validation Accuracy: 0.8505, Loss: 0.1195\n",
      "Epoch   4 Batch  160/1077 - Train Accuracy: 0.8625, Validation Accuracy: 0.8739, Loss: 0.1156\n",
      "Epoch   4 Batch  180/1077 - Train Accuracy: 0.8520, Validation Accuracy: 0.8452, Loss: 0.1075\n",
      "Epoch   4 Batch  200/1077 - Train Accuracy: 0.8223, Validation Accuracy: 0.8413, Loss: 0.1177\n",
      "Epoch   4 Batch  220/1077 - Train Accuracy: 0.8606, Validation Accuracy: 0.8274, Loss: 0.1005\n",
      "Epoch   4 Batch  240/1077 - Train Accuracy: 0.8941, Validation Accuracy: 0.8320, Loss: 0.1009\n",
      "Epoch   4 Batch  260/1077 - Train Accuracy: 0.8891, Validation Accuracy: 0.8718, Loss: 0.0900\n",
      "Epoch   4 Batch  280/1077 - Train Accuracy: 0.8316, Validation Accuracy: 0.8540, Loss: 0.1137\n",
      "Epoch   4 Batch  300/1077 - Train Accuracy: 0.8877, Validation Accuracy: 0.8565, Loss: 0.0936\n",
      "Epoch   4 Batch  320/1077 - Train Accuracy: 0.8852, Validation Accuracy: 0.8452, Loss: 0.1112\n",
      "Epoch   4 Batch  340/1077 - Train Accuracy: 0.9079, Validation Accuracy: 0.8658, Loss: 0.1069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4 Batch  360/1077 - Train Accuracy: 0.8809, Validation Accuracy: 0.8565, Loss: 0.1022\n",
      "Epoch   4 Batch  380/1077 - Train Accuracy: 0.8840, Validation Accuracy: 0.8651, Loss: 0.0947\n",
      "Epoch   4 Batch  400/1077 - Train Accuracy: 0.8773, Validation Accuracy: 0.8846, Loss: 0.1045\n",
      "Epoch   4 Batch  420/1077 - Train Accuracy: 0.8863, Validation Accuracy: 0.8693, Loss: 0.0836\n",
      "Epoch   4 Batch  440/1077 - Train Accuracy: 0.8309, Validation Accuracy: 0.8604, Loss: 0.1143\n",
      "Epoch   4 Batch  460/1077 - Train Accuracy: 0.9012, Validation Accuracy: 0.8857, Loss: 0.1030\n",
      "Epoch   4 Batch  480/1077 - Train Accuracy: 0.8840, Validation Accuracy: 0.8647, Loss: 0.0883\n",
      "Epoch   4 Batch  500/1077 - Train Accuracy: 0.8773, Validation Accuracy: 0.8420, Loss: 0.0822\n",
      "Epoch   4 Batch  520/1077 - Train Accuracy: 0.9022, Validation Accuracy: 0.8672, Loss: 0.0842\n",
      "Epoch   4 Batch  540/1077 - Train Accuracy: 0.8914, Validation Accuracy: 0.8565, Loss: 0.0839\n",
      "Epoch   4 Batch  560/1077 - Train Accuracy: 0.8703, Validation Accuracy: 0.8413, Loss: 0.0826\n",
      "Epoch   4 Batch  580/1077 - Train Accuracy: 0.9055, Validation Accuracy: 0.8484, Loss: 0.0802\n",
      "Epoch   4 Batch  600/1077 - Train Accuracy: 0.8594, Validation Accuracy: 0.8821, Loss: 0.0898\n",
      "Epoch   4 Batch  620/1077 - Train Accuracy: 0.9047, Validation Accuracy: 0.8899, Loss: 0.0839\n",
      "Epoch   4 Batch  640/1077 - Train Accuracy: 0.8910, Validation Accuracy: 0.8963, Loss: 0.0847\n",
      "Epoch   4 Batch  660/1077 - Train Accuracy: 0.8918, Validation Accuracy: 0.8690, Loss: 0.0999\n",
      "Epoch   4 Batch  680/1077 - Train Accuracy: 0.8780, Validation Accuracy: 0.8963, Loss: 0.0918\n",
      "Epoch   4 Batch  700/1077 - Train Accuracy: 0.9109, Validation Accuracy: 0.8960, Loss: 0.0734\n",
      "Epoch   4 Batch  720/1077 - Train Accuracy: 0.9104, Validation Accuracy: 0.8736, Loss: 0.0945\n",
      "Epoch   4 Batch  740/1077 - Train Accuracy: 0.9043, Validation Accuracy: 0.8803, Loss: 0.0710\n",
      "Epoch   4 Batch  760/1077 - Train Accuracy: 0.8914, Validation Accuracy: 0.8796, Loss: 0.0854\n",
      "Epoch   4 Batch  780/1077 - Train Accuracy: 0.8531, Validation Accuracy: 0.9130, Loss: 0.0953\n",
      "Epoch   4 Batch  800/1077 - Train Accuracy: 0.8902, Validation Accuracy: 0.8786, Loss: 0.0792\n",
      "Epoch   4 Batch  820/1077 - Train Accuracy: 0.8473, Validation Accuracy: 0.8988, Loss: 0.0841\n",
      "Epoch   4 Batch  840/1077 - Train Accuracy: 0.8977, Validation Accuracy: 0.8725, Loss: 0.0758\n",
      "Epoch   4 Batch  860/1077 - Train Accuracy: 0.9074, Validation Accuracy: 0.8938, Loss: 0.0825\n",
      "Epoch   4 Batch  880/1077 - Train Accuracy: 0.9039, Validation Accuracy: 0.8949, Loss: 0.0866\n",
      "Epoch   4 Batch  900/1077 - Train Accuracy: 0.9023, Validation Accuracy: 0.8821, Loss: 0.0774\n",
      "Epoch   4 Batch  920/1077 - Train Accuracy: 0.8730, Validation Accuracy: 0.8952, Loss: 0.0660\n",
      "Epoch   4 Batch  940/1077 - Train Accuracy: 0.9035, Validation Accuracy: 0.8924, Loss: 0.0600\n",
      "Epoch   4 Batch  960/1077 - Train Accuracy: 0.8977, Validation Accuracy: 0.8807, Loss: 0.0712\n",
      "Epoch   4 Batch  980/1077 - Train Accuracy: 0.8750, Validation Accuracy: 0.8789, Loss: 0.0727\n",
      "Epoch   4 Batch 1000/1077 - Train Accuracy: 0.9196, Validation Accuracy: 0.9020, Loss: 0.0617\n",
      "Epoch   4 Batch 1020/1077 - Train Accuracy: 0.9156, Validation Accuracy: 0.8967, Loss: 0.0592\n",
      "Epoch   4 Batch 1040/1077 - Train Accuracy: 0.9071, Validation Accuracy: 0.8846, Loss: 0.0695\n",
      "Epoch   4 Batch 1060/1077 - Train Accuracy: 0.9031, Validation Accuracy: 0.8924, Loss: 0.0562\n",
      "Epoch   5 Batch   20/1077 - Train Accuracy: 0.8871, Validation Accuracy: 0.8974, Loss: 0.0638\n",
      "Epoch   5 Batch   40/1077 - Train Accuracy: 0.9309, Validation Accuracy: 0.8832, Loss: 0.0570\n",
      "Epoch   5 Batch   60/1077 - Train Accuracy: 0.9249, Validation Accuracy: 0.8828, Loss: 0.0563\n",
      "Epoch   5 Batch   80/1077 - Train Accuracy: 0.9109, Validation Accuracy: 0.9006, Loss: 0.0633\n",
      "Epoch   5 Batch  100/1077 - Train Accuracy: 0.9258, Validation Accuracy: 0.9173, Loss: 0.0563\n",
      "Epoch   5 Batch  120/1077 - Train Accuracy: 0.9039, Validation Accuracy: 0.8960, Loss: 0.0636\n",
      "Epoch   5 Batch  140/1077 - Train Accuracy: 0.9038, Validation Accuracy: 0.8739, Loss: 0.0528\n",
      "Epoch   5 Batch  160/1077 - Train Accuracy: 0.9059, Validation Accuracy: 0.9126, Loss: 0.0548\n",
      "Epoch   5 Batch  180/1077 - Train Accuracy: 0.9117, Validation Accuracy: 0.9041, Loss: 0.0560\n",
      "Epoch   5 Batch  200/1077 - Train Accuracy: 0.9059, Validation Accuracy: 0.8981, Loss: 0.0620\n",
      "Epoch   5 Batch  220/1077 - Train Accuracy: 0.9231, Validation Accuracy: 0.8871, Loss: 0.0534\n",
      "Epoch   5 Batch  240/1077 - Train Accuracy: 0.9469, Validation Accuracy: 0.9102, Loss: 0.0524\n",
      "Epoch   5 Batch  260/1077 - Train Accuracy: 0.9219, Validation Accuracy: 0.9286, Loss: 0.0511\n",
      "Epoch   5 Batch  280/1077 - Train Accuracy: 0.8918, Validation Accuracy: 0.8995, Loss: 0.0658\n",
      "Epoch   5 Batch  300/1077 - Train Accuracy: 0.9428, Validation Accuracy: 0.9158, Loss: 0.0495\n",
      "Epoch   5 Batch  320/1077 - Train Accuracy: 0.9320, Validation Accuracy: 0.9070, Loss: 0.0635\n",
      "Epoch   5 Batch  340/1077 - Train Accuracy: 0.9235, Validation Accuracy: 0.9105, Loss: 0.0543\n",
      "Epoch   5 Batch  360/1077 - Train Accuracy: 0.9293, Validation Accuracy: 0.9066, Loss: 0.0460\n",
      "Epoch   5 Batch  380/1077 - Train Accuracy: 0.9508, Validation Accuracy: 0.9237, Loss: 0.0426\n",
      "Epoch   5 Batch  400/1077 - Train Accuracy: 0.8848, Validation Accuracy: 0.9038, Loss: 0.0603\n",
      "Epoch   5 Batch  420/1077 - Train Accuracy: 0.9594, Validation Accuracy: 0.9293, Loss: 0.0394\n",
      "Epoch   5 Batch  440/1077 - Train Accuracy: 0.9191, Validation Accuracy: 0.8984, Loss: 0.0569\n",
      "Epoch   5 Batch  460/1077 - Train Accuracy: 0.9156, Validation Accuracy: 0.9272, Loss: 0.0525\n",
      "Epoch   5 Batch  480/1077 - Train Accuracy: 0.9272, Validation Accuracy: 0.9276, Loss: 0.0423\n",
      "Epoch   5 Batch  500/1077 - Train Accuracy: 0.9066, Validation Accuracy: 0.8938, Loss: 0.0420\n",
      "Epoch   5 Batch  520/1077 - Train Accuracy: 0.9591, Validation Accuracy: 0.9148, Loss: 0.0410\n",
      "Epoch   5 Batch  540/1077 - Train Accuracy: 0.9398, Validation Accuracy: 0.8999, Loss: 0.0356\n",
      "Epoch   5 Batch  560/1077 - Train Accuracy: 0.9211, Validation Accuracy: 0.8988, Loss: 0.0456\n",
      "Epoch   5 Batch  580/1077 - Train Accuracy: 0.9249, Validation Accuracy: 0.9197, Loss: 0.0382\n",
      "Epoch   5 Batch  600/1077 - Train Accuracy: 0.9066, Validation Accuracy: 0.9187, Loss: 0.0517\n",
      "Epoch   5 Batch  620/1077 - Train Accuracy: 0.9656, Validation Accuracy: 0.9268, Loss: 0.0434\n",
      "Epoch   5 Batch  640/1077 - Train Accuracy: 0.9308, Validation Accuracy: 0.9116, Loss: 0.0440\n",
      "Epoch   5 Batch  660/1077 - Train Accuracy: 0.9160, Validation Accuracy: 0.9290, Loss: 0.0445\n",
      "Epoch   5 Batch  680/1077 - Train Accuracy: 0.9170, Validation Accuracy: 0.9336, Loss: 0.0464\n",
      "Epoch   5 Batch  700/1077 - Train Accuracy: 0.9426, Validation Accuracy: 0.9094, Loss: 0.0351\n",
      "Epoch   5 Batch  720/1077 - Train Accuracy: 0.9165, Validation Accuracy: 0.9258, Loss: 0.0441\n",
      "Epoch   5 Batch  740/1077 - Train Accuracy: 0.9293, Validation Accuracy: 0.9350, Loss: 0.0367\n",
      "Epoch   5 Batch  760/1077 - Train Accuracy: 0.9277, Validation Accuracy: 0.9354, Loss: 0.0454\n",
      "Epoch   5 Batch  780/1077 - Train Accuracy: 0.9055, Validation Accuracy: 0.9467, Loss: 0.0577\n",
      "Epoch   5 Batch  800/1077 - Train Accuracy: 0.9371, Validation Accuracy: 0.9407, Loss: 0.0435\n",
      "Epoch   5 Batch  820/1077 - Train Accuracy: 0.8941, Validation Accuracy: 0.9155, Loss: 0.0440\n",
      "Epoch   5 Batch  840/1077 - Train Accuracy: 0.9305, Validation Accuracy: 0.9304, Loss: 0.0450\n",
      "Epoch   5 Batch  860/1077 - Train Accuracy: 0.9368, Validation Accuracy: 0.9354, Loss: 0.0497\n",
      "Epoch   5 Batch  880/1077 - Train Accuracy: 0.9387, Validation Accuracy: 0.9116, Loss: 0.0593\n",
      "Epoch   5 Batch  900/1077 - Train Accuracy: 0.9293, Validation Accuracy: 0.9457, Loss: 0.0501\n",
      "Epoch   5 Batch  920/1077 - Train Accuracy: 0.9352, Validation Accuracy: 0.9425, Loss: 0.0407\n",
      "Epoch   5 Batch  940/1077 - Train Accuracy: 0.9578, Validation Accuracy: 0.9233, Loss: 0.0352\n",
      "Epoch   5 Batch  960/1077 - Train Accuracy: 0.9483, Validation Accuracy: 0.9329, Loss: 0.0372\n",
      "Epoch   5 Batch  980/1077 - Train Accuracy: 0.8992, Validation Accuracy: 0.9208, Loss: 0.0435\n",
      "Epoch   5 Batch 1000/1077 - Train Accuracy: 0.9382, Validation Accuracy: 0.9315, Loss: 0.0349\n",
      "Epoch   5 Batch 1020/1077 - Train Accuracy: 0.9512, Validation Accuracy: 0.9212, Loss: 0.0317\n",
      "Epoch   5 Batch 1040/1077 - Train Accuracy: 0.9490, Validation Accuracy: 0.9432, Loss: 0.0404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5 Batch 1060/1077 - Train Accuracy: 0.9441, Validation Accuracy: 0.9155, Loss: 0.0323\n",
      "Epoch   6 Batch   20/1077 - Train Accuracy: 0.9230, Validation Accuracy: 0.9450, Loss: 0.0312\n",
      "Epoch   6 Batch   40/1077 - Train Accuracy: 0.9477, Validation Accuracy: 0.9212, Loss: 0.0304\n",
      "Epoch   6 Batch   60/1077 - Train Accuracy: 0.9442, Validation Accuracy: 0.9194, Loss: 0.0305\n",
      "Epoch   6 Batch   80/1077 - Train Accuracy: 0.9332, Validation Accuracy: 0.9474, Loss: 0.0380\n",
      "Epoch   6 Batch  100/1077 - Train Accuracy: 0.9309, Validation Accuracy: 0.9503, Loss: 0.0366\n",
      "Epoch   6 Batch  120/1077 - Train Accuracy: 0.9352, Validation Accuracy: 0.9364, Loss: 0.0390\n",
      "Epoch   6 Batch  140/1077 - Train Accuracy: 0.9338, Validation Accuracy: 0.9322, Loss: 0.0371\n",
      "Epoch   6 Batch  160/1077 - Train Accuracy: 0.9590, Validation Accuracy: 0.9435, Loss: 0.0331\n",
      "Epoch   6 Batch  180/1077 - Train Accuracy: 0.9414, Validation Accuracy: 0.9411, Loss: 0.0320\n",
      "Epoch   6 Batch  200/1077 - Train Accuracy: 0.9504, Validation Accuracy: 0.9354, Loss: 0.0362\n",
      "Epoch   6 Batch  220/1077 - Train Accuracy: 0.9572, Validation Accuracy: 0.9279, Loss: 0.0328\n",
      "Epoch   6 Batch  240/1077 - Train Accuracy: 0.9480, Validation Accuracy: 0.9560, Loss: 0.0350\n",
      "Epoch   6 Batch  260/1077 - Train Accuracy: 0.9464, Validation Accuracy: 0.9528, Loss: 0.0274\n",
      "Epoch   6 Batch  280/1077 - Train Accuracy: 0.9086, Validation Accuracy: 0.9393, Loss: 0.0406\n",
      "Epoch   6 Batch  300/1077 - Train Accuracy: 0.9470, Validation Accuracy: 0.9471, Loss: 0.0291\n",
      "Epoch   6 Batch  320/1077 - Train Accuracy: 0.9500, Validation Accuracy: 0.9585, Loss: 0.0432\n",
      "Epoch   6 Batch  340/1077 - Train Accuracy: 0.9568, Validation Accuracy: 0.9368, Loss: 0.0349\n",
      "Epoch   6 Batch  360/1077 - Train Accuracy: 0.9492, Validation Accuracy: 0.9588, Loss: 0.0256\n",
      "Epoch   6 Batch  380/1077 - Train Accuracy: 0.9699, Validation Accuracy: 0.9411, Loss: 0.0292\n",
      "Epoch   6 Batch  400/1077 - Train Accuracy: 0.9285, Validation Accuracy: 0.9563, Loss: 0.0326\n",
      "Epoch   6 Batch  420/1077 - Train Accuracy: 0.9645, Validation Accuracy: 0.9226, Loss: 0.0252\n",
      "Epoch   6 Batch  440/1077 - Train Accuracy: 0.9211, Validation Accuracy: 0.9265, Loss: 0.0419\n",
      "Epoch   6 Batch  460/1077 - Train Accuracy: 0.9297, Validation Accuracy: 0.9347, Loss: 0.0368\n",
      "Epoch   6 Batch  480/1077 - Train Accuracy: 0.9391, Validation Accuracy: 0.9325, Loss: 0.0242\n",
      "Epoch   6 Batch  500/1077 - Train Accuracy: 0.9324, Validation Accuracy: 0.9354, Loss: 0.0286\n",
      "Epoch   6 Batch  520/1077 - Train Accuracy: 0.9598, Validation Accuracy: 0.9268, Loss: 0.0267\n",
      "Epoch   6 Batch  540/1077 - Train Accuracy: 0.9508, Validation Accuracy: 0.9531, Loss: 0.0246\n",
      "Epoch   6 Batch  560/1077 - Train Accuracy: 0.9266, Validation Accuracy: 0.9482, Loss: 0.0308\n",
      "Epoch   6 Batch  580/1077 - Train Accuracy: 0.9661, Validation Accuracy: 0.9368, Loss: 0.0292\n",
      "Epoch   6 Batch  600/1077 - Train Accuracy: 0.9442, Validation Accuracy: 0.9403, Loss: 0.0353\n",
      "Epoch   6 Batch  620/1077 - Train Accuracy: 0.9605, Validation Accuracy: 0.9698, Loss: 0.0267\n",
      "Epoch   6 Batch  640/1077 - Train Accuracy: 0.9412, Validation Accuracy: 0.9130, Loss: 0.0258\n",
      "Epoch   6 Batch  660/1077 - Train Accuracy: 0.9414, Validation Accuracy: 0.9474, Loss: 0.0322\n",
      "Epoch   6 Batch  680/1077 - Train Accuracy: 0.9342, Validation Accuracy: 0.9350, Loss: 0.0346\n",
      "Epoch   6 Batch  700/1077 - Train Accuracy: 0.9750, Validation Accuracy: 0.9588, Loss: 0.0232\n",
      "Epoch   6 Batch  720/1077 - Train Accuracy: 0.9396, Validation Accuracy: 0.9695, Loss: 0.0355\n",
      "Epoch   6 Batch  740/1077 - Train Accuracy: 0.9437, Validation Accuracy: 0.9261, Loss: 0.0279\n",
      "Epoch   6 Batch  760/1077 - Train Accuracy: 0.9457, Validation Accuracy: 0.9396, Loss: 0.0328\n",
      "Epoch   6 Batch  780/1077 - Train Accuracy: 0.9461, Validation Accuracy: 0.9411, Loss: 0.0462\n",
      "Epoch   6 Batch  800/1077 - Train Accuracy: 0.9355, Validation Accuracy: 0.9276, Loss: 0.0301\n",
      "Epoch   6 Batch  820/1077 - Train Accuracy: 0.9266, Validation Accuracy: 0.9332, Loss: 0.0319\n",
      "Epoch   6 Batch  840/1077 - Train Accuracy: 0.9324, Validation Accuracy: 0.9460, Loss: 0.0306\n",
      "Epoch   6 Batch  860/1077 - Train Accuracy: 0.9408, Validation Accuracy: 0.9439, Loss: 0.0331\n",
      "Epoch   6 Batch  880/1077 - Train Accuracy: 0.9418, Validation Accuracy: 0.9627, Loss: 0.0421\n",
      "Epoch   6 Batch  900/1077 - Train Accuracy: 0.9441, Validation Accuracy: 0.9308, Loss: 0.0332\n",
      "Epoch   6 Batch  920/1077 - Train Accuracy: 0.9336, Validation Accuracy: 0.9222, Loss: 0.0298\n",
      "Epoch   6 Batch  940/1077 - Train Accuracy: 0.9574, Validation Accuracy: 0.9286, Loss: 0.0236\n",
      "Epoch   6 Batch  960/1077 - Train Accuracy: 0.9628, Validation Accuracy: 0.9425, Loss: 0.0288\n",
      "Epoch   6 Batch  980/1077 - Train Accuracy: 0.9270, Validation Accuracy: 0.9474, Loss: 0.0313\n",
      "Epoch   6 Batch 1000/1077 - Train Accuracy: 0.9535, Validation Accuracy: 0.9421, Loss: 0.0260\n",
      "Epoch   6 Batch 1020/1077 - Train Accuracy: 0.9484, Validation Accuracy: 0.9254, Loss: 0.0276\n",
      "Epoch   6 Batch 1040/1077 - Train Accuracy: 0.9498, Validation Accuracy: 0.9339, Loss: 0.0318\n",
      "Epoch   6 Batch 1060/1077 - Train Accuracy: 0.9527, Validation Accuracy: 0.9261, Loss: 0.0257\n",
      "Epoch   7 Batch   20/1077 - Train Accuracy: 0.9352, Validation Accuracy: 0.9357, Loss: 0.0222\n",
      "Epoch   7 Batch   40/1077 - Train Accuracy: 0.9484, Validation Accuracy: 0.9474, Loss: 0.0261\n",
      "Epoch   7 Batch   60/1077 - Train Accuracy: 0.9423, Validation Accuracy: 0.9489, Loss: 0.0283\n",
      "Epoch   7 Batch   80/1077 - Train Accuracy: 0.9266, Validation Accuracy: 0.9553, Loss: 0.0272\n",
      "Epoch   7 Batch  100/1077 - Train Accuracy: 0.9617, Validation Accuracy: 0.9553, Loss: 0.0247\n",
      "Epoch   7 Batch  120/1077 - Train Accuracy: 0.9375, Validation Accuracy: 0.9513, Loss: 0.0265\n",
      "Epoch   7 Batch  140/1077 - Train Accuracy: 0.9346, Validation Accuracy: 0.9613, Loss: 0.0279\n",
      "Epoch   7 Batch  160/1077 - Train Accuracy: 0.9535, Validation Accuracy: 0.9688, Loss: 0.0220\n",
      "Epoch   7 Batch  180/1077 - Train Accuracy: 0.9664, Validation Accuracy: 0.9538, Loss: 0.0263\n",
      "Epoch   7 Batch  200/1077 - Train Accuracy: 0.9652, Validation Accuracy: 0.9478, Loss: 0.0234\n",
      "Epoch   7 Batch  220/1077 - Train Accuracy: 0.9564, Validation Accuracy: 0.9268, Loss: 0.0267\n",
      "Epoch   7 Batch  240/1077 - Train Accuracy: 0.9727, Validation Accuracy: 0.9418, Loss: 0.0250\n",
      "Epoch   7 Batch  260/1077 - Train Accuracy: 0.9557, Validation Accuracy: 0.9485, Loss: 0.0223\n",
      "Epoch   7 Batch  280/1077 - Train Accuracy: 0.9074, Validation Accuracy: 0.9460, Loss: 0.0299\n",
      "Epoch   7 Batch  300/1077 - Train Accuracy: 0.9527, Validation Accuracy: 0.9513, Loss: 0.0221\n",
      "Epoch   7 Batch  320/1077 - Train Accuracy: 0.9676, Validation Accuracy: 0.9279, Loss: 0.0380\n",
      "Epoch   7 Batch  340/1077 - Train Accuracy: 0.9712, Validation Accuracy: 0.9347, Loss: 0.0270\n",
      "Epoch   7 Batch  360/1077 - Train Accuracy: 0.9574, Validation Accuracy: 0.9503, Loss: 0.0218\n",
      "Epoch   7 Batch  380/1077 - Train Accuracy: 0.9594, Validation Accuracy: 0.9489, Loss: 0.0238\n",
      "Epoch   7 Batch  400/1077 - Train Accuracy: 0.9441, Validation Accuracy: 0.9421, Loss: 0.0305\n",
      "Epoch   7 Batch  420/1077 - Train Accuracy: 0.9707, Validation Accuracy: 0.9325, Loss: 0.0211\n",
      "Epoch   7 Batch  440/1077 - Train Accuracy: 0.9492, Validation Accuracy: 0.9322, Loss: 0.0317\n",
      "Epoch   7 Batch  460/1077 - Train Accuracy: 0.9512, Validation Accuracy: 0.9592, Loss: 0.0254\n",
      "Epoch   7 Batch  480/1077 - Train Accuracy: 0.9589, Validation Accuracy: 0.9503, Loss: 0.0199\n",
      "Epoch   7 Batch  500/1077 - Train Accuracy: 0.9680, Validation Accuracy: 0.9474, Loss: 0.0238\n",
      "Epoch   7 Batch  520/1077 - Train Accuracy: 0.9821, Validation Accuracy: 0.9766, Loss: 0.0173\n",
      "Epoch   7 Batch  540/1077 - Train Accuracy: 0.9719, Validation Accuracy: 0.9634, Loss: 0.0170\n",
      "Epoch   7 Batch  560/1077 - Train Accuracy: 0.9465, Validation Accuracy: 0.9339, Loss: 0.0242\n",
      "Epoch   7 Batch  580/1077 - Train Accuracy: 0.9621, Validation Accuracy: 0.9663, Loss: 0.0208\n",
      "Epoch   7 Batch  600/1077 - Train Accuracy: 0.9498, Validation Accuracy: 0.9513, Loss: 0.0272\n",
      "Epoch   7 Batch  620/1077 - Train Accuracy: 0.9566, Validation Accuracy: 0.9499, Loss: 0.0237\n",
      "Epoch   7 Batch  640/1077 - Train Accuracy: 0.9714, Validation Accuracy: 0.9577, Loss: 0.0204\n",
      "Epoch   7 Batch  660/1077 - Train Accuracy: 0.9648, Validation Accuracy: 0.9524, Loss: 0.0214\n",
      "Epoch   7 Batch  680/1077 - Train Accuracy: 0.9446, Validation Accuracy: 0.9460, Loss: 0.0253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7 Batch  700/1077 - Train Accuracy: 0.9578, Validation Accuracy: 0.9719, Loss: 0.0195\n",
      "Epoch   7 Batch  720/1077 - Train Accuracy: 0.9441, Validation Accuracy: 0.9510, Loss: 0.0274\n",
      "Epoch   7 Batch  740/1077 - Train Accuracy: 0.9746, Validation Accuracy: 0.9553, Loss: 0.0248\n",
      "Epoch   7 Batch  760/1077 - Train Accuracy: 0.9453, Validation Accuracy: 0.9336, Loss: 0.0223\n",
      "Epoch   7 Batch  780/1077 - Train Accuracy: 0.9098, Validation Accuracy: 0.9531, Loss: 0.0339\n",
      "Epoch   7 Batch  800/1077 - Train Accuracy: 0.9437, Validation Accuracy: 0.9517, Loss: 0.0270\n",
      "Epoch   7 Batch  820/1077 - Train Accuracy: 0.9398, Validation Accuracy: 0.9599, Loss: 0.0203\n",
      "Epoch   7 Batch  840/1077 - Train Accuracy: 0.9684, Validation Accuracy: 0.9634, Loss: 0.0220\n",
      "Epoch   7 Batch  860/1077 - Train Accuracy: 0.9438, Validation Accuracy: 0.9588, Loss: 0.0267\n",
      "Epoch   7 Batch  880/1077 - Train Accuracy: 0.9504, Validation Accuracy: 0.9549, Loss: 0.0279\n",
      "Epoch   7 Batch  900/1077 - Train Accuracy: 0.9586, Validation Accuracy: 0.9421, Loss: 0.0263\n",
      "Epoch   7 Batch  920/1077 - Train Accuracy: 0.9512, Validation Accuracy: 0.9585, Loss: 0.0214\n",
      "Epoch   7 Batch  940/1077 - Train Accuracy: 0.9582, Validation Accuracy: 0.9645, Loss: 0.0148\n",
      "Epoch   7 Batch  960/1077 - Train Accuracy: 0.9680, Validation Accuracy: 0.9567, Loss: 0.0187\n",
      "Epoch   7 Batch  980/1077 - Train Accuracy: 0.9293, Validation Accuracy: 0.9606, Loss: 0.0246\n",
      "Epoch   7 Batch 1000/1077 - Train Accuracy: 0.9561, Validation Accuracy: 0.9549, Loss: 0.0249\n",
      "Epoch   7 Batch 1020/1077 - Train Accuracy: 0.9711, Validation Accuracy: 0.9478, Loss: 0.0207\n",
      "Epoch   7 Batch 1040/1077 - Train Accuracy: 0.9539, Validation Accuracy: 0.9581, Loss: 0.0252\n",
      "Epoch   7 Batch 1060/1077 - Train Accuracy: 0.9730, Validation Accuracy: 0.9599, Loss: 0.0158\n",
      "Epoch   8 Batch   20/1077 - Train Accuracy: 0.9617, Validation Accuracy: 0.9581, Loss: 0.0157\n",
      "Epoch   8 Batch   40/1077 - Train Accuracy: 0.9754, Validation Accuracy: 0.9723, Loss: 0.0230\n",
      "Epoch   8 Batch   60/1077 - Train Accuracy: 0.9572, Validation Accuracy: 0.9453, Loss: 0.0168\n",
      "Epoch   8 Batch   80/1077 - Train Accuracy: 0.9609, Validation Accuracy: 0.9581, Loss: 0.0258\n",
      "Epoch   8 Batch  100/1077 - Train Accuracy: 0.9578, Validation Accuracy: 0.9705, Loss: 0.0203\n",
      "Epoch   8 Batch  120/1077 - Train Accuracy: 0.9484, Validation Accuracy: 0.9648, Loss: 0.0198\n",
      "Epoch   8 Batch  140/1077 - Train Accuracy: 0.9498, Validation Accuracy: 0.9457, Loss: 0.0244\n",
      "Epoch   8 Batch  160/1077 - Train Accuracy: 0.9641, Validation Accuracy: 0.9641, Loss: 0.0190\n",
      "Epoch   8 Batch  180/1077 - Train Accuracy: 0.9602, Validation Accuracy: 0.9492, Loss: 0.0185\n",
      "Epoch   8 Batch  200/1077 - Train Accuracy: 0.9551, Validation Accuracy: 0.9510, Loss: 0.0198\n",
      "Epoch   8 Batch  220/1077 - Train Accuracy: 0.9725, Validation Accuracy: 0.9684, Loss: 0.0206\n",
      "Epoch   8 Batch  240/1077 - Train Accuracy: 0.9828, Validation Accuracy: 0.9631, Loss: 0.0180\n",
      "Epoch   8 Batch  260/1077 - Train Accuracy: 0.9594, Validation Accuracy: 0.9627, Loss: 0.0148\n",
      "Epoch   8 Batch  280/1077 - Train Accuracy: 0.9250, Validation Accuracy: 0.9567, Loss: 0.0223\n",
      "Epoch   8 Batch  300/1077 - Train Accuracy: 0.9729, Validation Accuracy: 0.9535, Loss: 0.0192\n",
      "Epoch   8 Batch  320/1077 - Train Accuracy: 0.9590, Validation Accuracy: 0.9563, Loss: 0.0296\n",
      "Epoch   8 Batch  340/1077 - Train Accuracy: 0.9786, Validation Accuracy: 0.9560, Loss: 0.0172\n",
      "Epoch   8 Batch  360/1077 - Train Accuracy: 0.9660, Validation Accuracy: 0.9702, Loss: 0.0172\n",
      "Epoch   8 Batch  380/1077 - Train Accuracy: 0.9602, Validation Accuracy: 0.9513, Loss: 0.0177\n",
      "Epoch   8 Batch  400/1077 - Train Accuracy: 0.9664, Validation Accuracy: 0.9556, Loss: 0.0200\n",
      "Epoch   8 Batch  420/1077 - Train Accuracy: 0.9801, Validation Accuracy: 0.9471, Loss: 0.0156\n",
      "Epoch   8 Batch  440/1077 - Train Accuracy: 0.9563, Validation Accuracy: 0.9457, Loss: 0.0284\n",
      "Epoch   8 Batch  460/1077 - Train Accuracy: 0.9535, Validation Accuracy: 0.9524, Loss: 0.0231\n",
      "Epoch   8 Batch  480/1077 - Train Accuracy: 0.9576, Validation Accuracy: 0.9567, Loss: 0.0201\n",
      "Epoch   8 Batch  500/1077 - Train Accuracy: 0.9586, Validation Accuracy: 0.9563, Loss: 0.0173\n",
      "Epoch   8 Batch  520/1077 - Train Accuracy: 0.9840, Validation Accuracy: 0.9680, Loss: 0.0135\n",
      "Epoch   8 Batch  540/1077 - Train Accuracy: 0.9676, Validation Accuracy: 0.9741, Loss: 0.0173\n",
      "Epoch   8 Batch  560/1077 - Train Accuracy: 0.9516, Validation Accuracy: 0.9513, Loss: 0.0198\n",
      "Epoch   8 Batch  580/1077 - Train Accuracy: 0.9576, Validation Accuracy: 0.9592, Loss: 0.0184\n",
      "Epoch   8 Batch  600/1077 - Train Accuracy: 0.9695, Validation Accuracy: 0.9812, Loss: 0.0243\n",
      "Epoch   8 Batch  620/1077 - Train Accuracy: 0.9406, Validation Accuracy: 0.9496, Loss: 0.0250\n",
      "Epoch   8 Batch  640/1077 - Train Accuracy: 0.9546, Validation Accuracy: 0.9677, Loss: 0.0192\n",
      "Epoch   8 Batch  660/1077 - Train Accuracy: 0.9867, Validation Accuracy: 0.9688, Loss: 0.0189\n",
      "Epoch   8 Batch  680/1077 - Train Accuracy: 0.9479, Validation Accuracy: 0.9762, Loss: 0.0213\n",
      "Epoch   8 Batch  700/1077 - Train Accuracy: 0.9762, Validation Accuracy: 0.9620, Loss: 0.0168\n",
      "Epoch   8 Batch  720/1077 - Train Accuracy: 0.9531, Validation Accuracy: 0.9634, Loss: 0.0196\n",
      "Epoch   8 Batch  740/1077 - Train Accuracy: 0.9625, Validation Accuracy: 0.9471, Loss: 0.0183\n",
      "Epoch   8 Batch  760/1077 - Train Accuracy: 0.9613, Validation Accuracy: 0.9631, Loss: 0.0194\n",
      "Epoch   8 Batch  780/1077 - Train Accuracy: 0.9539, Validation Accuracy: 0.9748, Loss: 0.0242\n",
      "Epoch   8 Batch  800/1077 - Train Accuracy: 0.9723, Validation Accuracy: 0.9645, Loss: 0.0144\n",
      "Epoch   8 Batch  820/1077 - Train Accuracy: 0.9535, Validation Accuracy: 0.9620, Loss: 0.0142\n",
      "Epoch   8 Batch  840/1077 - Train Accuracy: 0.9695, Validation Accuracy: 0.9663, Loss: 0.0192\n",
      "Epoch   8 Batch  860/1077 - Train Accuracy: 0.9706, Validation Accuracy: 0.9808, Loss: 0.0256\n",
      "Epoch   8 Batch  880/1077 - Train Accuracy: 0.9672, Validation Accuracy: 0.9748, Loss: 0.0271\n",
      "Epoch   8 Batch  900/1077 - Train Accuracy: 0.9656, Validation Accuracy: 0.9730, Loss: 0.0203\n",
      "Epoch   8 Batch  920/1077 - Train Accuracy: 0.9410, Validation Accuracy: 0.9609, Loss: 0.0222\n",
      "Epoch   8 Batch  940/1077 - Train Accuracy: 0.9695, Validation Accuracy: 0.9712, Loss: 0.0166\n",
      "Epoch   8 Batch  960/1077 - Train Accuracy: 0.9710, Validation Accuracy: 0.9638, Loss: 0.0164\n",
      "Epoch   8 Batch  980/1077 - Train Accuracy: 0.9418, Validation Accuracy: 0.9659, Loss: 0.0179\n",
      "Epoch   8 Batch 1000/1077 - Train Accuracy: 0.9587, Validation Accuracy: 0.9709, Loss: 0.0189\n",
      "Epoch   8 Batch 1020/1077 - Train Accuracy: 0.9734, Validation Accuracy: 0.9620, Loss: 0.0154\n",
      "Epoch   8 Batch 1040/1077 - Train Accuracy: 0.9642, Validation Accuracy: 0.9609, Loss: 0.0196\n",
      "Epoch   8 Batch 1060/1077 - Train Accuracy: 0.9684, Validation Accuracy: 0.9524, Loss: 0.0135\n",
      "Epoch   9 Batch   20/1077 - Train Accuracy: 0.9734, Validation Accuracy: 0.9773, Loss: 0.0155\n",
      "Epoch   9 Batch   40/1077 - Train Accuracy: 0.9594, Validation Accuracy: 0.9769, Loss: 0.0153\n",
      "Epoch   9 Batch   60/1077 - Train Accuracy: 0.9621, Validation Accuracy: 0.9482, Loss: 0.0139\n",
      "Epoch   9 Batch   80/1077 - Train Accuracy: 0.9641, Validation Accuracy: 0.9627, Loss: 0.0164\n",
      "Epoch   9 Batch  100/1077 - Train Accuracy: 0.9820, Validation Accuracy: 0.9769, Loss: 0.0167\n",
      "Epoch   9 Batch  120/1077 - Train Accuracy: 0.9789, Validation Accuracy: 0.9663, Loss: 0.0174\n",
      "Epoch   9 Batch  140/1077 - Train Accuracy: 0.9708, Validation Accuracy: 0.9613, Loss: 0.0211\n",
      "Epoch   9 Batch  160/1077 - Train Accuracy: 0.9531, Validation Accuracy: 0.9624, Loss: 0.0157\n",
      "Epoch   9 Batch  180/1077 - Train Accuracy: 0.9727, Validation Accuracy: 0.9702, Loss: 0.0128\n",
      "Epoch   9 Batch  200/1077 - Train Accuracy: 0.9586, Validation Accuracy: 0.9599, Loss: 0.0175\n",
      "Epoch   9 Batch  220/1077 - Train Accuracy: 0.9708, Validation Accuracy: 0.9577, Loss: 0.0174\n",
      "Epoch   9 Batch  240/1077 - Train Accuracy: 0.9652, Validation Accuracy: 0.9513, Loss: 0.0167\n",
      "Epoch   9 Batch  260/1077 - Train Accuracy: 0.9639, Validation Accuracy: 0.9691, Loss: 0.0124\n",
      "Epoch   9 Batch  280/1077 - Train Accuracy: 0.9348, Validation Accuracy: 0.9467, Loss: 0.0222\n",
      "Epoch   9 Batch  300/1077 - Train Accuracy: 0.9618, Validation Accuracy: 0.9421, Loss: 0.0178\n",
      "Epoch   9 Batch  320/1077 - Train Accuracy: 0.9719, Validation Accuracy: 0.9577, Loss: 0.0275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9 Batch  340/1077 - Train Accuracy: 0.9753, Validation Accuracy: 0.9645, Loss: 0.0196\n",
      "Epoch   9 Batch  360/1077 - Train Accuracy: 0.9723, Validation Accuracy: 0.9648, Loss: 0.0138\n",
      "Epoch   9 Batch  380/1077 - Train Accuracy: 0.9711, Validation Accuracy: 0.9648, Loss: 0.0132\n",
      "Epoch   9 Batch  400/1077 - Train Accuracy: 0.9539, Validation Accuracy: 0.9631, Loss: 0.0171\n",
      "Epoch   9 Batch  420/1077 - Train Accuracy: 0.9730, Validation Accuracy: 0.9425, Loss: 0.0110\n",
      "Epoch   9 Batch  440/1077 - Train Accuracy: 0.9555, Validation Accuracy: 0.9496, Loss: 0.0202\n",
      "Epoch   9 Batch  460/1077 - Train Accuracy: 0.9750, Validation Accuracy: 0.9627, Loss: 0.0187\n",
      "Epoch   9 Batch  480/1077 - Train Accuracy: 0.9498, Validation Accuracy: 0.9499, Loss: 0.0145\n",
      "Epoch   9 Batch  500/1077 - Train Accuracy: 0.9719, Validation Accuracy: 0.9432, Loss: 0.0216\n",
      "Epoch   9 Batch  520/1077 - Train Accuracy: 0.9769, Validation Accuracy: 0.9506, Loss: 0.0105\n",
      "Epoch   9 Batch  540/1077 - Train Accuracy: 0.9660, Validation Accuracy: 0.9478, Loss: 0.0152\n",
      "Epoch   9 Batch  560/1077 - Train Accuracy: 0.9500, Validation Accuracy: 0.9762, Loss: 0.0187\n",
      "Epoch   9 Batch  580/1077 - Train Accuracy: 0.9784, Validation Accuracy: 0.9542, Loss: 0.0118\n",
      "Epoch   9 Batch  600/1077 - Train Accuracy: 0.9531, Validation Accuracy: 0.9716, Loss: 0.0214\n",
      "Epoch   9 Batch  620/1077 - Train Accuracy: 0.9711, Validation Accuracy: 0.9719, Loss: 0.0208\n",
      "Epoch   9 Batch  640/1077 - Train Accuracy: 0.9695, Validation Accuracy: 0.9691, Loss: 0.0125\n",
      "Epoch   9 Batch  660/1077 - Train Accuracy: 0.9875, Validation Accuracy: 0.9680, Loss: 0.0164\n",
      "Epoch   9 Batch  680/1077 - Train Accuracy: 0.9591, Validation Accuracy: 0.9716, Loss: 0.0214\n",
      "Epoch   9 Batch  700/1077 - Train Accuracy: 0.9840, Validation Accuracy: 0.9670, Loss: 0.0162\n",
      "Epoch   9 Batch  720/1077 - Train Accuracy: 0.9576, Validation Accuracy: 0.9698, Loss: 0.0169\n",
      "Epoch   9 Batch  740/1077 - Train Accuracy: 0.9621, Validation Accuracy: 0.9606, Loss: 0.0151\n",
      "Epoch   9 Batch  760/1077 - Train Accuracy: 0.9781, Validation Accuracy: 0.9638, Loss: 0.0131\n",
      "Epoch   9 Batch  780/1077 - Train Accuracy: 0.9500, Validation Accuracy: 0.9613, Loss: 0.0190\n",
      "Epoch   9 Batch  800/1077 - Train Accuracy: 0.9488, Validation Accuracy: 0.9549, Loss: 0.0168\n",
      "Epoch   9 Batch  820/1077 - Train Accuracy: 0.9559, Validation Accuracy: 0.9641, Loss: 0.0159\n",
      "Epoch   9 Batch  840/1077 - Train Accuracy: 0.9668, Validation Accuracy: 0.9698, Loss: 0.0167\n",
      "Epoch   9 Batch  860/1077 - Train Accuracy: 0.9751, Validation Accuracy: 0.9691, Loss: 0.0172\n",
      "Epoch   9 Batch  880/1077 - Train Accuracy: 0.9777, Validation Accuracy: 0.9688, Loss: 0.0245\n",
      "Epoch   9 Batch  900/1077 - Train Accuracy: 0.9723, Validation Accuracy: 0.9709, Loss: 0.0184\n",
      "Epoch   9 Batch  920/1077 - Train Accuracy: 0.9707, Validation Accuracy: 0.9656, Loss: 0.0147\n",
      "Epoch   9 Batch  940/1077 - Train Accuracy: 0.9555, Validation Accuracy: 0.9670, Loss: 0.0179\n",
      "Epoch   9 Batch  960/1077 - Train Accuracy: 0.9784, Validation Accuracy: 0.9663, Loss: 0.0140\n",
      "Epoch   9 Batch  980/1077 - Train Accuracy: 0.9492, Validation Accuracy: 0.9751, Loss: 0.0195\n",
      "Epoch   9 Batch 1000/1077 - Train Accuracy: 0.9632, Validation Accuracy: 0.9716, Loss: 0.0139\n",
      "Epoch   9 Batch 1020/1077 - Train Accuracy: 0.9758, Validation Accuracy: 0.9599, Loss: 0.0139\n",
      "Epoch   9 Batch 1040/1077 - Train Accuracy: 0.9667, Validation Accuracy: 0.9581, Loss: 0.0176\n",
      "Epoch   9 Batch 1060/1077 - Train Accuracy: 0.9797, Validation Accuracy: 0.9627, Loss: 0.0100\n",
      "Epoch  10 Batch   20/1077 - Train Accuracy: 0.9699, Validation Accuracy: 0.9670, Loss: 0.0134\n",
      "Epoch  10 Batch   40/1077 - Train Accuracy: 0.9816, Validation Accuracy: 0.9790, Loss: 0.0122\n",
      "Epoch  10 Batch   60/1077 - Train Accuracy: 0.9773, Validation Accuracy: 0.9702, Loss: 0.0118\n",
      "Epoch  10 Batch   80/1077 - Train Accuracy: 0.9516, Validation Accuracy: 0.9695, Loss: 0.0152\n",
      "Epoch  10 Batch  100/1077 - Train Accuracy: 0.9789, Validation Accuracy: 0.9751, Loss: 0.0119\n",
      "Epoch  10 Batch  120/1077 - Train Accuracy: 0.9809, Validation Accuracy: 0.9684, Loss: 0.0177\n",
      "Epoch  10 Batch  140/1077 - Train Accuracy: 0.9831, Validation Accuracy: 0.9790, Loss: 0.0144\n",
      "Epoch  10 Batch  160/1077 - Train Accuracy: 0.9727, Validation Accuracy: 0.9624, Loss: 0.0148\n",
      "Epoch  10 Batch  180/1077 - Train Accuracy: 0.9758, Validation Accuracy: 0.9815, Loss: 0.0116\n",
      "Epoch  10 Batch  200/1077 - Train Accuracy: 0.9719, Validation Accuracy: 0.9680, Loss: 0.0131\n",
      "Epoch  10 Batch  220/1077 - Train Accuracy: 0.9803, Validation Accuracy: 0.9755, Loss: 0.0181\n",
      "Epoch  10 Batch  240/1077 - Train Accuracy: 0.9648, Validation Accuracy: 0.9727, Loss: 0.0167\n",
      "Epoch  10 Batch  260/1077 - Train Accuracy: 0.9513, Validation Accuracy: 0.9897, Loss: 0.0117\n",
      "Epoch  10 Batch  280/1077 - Train Accuracy: 0.9688, Validation Accuracy: 0.9595, Loss: 0.0173\n",
      "Epoch  10 Batch  300/1077 - Train Accuracy: 0.9733, Validation Accuracy: 0.9741, Loss: 0.0143\n",
      "Epoch  10 Batch  320/1077 - Train Accuracy: 0.9609, Validation Accuracy: 0.9581, Loss: 0.0210\n",
      "Epoch  10 Batch  340/1077 - Train Accuracy: 0.9663, Validation Accuracy: 0.9513, Loss: 0.0176\n",
      "Epoch  10 Batch  360/1077 - Train Accuracy: 0.9641, Validation Accuracy: 0.9563, Loss: 0.0103\n",
      "Epoch  10 Batch  380/1077 - Train Accuracy: 0.9766, Validation Accuracy: 0.9545, Loss: 0.0132\n",
      "Epoch  10 Batch  400/1077 - Train Accuracy: 0.9727, Validation Accuracy: 0.9805, Loss: 0.0114\n",
      "Epoch  10 Batch  420/1077 - Train Accuracy: 0.9922, Validation Accuracy: 0.9688, Loss: 0.0093\n",
      "Epoch  10 Batch  440/1077 - Train Accuracy: 0.9723, Validation Accuracy: 0.9542, Loss: 0.0173\n",
      "Epoch  10 Batch  460/1077 - Train Accuracy: 0.9754, Validation Accuracy: 0.9695, Loss: 0.0152\n",
      "Epoch  10 Batch  480/1077 - Train Accuracy: 0.9774, Validation Accuracy: 0.9585, Loss: 0.0130\n",
      "Epoch  10 Batch  500/1077 - Train Accuracy: 0.9563, Validation Accuracy: 0.9606, Loss: 0.0112\n",
      "Epoch  10 Batch  520/1077 - Train Accuracy: 0.9993, Validation Accuracy: 0.9705, Loss: 0.0089\n",
      "Epoch  10 Batch  540/1077 - Train Accuracy: 0.9641, Validation Accuracy: 0.9670, Loss: 0.0148\n",
      "Epoch  10 Batch  560/1077 - Train Accuracy: 0.9574, Validation Accuracy: 0.9780, Loss: 0.0117\n",
      "Epoch  10 Batch  580/1077 - Train Accuracy: 0.9635, Validation Accuracy: 0.9691, Loss: 0.0120\n",
      "Epoch  10 Batch  600/1077 - Train Accuracy: 0.9613, Validation Accuracy: 0.9656, Loss: 0.0209\n",
      "Epoch  10 Batch  620/1077 - Train Accuracy: 0.9715, Validation Accuracy: 0.9652, Loss: 0.0170\n",
      "Epoch  10 Batch  640/1077 - Train Accuracy: 0.9803, Validation Accuracy: 0.9851, Loss: 0.0115\n",
      "Epoch  10 Batch  660/1077 - Train Accuracy: 0.9852, Validation Accuracy: 0.9656, Loss: 0.0094\n",
      "Epoch  10 Batch  680/1077 - Train Accuracy: 0.9472, Validation Accuracy: 0.9620, Loss: 0.0149\n",
      "Epoch  10 Batch  700/1077 - Train Accuracy: 0.9676, Validation Accuracy: 0.9727, Loss: 0.0142\n",
      "Epoch  10 Batch  720/1077 - Train Accuracy: 0.9762, Validation Accuracy: 0.9727, Loss: 0.0130\n",
      "Epoch  10 Batch  740/1077 - Train Accuracy: 0.9703, Validation Accuracy: 0.9790, Loss: 0.0134\n",
      "Epoch  10 Batch  760/1077 - Train Accuracy: 0.9680, Validation Accuracy: 0.9648, Loss: 0.0168\n",
      "Epoch  10 Batch  780/1077 - Train Accuracy: 0.9578, Validation Accuracy: 0.9673, Loss: 0.0221\n",
      "Epoch  10 Batch  800/1077 - Train Accuracy: 0.9641, Validation Accuracy: 0.9730, Loss: 0.0116\n",
      "Epoch  10 Batch  820/1077 - Train Accuracy: 0.9762, Validation Accuracy: 0.9702, Loss: 0.0121\n",
      "Epoch  10 Batch  840/1077 - Train Accuracy: 0.9645, Validation Accuracy: 0.9659, Loss: 0.0159\n",
      "Epoch  10 Batch  860/1077 - Train Accuracy: 0.9754, Validation Accuracy: 0.9680, Loss: 0.0135\n",
      "Epoch  10 Batch  880/1077 - Train Accuracy: 0.9758, Validation Accuracy: 0.9677, Loss: 0.0196\n",
      "Epoch  10 Batch  900/1077 - Train Accuracy: 0.9688, Validation Accuracy: 0.9602, Loss: 0.0149\n",
      "Epoch  10 Batch  920/1077 - Train Accuracy: 0.9766, Validation Accuracy: 0.9812, Loss: 0.0133\n",
      "Epoch  10 Batch  940/1077 - Train Accuracy: 0.9625, Validation Accuracy: 0.9798, Loss: 0.0129\n",
      "Epoch  10 Batch  960/1077 - Train Accuracy: 0.9847, Validation Accuracy: 0.9719, Loss: 0.0133\n",
      "Epoch  10 Batch  980/1077 - Train Accuracy: 0.9594, Validation Accuracy: 0.9808, Loss: 0.0148\n",
      "Epoch  10 Batch 1000/1077 - Train Accuracy: 0.9680, Validation Accuracy: 0.9705, Loss: 0.0162\n",
      "Epoch  10 Batch 1020/1077 - Train Accuracy: 0.9801, Validation Accuracy: 0.9819, Loss: 0.0136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10 Batch 1040/1077 - Train Accuracy: 0.9663, Validation Accuracy: 0.9695, Loss: 0.0169\n",
      "Epoch  10 Batch 1060/1077 - Train Accuracy: 0.9730, Validation Accuracy: 0.9741, Loss: 0.0119\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "modelTrainer = ModelTrainer()\n",
    "\n",
    "modelTrainer.train_seq2seq_model(rnn, train_graph) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickleHelper.save_params(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(_, _, \n",
    "(source_vocab_to_int, target_vocab_to_int), \n",
    "(source_int_to_vocab, target_int_to_vocab)) = pickleHelper.load_preprocessed_data()\n",
    "\n",
    "load_path = pickleHelper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TranslationChecker:\n",
    "    \n",
    "    def check_translation(self, \n",
    "                          checkpoint, \n",
    "                          sentence, \n",
    "                          source_vocab_to_int,\n",
    "                          source_int_to_vocab,\n",
    "                          target_int_to_vocab):\n",
    "        \"\"\"\n",
    "        Check translation of a sample sentence.\n",
    "        \n",
    "        :param checkpoint: Checkpoint\n",
    "        :param sentence: Input sentence\n",
    "        :param source_vocab_to_int: Dictionary of source vocab to ids\n",
    "        :param source_int_to_vocab: Dictionary of source ids to words\n",
    "        :param target_int_to_vocab: Dictionary of target ids to words\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert input sentence into int seq\n",
    "        inputSentencePreparer = InputSentencePreparer()\n",
    "        input_seq = inputSentencePreparer.sentence_to_seq(sentence, source_vocab_to_int)\n",
    "        \n",
    "        # Get translation logits\n",
    "        translation_logits = self.get_translation_logits(checkpoint, input_seq)\n",
    "        \n",
    "        # Print translation\n",
    "        self.print_translation(input_seq, \n",
    "                               translation_logits, \n",
    "                               source_int_to_vocab, \n",
    "                               target_int_to_vocab)\n",
    "        \n",
    "        \n",
    "    def get_translation_logits(self, checkpoint, input_seq):\n",
    "        \"\"\"\n",
    "        Load saved model and get output logits.\n",
    "        \n",
    "        :param checkpoint: Checkpoint\n",
    "        :param input_seq: Input sequence\n",
    "        \"\"\"\n",
    "        loaded_graph = tf.Graph()\n",
    "        \n",
    "        with tf.Session(graph=loaded_graph) as sess:\n",
    "            # Load saved model\n",
    "            loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "            loader.restore(sess, checkpoint)\n",
    "            \n",
    "            # Load tensors\n",
    "            inputs = loaded_graph.get_tensor_by_name('input:0')\n",
    "            logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "            source_seq_len = loaded_graph.get_tensor_by_name('source_seq_len:0')\n",
    "            target_seq_len = loaded_graph.get_tensor_by_name('target_seq_len:0')\n",
    "            keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "            \n",
    "            # Get translation logits\n",
    "            feed = {inputs: [input_seq]*batch_size,\n",
    "                    source_seq_len: [len(input_seq)]*batch_size,\n",
    "                    target_seq_len: [len(input_seq)*2]*batch_size,\n",
    "                    keep_prob: 1.0}\n",
    "            \n",
    "            translation_logits = sess.run(logits,\n",
    "                                          feed_dict=feed)[0]\n",
    "            \n",
    "            return translation_logits\n",
    "\n",
    "    \n",
    "    def print_translation(self, \n",
    "                          input_seq, \n",
    "                          logits, \n",
    "                          source_int_to_vocab, \n",
    "                          target_int_to_vocab):\n",
    "        \"\"\"\n",
    "        Print translation of input sentence.\n",
    "        \n",
    "        :param input_seq: Int sequence corresponding to input sentence\n",
    "        :param logits: Translation logits\n",
    "        :param source_int_to_vocab: Dictionary of source ids to words\n",
    "        :param target_int_to_vocab: Dictionary of target ids to words\n",
    "        \"\"\"\n",
    "        \n",
    "        print('Input')\n",
    "        print('  Word Ids:      {}'.format([i for i in input_seq]))\n",
    "        print('  English Words: {}'.format([source_int_to_vocab[i] for i in input_seq]))\n",
    "\n",
    "        print('\\nPrediction')\n",
    "        print('  Word Ids:      {}'.format([i for i in logits]))\n",
    "        print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InputSentencePreparer:\n",
    "    \n",
    "    def sentence_to_seq(self, sentence, vocab_to_int):\n",
    "        \"\"\"\n",
    "        Convert a sentence to a sequence of ids.\n",
    "        \n",
    "        :param sentence: String\n",
    "        :param vocab_to_int: Dictionary to go from the words to an id\n",
    "        :return: List of word ids\n",
    "        \"\"\"\n",
    "        words = sentence.lower().split(' ')\n",
    "        return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'he saw a old yellow truck .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [70, 153, 51, 55, 12, 140, 224]\n",
      "  English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [16, 21, 299, 81, 190, 97, 104, 1, 0, 0, 0, 0, 0, 0]\n",
      "  French Words: il a vu un vieux camion . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "translationChecker = TranslationChecker()\n",
    "\n",
    "translationChecker.check_translation(load_path, \n",
    "                                     sentence, \n",
    "                                     source_vocab_to_int, \n",
    "                                     source_int_to_vocab,\n",
    "                                     target_int_to_vocab)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
