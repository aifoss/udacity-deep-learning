{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def load_data(self, path):\n",
    "        \"\"\"\n",
    "        Load Dataset from File\n",
    "        \"\"\"\n",
    " \n",
    "        input_file = os.path.join(path)\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './data/simpsons/moes_tavern_lines.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataLoader = DataLoader()\n",
    "\n",
    "text = dataLoader.load_data(data_dir)\n",
    "\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataExplorer:\n",
    "    \n",
    "    def explore_data(self, text, view_sentence_range):\n",
    "        \"\"\"\n",
    "        Explore input data text.\n",
    "        \n",
    "        :param text: Input text to explore\n",
    "        :param view_sentence_range: Range of sentences to display\n",
    "        \"\"\"\n",
    "        \n",
    "        print('Dataset Stats')\n",
    "        print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "        scenes = text.split('\\n\\n')\n",
    "        print('Number of scenes: {}'.format(len(scenes)))\n",
    "        sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "        print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "        sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "        print('Number of lines: {}'.format(len(sentences)))\n",
    "        word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "        print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "        print()\n",
    "        print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "        print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "view_sentence_range = (0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 10:\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataExplorer = DataExplorer()\n",
    "\n",
    "dataExplorer.explore_data(text, view_sentence_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \n",
    "    def preprocess_and_save_data(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess and save text data\n",
    "        \n",
    "        :param text: The text of tv scripts split into words\n",
    "        \"\"\"\n",
    "\n",
    "        punc_dict = self.create_punc_lookup_table()\n",
    "        \n",
    "        for key, token in punc_dict.items():\n",
    "            text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "        text = text.lower()\n",
    "        text = text.split()\n",
    "\n",
    "        vocab_to_int, int_to_vocab = self.create_lookup_tables(text)\n",
    "        int_text = [vocab_to_int[word] for word in text]\n",
    "        \n",
    "        PickleHelper().save_preprocessed_data((int_text, vocab_to_int, int_to_vocab, punc_dict))\n",
    "        \n",
    "    \n",
    "    def create_lookup_tables(self, text):\n",
    "        \"\"\"\n",
    "        Create lookup tables for vocabulary\n",
    "        \n",
    "        :param text: The text of tv scripts split into words\n",
    "        :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "        \"\"\"\n",
    "        \n",
    "        word_counts = Counter(text)\n",
    "        sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "        vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "        \n",
    "        return (vocab_to_int, int_to_vocab)\n",
    "    \n",
    "    \n",
    "    def create_punc_lookup_table(self):\n",
    "        \"\"\"\n",
    "        Generate a dict to turn punctuation into a token.\n",
    "        \n",
    "        :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "        \"\"\"\n",
    "        \n",
    "        punc_dict = {}\n",
    "        punc_dict['.']  = \"||Period||\"\n",
    "        punc_dict[',']  = \"||Comma||\"\n",
    "        punc_dict['\"']  = \"||Quotation_Mark||\"\n",
    "        punc_dict[';']  = \"||Semicolon||\"\n",
    "        punc_dict['!']  = \"||Exclamation_Mark||\"\n",
    "        punc_dict['?']  = \"||Question_Mark\"\n",
    "        punc_dict['(']  = \"||Left_Parenthesis||\"\n",
    "        punc_dict[')']  = \"||Right_Parenthesis||\"\n",
    "        punc_dict['--'] = \"||Dash||\"\n",
    "        punc_dict['\\n'] = \"||Return||\"\n",
    "\n",
    "        return punc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PickleHelper:\n",
    "    \n",
    "    def save_preprocessed_data(self, data):\n",
    "        \"\"\"\n",
    "        Save preprocessed training data.\n",
    "        \"\"\"\n",
    "        pickle.dump(data, open('preprocess.p', 'wb'))\n",
    "        \n",
    "    def load_preprocessed_data(self):\n",
    "        \"\"\"\n",
    "        Load the Preprocessed training data and return them in batches of <batch_size> or less\n",
    "        \"\"\"\n",
    "        return pickle.load(open('preprocess.p', mode='rb'))\n",
    "    \n",
    "    def save_params(self, params):\n",
    "        \"\"\"\n",
    "        Save parameters to file\n",
    "        \"\"\"\n",
    "        pickle.dump(params, open('params.p', 'wb'))\n",
    "    \n",
    "    def load_params(self):\n",
    "        \"\"\"\n",
    "        Load parameters from file\n",
    "        \"\"\"\n",
    "        return pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPreprocessor = DataPreprocessor()\n",
    "\n",
    "dataPreprocessor.preprocess_and_save_data(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = PickleHelper().load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Tensorflow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorflowVersionChecker:\n",
    "    \n",
    "    def check_version(self):\n",
    "        # Check tensorflow version\n",
    "        assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), \\\n",
    "            'Please use TensorFlow version 1.0 or newer'\n",
    "        print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "        # Check for a GPU\n",
    "        if not tf.test.gpu_device_name():\n",
    "            warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "        else:\n",
    "            print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "tfVersionChecker = TensorflowVersionChecker()\n",
    "\n",
    "tfVersionChecker.check_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_text = None\n",
    "        self.targets = None\n",
    "        self.lr = None\n",
    "        self.initial_state = None\n",
    "        self.final_state = None\n",
    "        self.cost = None\n",
    "        self.train_op = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNBuilder:\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        \"\"\"\n",
    "        Create TF Placeholders for input, targets, and learning rate.\n",
    "        \n",
    "        :return: Tuple (input, targets, learning rate)\n",
    "        \"\"\"\n",
    "        inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "        learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        return (inputs, targets, learning_rate)\n",
    "    \n",
    "    \n",
    "    def build_init_cell(self, batch_size, rnn_size, num_rnn_layers, keep_prob):\n",
    "        \"\"\"\n",
    "        Create an RNN Cell and initialize it.\n",
    "        \n",
    "        :param batch_size: Size of batches\n",
    "        :param rnn_size: Size of RNNs\n",
    "        :param num_rnn_layers: Number of RNN (LSTM) layers\n",
    "        :param keep_prob: Keep probability value\n",
    "        :return: Tuple (cell, initialize state)\n",
    "        \"\"\"\n",
    "        \n",
    "        cell = tf.contrib.rnn.MultiRNNCell(\n",
    "                [self.build_lstm_cell(rnn_size, keep_prob) for _ in range(num_rnn_layers)])\n",
    "\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        initial_state = tf.identity(initial_state, name='initial_state')\n",
    "\n",
    "        return (cell, initial_state)\n",
    "    \n",
    "    \n",
    "    def build_lstm_cell(self, rnn_size, keep_prob):\n",
    "        \"\"\" \n",
    "        Build LSTM cell and apply dropout.\n",
    "        :param rnn_size: Size of RNNs\n",
    "        :param keep_prob: Keep probability value\n",
    "        \"\"\"\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    def get_embed(self, input_data, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Create embedding for <input_data>.\n",
    "        \n",
    "        :param input_data: TF placeholder for text input\n",
    "        :param vocab_size: Number of words in vocabulary\n",
    "        :param embed_dim: Number of embedding dimensions\n",
    "        :return: Embedded input.\n",
    "        \"\"\"        \n",
    "        embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "        return embed\n",
    "    \n",
    "    \n",
    "    def build_rnn(self, cell, inputs):\n",
    "        \"\"\"\n",
    "        Create a RNN using a RNN Cell\n",
    "        \n",
    "        :param cell: RNN Cell\n",
    "        :param inputs: Input text data\n",
    "        :return: Tuple (Outputs, Final State)\n",
    "        \"\"\"\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "        final_state = tf.identity(final_state, name='final_state')\n",
    "        return (outputs, final_state)\n",
    "    \n",
    "    \n",
    "    def build_nn(self, cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Build part of the neural network\n",
    "        \n",
    "        :param cell: RNN cell\n",
    "        :param rnn_size: Size of rnns\n",
    "        :param input_data: Input data\n",
    "        :param vocab_size: Vocabulary size\n",
    "        :param embed_dim: Number of embedding dimensions\n",
    "        :return: Tuple (Logits, FinalState)\n",
    "        \"\"\"\n",
    "        \n",
    "        embed = self.get_embed(input_data, vocab_size, embed_dim)\n",
    "        outputs, final_state = self.build_rnn(cell, embed)\n",
    "        \n",
    "        logits = tf.contrib.layers.fully_connected(inputs=outputs,\n",
    "                                                   num_outputs=vocab_size,\n",
    "                                                   activation_fn=None)\n",
    "        \n",
    "        return (logits, final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNGraphBuilder:\n",
    "    \n",
    "    def build_rnn_graph(self, \n",
    "                        int_to_vocab, \n",
    "                        rnn_size,\n",
    "                        num_rnn_layers,\n",
    "                        keep_prob,\n",
    "                        embed_dim):\n",
    "        \n",
    "        \"\"\"\n",
    "        Build RNN graph.\n",
    "        \n",
    "        :param int_to_vocab: Mapping of input words\n",
    "        :param rnn_size: Size of rnns\n",
    "        :param num_rnn_layers: Number of RNN (LSTM) layers\n",
    "        :param keep_prob: Keep probability value\n",
    "        :param embed_dim: Number of embedding dimensions\n",
    "        :return: Tuple (RNN, TrainGraph, Probs) \n",
    "        \"\"\"\n",
    "        \n",
    "        train_graph = tf.Graph()\n",
    "        rnn = RNN()\n",
    "        rnnBuilder = RNNBuilder()\n",
    "\n",
    "        with train_graph.as_default():\n",
    "            vocab_size = len(int_to_vocab)\n",
    "            \n",
    "            input_text, targets, lr = rnnBuilder.create_placeholders()\n",
    "            input_data_shape = tf.shape(input_text)\n",
    "            \n",
    "            rnn.input_text = input_text\n",
    "            rnn.targets = targets\n",
    "            rnn.lr = lr\n",
    "            \n",
    "            cell, initial_state = rnnBuilder.build_init_cell(input_data_shape[0], \n",
    "                                                             rnn_size, \n",
    "                                                             num_rnn_layers, \n",
    "                                                             keep_prob)\n",
    "            \n",
    "            rnn.initial_state = initial_state\n",
    " \n",
    "            logits, final_state = rnnBuilder.build_nn(cell, \n",
    "                                                      rnn_size, \n",
    "                                                      input_text, \n",
    "                                                      vocab_size, \n",
    "                                                      embed_dim)\n",
    "    \n",
    "            rnn.final_state = final_state\n",
    "\n",
    "            # Probabilities for generating words\n",
    "            probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "            # Loss function\n",
    "            cost = seq2seq.sequence_loss(logits,\n",
    "                                         targets, \n",
    "                                         tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "            # Optimizer\n",
    "            optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "            # Gradient Clipping\n",
    "            gradients = optimizer.compute_gradients(cost)\n",
    "            capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) \\\n",
    "                                for grad, var in gradients if grad is not None]\n",
    "            train_op = optimizer.apply_gradients(capped_gradients)\n",
    "            \n",
    "            rnn.cost = cost\n",
    "            rnn.train_op = train_op\n",
    "            \n",
    "            return (rnn, train_graph, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_size = 256\n",
    "num_rnn_layers = 2\n",
    "keep_prob = 0.75\n",
    "embed_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnnGraphBuilder = RNNGraphBuilder()\n",
    "\n",
    "rnn, train_graph, probs = rnnGraphBuilder.build_rnn_graph(int_to_vocab, \n",
    "                                                          rnn_size,\n",
    "                                                          num_rnn_layers,\n",
    "                                                          keep_prob,\n",
    "                                                          embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    \n",
    "    def train_rnn(self, rnn, train_graph):\n",
    "        \"\"\"\n",
    "        Train and save RNN model.\n",
    "        \n",
    "        :param rnn: RNN to train and save\n",
    "        :train_graph: RNN graph\n",
    "        \"\"\"\n",
    "            \n",
    "        with tf.Session(graph=train_graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            dataBatchGenerator = DataBatchGenerator()\n",
    "        \n",
    "            batches = dataBatchGenerator.get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "            for epoch_i in range(num_epochs):\n",
    "                feed = {rnn.input_text: batches[0][0]}\n",
    "                \n",
    "                state = sess.run(rnn.initial_state, feed_dict=feed)\n",
    "\n",
    "                for batch_i, (x, y) in enumerate(batches):\n",
    "                    feed = {\n",
    "                        rnn.input_text: x,\n",
    "                        rnn.targets: y,\n",
    "                        rnn.initial_state: state,\n",
    "                        rnn.lr: learning_rate}\n",
    "                    \n",
    "                    train_loss, state, _ = sess.run([rnn.cost, rnn.final_state, rnn.train_op],\n",
    "                                                     feed_dict=feed)\n",
    "\n",
    "                    # Show every <show_every_n_batches> batches\n",
    "                    if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                        print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                                epoch_i+1,\n",
    "                                batch_i+1,\n",
    "                                len(batches),\n",
    "                                train_loss))\n",
    "\n",
    "            # Save Model\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, save_dir)\n",
    "            \n",
    "            print('\\nModel Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataBatchGenerator:\n",
    "\n",
    "    def get_batches(self, int_text, batch_size, seq_len):\n",
    "        \"\"\"\n",
    "        Return batches of input and target\n",
    "        \n",
    "        :param int_text: Text with the words replaced by their ids\n",
    "        :param batch_size: The size of batch\n",
    "        :param seq_len: The length of sequence\n",
    "        :return: Batches as a Numpy array\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Example Input:\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)\n",
    "        \n",
    "        Example Output:\n",
    "        [\n",
    "          # First Batch\n",
    "          [\n",
    "            # Batch of Input\n",
    "            [[ 1  2], [ 7  8], [13 14]]\n",
    "            # Batch of targets\n",
    "            [[ 2  3], [ 8  9], [14 15]]\n",
    "          ]\n",
    "\n",
    "          # Second Batch\n",
    "          [\n",
    "            # Batch of Input\n",
    "            [[ 3  4], [ 9 10], [15 16]]\n",
    "            # Batch of targets\n",
    "            [[ 4  5], [10 11], [16 17]]\n",
    "          ]\n",
    "\n",
    "          # Third Batch\n",
    "          [\n",
    "            # Batch of Input\n",
    "            [[ 5  6], [11 12], [17 18]]\n",
    "            # Batch of targets\n",
    "            [[ 6  7], [12 13], [18  1]]\n",
    "          ]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        \n",
    "        n_chars_per_batch = batch_size * seq_len\n",
    "        n_batches = int(len(int_text) / n_chars_per_batch)\n",
    "        full_batch_size = n_batches * n_chars_per_batch\n",
    "\n",
    "        # Drop the last few characters to make only full batches\n",
    "        x_data = np.array(int_text[: full_batch_size])\n",
    "        y_data = np.array(int_text[1: full_batch_size + 1])\n",
    "\n",
    "        x_batches = np.split(x_data.reshape(batch_size, -1), n_batches, 1)\n",
    "        y_batches = np.split(y_data.reshape(batch_size, -1), n_batches, 1)\n",
    "\n",
    "        first_x_char = x_batches[0][0][0]\n",
    "        y_batches[n_batches-1][batch_size-1][-1] = first_x_char\n",
    "\n",
    "        return np.array(list(zip(x_batches, y_batches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "seq_length = 2\n",
    "show_every_n_batches = 10\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch    1/269   train_loss = 8.821\n",
      "Epoch   1 Batch   11/269   train_loss = 8.749\n",
      "Epoch   1 Batch   21/269   train_loss = 8.129\n",
      "Epoch   1 Batch   31/269   train_loss = 6.992\n",
      "Epoch   1 Batch   41/269   train_loss = 7.054\n",
      "Epoch   1 Batch   51/269   train_loss = 7.123\n",
      "Epoch   1 Batch   61/269   train_loss = 6.533\n",
      "Epoch   1 Batch   71/269   train_loss = 6.295\n",
      "Epoch   1 Batch   81/269   train_loss = 6.014\n",
      "Epoch   1 Batch   91/269   train_loss = 6.140\n",
      "Epoch   1 Batch  101/269   train_loss = 6.347\n",
      "Epoch   1 Batch  111/269   train_loss = 5.768\n",
      "Epoch   1 Batch  121/269   train_loss = 5.789\n",
      "Epoch   1 Batch  131/269   train_loss = 5.844\n",
      "Epoch   1 Batch  141/269   train_loss = 6.063\n",
      "Epoch   1 Batch  151/269   train_loss = 5.925\n",
      "Epoch   1 Batch  161/269   train_loss = 5.983\n",
      "Epoch   1 Batch  171/269   train_loss = 5.953\n",
      "Epoch   1 Batch  181/269   train_loss = 5.805\n",
      "Epoch   1 Batch  191/269   train_loss = 5.557\n",
      "Epoch   1 Batch  201/269   train_loss = 5.711\n",
      "Epoch   1 Batch  211/269   train_loss = 5.312\n",
      "Epoch   1 Batch  221/269   train_loss = 5.550\n",
      "Epoch   1 Batch  231/269   train_loss = 5.516\n",
      "Epoch   1 Batch  241/269   train_loss = 5.762\n",
      "Epoch   1 Batch  251/269   train_loss = 5.798\n",
      "Epoch   1 Batch  261/269   train_loss = 5.735\n",
      "Epoch   2 Batch    2/269   train_loss = 5.381\n",
      "Epoch   2 Batch   12/269   train_loss = 5.349\n",
      "Epoch   2 Batch   22/269   train_loss = 5.395\n",
      "Epoch   2 Batch   32/269   train_loss = 5.503\n",
      "Epoch   2 Batch   42/269   train_loss = 5.332\n",
      "Epoch   2 Batch   52/269   train_loss = 4.947\n",
      "Epoch   2 Batch   62/269   train_loss = 5.079\n",
      "Epoch   2 Batch   72/269   train_loss = 5.440\n",
      "Epoch   2 Batch   82/269   train_loss = 5.324\n",
      "Epoch   2 Batch   92/269   train_loss = 5.507\n",
      "Epoch   2 Batch  102/269   train_loss = 5.435\n",
      "Epoch   2 Batch  112/269   train_loss = 5.300\n",
      "Epoch   2 Batch  122/269   train_loss = 5.367\n",
      "Epoch   2 Batch  132/269   train_loss = 5.045\n",
      "Epoch   2 Batch  142/269   train_loss = 5.109\n",
      "Epoch   2 Batch  152/269   train_loss = 5.093\n",
      "Epoch   2 Batch  162/269   train_loss = 5.206\n",
      "Epoch   2 Batch  172/269   train_loss = 5.305\n",
      "Epoch   2 Batch  182/269   train_loss = 5.208\n",
      "Epoch   2 Batch  192/269   train_loss = 4.941\n",
      "Epoch   2 Batch  202/269   train_loss = 5.745\n",
      "Epoch   2 Batch  212/269   train_loss = 5.377\n",
      "Epoch   2 Batch  222/269   train_loss = 5.104\n",
      "Epoch   2 Batch  232/269   train_loss = 5.144\n",
      "Epoch   2 Batch  242/269   train_loss = 5.316\n",
      "Epoch   2 Batch  252/269   train_loss = 5.024\n",
      "Epoch   2 Batch  262/269   train_loss = 5.423\n",
      "Epoch   3 Batch    3/269   train_loss = 4.735\n",
      "Epoch   3 Batch   13/269   train_loss = 5.017\n",
      "Epoch   3 Batch   23/269   train_loss = 5.145\n",
      "Epoch   3 Batch   33/269   train_loss = 5.020\n",
      "Epoch   3 Batch   43/269   train_loss = 4.840\n",
      "Epoch   3 Batch   53/269   train_loss = 4.777\n",
      "Epoch   3 Batch   63/269   train_loss = 5.157\n",
      "Epoch   3 Batch   73/269   train_loss = 4.862\n",
      "Epoch   3 Batch   83/269   train_loss = 5.068\n",
      "Epoch   3 Batch   93/269   train_loss = 5.312\n",
      "Epoch   3 Batch  103/269   train_loss = 5.102\n",
      "Epoch   3 Batch  113/269   train_loss = 5.153\n",
      "Epoch   3 Batch  123/269   train_loss = 5.103\n",
      "Epoch   3 Batch  133/269   train_loss = 5.199\n",
      "Epoch   3 Batch  143/269   train_loss = 4.905\n",
      "Epoch   3 Batch  153/269   train_loss = 5.468\n",
      "Epoch   3 Batch  163/269   train_loss = 4.950\n",
      "Epoch   3 Batch  173/269   train_loss = 4.594\n",
      "Epoch   3 Batch  183/269   train_loss = 4.583\n",
      "Epoch   3 Batch  193/269   train_loss = 4.801\n",
      "Epoch   3 Batch  203/269   train_loss = 4.903\n",
      "Epoch   3 Batch  213/269   train_loss = 5.020\n",
      "Epoch   3 Batch  223/269   train_loss = 4.867\n",
      "Epoch   3 Batch  233/269   train_loss = 4.898\n",
      "Epoch   3 Batch  243/269   train_loss = 4.879\n",
      "Epoch   3 Batch  253/269   train_loss = 5.282\n",
      "Epoch   3 Batch  263/269   train_loss = 4.628\n",
      "Epoch   4 Batch    4/269   train_loss = 4.812\n",
      "Epoch   4 Batch   14/269   train_loss = 4.945\n",
      "Epoch   4 Batch   24/269   train_loss = 5.298\n",
      "Epoch   4 Batch   34/269   train_loss = 4.966\n",
      "Epoch   4 Batch   44/269   train_loss = 4.920\n",
      "Epoch   4 Batch   54/269   train_loss = 4.885\n",
      "Epoch   4 Batch   64/269   train_loss = 5.009\n",
      "Epoch   4 Batch   74/269   train_loss = 4.725\n",
      "Epoch   4 Batch   84/269   train_loss = 4.704\n",
      "Epoch   4 Batch   94/269   train_loss = 4.793\n",
      "Epoch   4 Batch  104/269   train_loss = 4.558\n",
      "Epoch   4 Batch  114/269   train_loss = 4.823\n",
      "Epoch   4 Batch  124/269   train_loss = 4.884\n",
      "Epoch   4 Batch  134/269   train_loss = 4.633\n",
      "Epoch   4 Batch  144/269   train_loss = 4.733\n",
      "Epoch   4 Batch  154/269   train_loss = 4.961\n",
      "Epoch   4 Batch  164/269   train_loss = 4.552\n",
      "Epoch   4 Batch  174/269   train_loss = 4.626\n",
      "Epoch   4 Batch  184/269   train_loss = 4.871\n",
      "Epoch   4 Batch  194/269   train_loss = 4.646\n",
      "Epoch   4 Batch  204/269   train_loss = 4.942\n",
      "Epoch   4 Batch  214/269   train_loss = 4.769\n",
      "Epoch   4 Batch  224/269   train_loss = 4.766\n",
      "Epoch   4 Batch  234/269   train_loss = 4.337\n",
      "Epoch   4 Batch  244/269   train_loss = 4.601\n",
      "Epoch   4 Batch  254/269   train_loss = 4.728\n",
      "Epoch   4 Batch  264/269   train_loss = 4.964\n",
      "Epoch   5 Batch    5/269   train_loss = 4.458\n",
      "Epoch   5 Batch   15/269   train_loss = 4.710\n",
      "Epoch   5 Batch   25/269   train_loss = 4.345\n",
      "Epoch   5 Batch   35/269   train_loss = 4.690\n",
      "Epoch   5 Batch   45/269   train_loss = 4.925\n",
      "Epoch   5 Batch   55/269   train_loss = 4.561\n",
      "Epoch   5 Batch   65/269   train_loss = 4.360\n",
      "Epoch   5 Batch   75/269   train_loss = 4.714\n",
      "Epoch   5 Batch   85/269   train_loss = 4.614\n",
      "Epoch   5 Batch   95/269   train_loss = 4.761\n",
      "Epoch   5 Batch  105/269   train_loss = 4.353\n",
      "Epoch   5 Batch  115/269   train_loss = 4.601\n",
      "Epoch   5 Batch  125/269   train_loss = 4.610\n",
      "Epoch   5 Batch  135/269   train_loss = 4.643\n",
      "Epoch   5 Batch  145/269   train_loss = 4.137\n",
      "Epoch   5 Batch  155/269   train_loss = 4.470\n",
      "Epoch   5 Batch  165/269   train_loss = 4.450\n",
      "Epoch   5 Batch  175/269   train_loss = 5.023\n",
      "Epoch   5 Batch  185/269   train_loss = 4.515\n",
      "Epoch   5 Batch  195/269   train_loss = 4.550\n",
      "Epoch   5 Batch  205/269   train_loss = 4.428\n",
      "Epoch   5 Batch  215/269   train_loss = 4.468\n",
      "Epoch   5 Batch  225/269   train_loss = 4.374\n",
      "Epoch   5 Batch  235/269   train_loss = 4.400\n",
      "Epoch   5 Batch  245/269   train_loss = 4.527\n",
      "Epoch   5 Batch  255/269   train_loss = 4.784\n",
      "Epoch   5 Batch  265/269   train_loss = 4.744\n",
      "Epoch   6 Batch    6/269   train_loss = 4.165\n",
      "Epoch   6 Batch   16/269   train_loss = 4.303\n",
      "Epoch   6 Batch   26/269   train_loss = 4.291\n",
      "Epoch   6 Batch   36/269   train_loss = 4.251\n",
      "Epoch   6 Batch   46/269   train_loss = 4.881\n",
      "Epoch   6 Batch   56/269   train_loss = 4.364\n",
      "Epoch   6 Batch   66/269   train_loss = 4.537\n",
      "Epoch   6 Batch   76/269   train_loss = 4.674\n",
      "Epoch   6 Batch   86/269   train_loss = 4.504\n",
      "Epoch   6 Batch   96/269   train_loss = 4.651\n",
      "Epoch   6 Batch  106/269   train_loss = 4.165\n",
      "Epoch   6 Batch  116/269   train_loss = 4.799\n",
      "Epoch   6 Batch  126/269   train_loss = 4.432\n",
      "Epoch   6 Batch  136/269   train_loss = 4.084\n",
      "Epoch   6 Batch  146/269   train_loss = 4.391\n",
      "Epoch   6 Batch  156/269   train_loss = 4.464\n",
      "Epoch   6 Batch  166/269   train_loss = 4.356\n",
      "Epoch   6 Batch  176/269   train_loss = 4.534\n",
      "Epoch   6 Batch  186/269   train_loss = 4.611\n",
      "Epoch   6 Batch  196/269   train_loss = 4.379\n",
      "Epoch   6 Batch  206/269   train_loss = 4.400\n",
      "Epoch   6 Batch  216/269   train_loss = 4.566\n",
      "Epoch   6 Batch  226/269   train_loss = 4.143\n",
      "Epoch   6 Batch  236/269   train_loss = 4.291\n",
      "Epoch   6 Batch  246/269   train_loss = 4.228\n",
      "Epoch   6 Batch  256/269   train_loss = 4.795\n",
      "Epoch   6 Batch  266/269   train_loss = 4.455\n",
      "Epoch   7 Batch    7/269   train_loss = 4.079\n",
      "Epoch   7 Batch   17/269   train_loss = 4.512\n",
      "Epoch   7 Batch   27/269   train_loss = 4.235\n",
      "Epoch   7 Batch   37/269   train_loss = 4.533\n",
      "Epoch   7 Batch   47/269   train_loss = 4.174\n",
      "Epoch   7 Batch   57/269   train_loss = 4.275\n",
      "Epoch   7 Batch   67/269   train_loss = 4.366\n",
      "Epoch   7 Batch   77/269   train_loss = 4.249\n",
      "Epoch   7 Batch   87/269   train_loss = 4.644\n",
      "Epoch   7 Batch   97/269   train_loss = 4.467\n",
      "Epoch   7 Batch  107/269   train_loss = 4.303\n",
      "Epoch   7 Batch  117/269   train_loss = 4.567\n",
      "Epoch   7 Batch  127/269   train_loss = 4.358\n",
      "Epoch   7 Batch  137/269   train_loss = 4.164\n",
      "Epoch   7 Batch  147/269   train_loss = 4.702\n",
      "Epoch   7 Batch  157/269   train_loss = 4.239\n",
      "Epoch   7 Batch  167/269   train_loss = 4.266\n",
      "Epoch   7 Batch  177/269   train_loss = 4.395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7 Batch  187/269   train_loss = 4.359\n",
      "Epoch   7 Batch  197/269   train_loss = 3.917\n",
      "Epoch   7 Batch  207/269   train_loss = 4.407\n",
      "Epoch   7 Batch  217/269   train_loss = 4.239\n",
      "Epoch   7 Batch  227/269   train_loss = 3.993\n",
      "Epoch   7 Batch  237/269   train_loss = 4.081\n",
      "Epoch   7 Batch  247/269   train_loss = 4.224\n",
      "Epoch   7 Batch  257/269   train_loss = 4.109\n",
      "Epoch   7 Batch  267/269   train_loss = 4.022\n",
      "Epoch   8 Batch    8/269   train_loss = 4.068\n",
      "Epoch   8 Batch   18/269   train_loss = 4.198\n",
      "Epoch   8 Batch   28/269   train_loss = 4.549\n",
      "Epoch   8 Batch   38/269   train_loss = 4.203\n",
      "Epoch   8 Batch   48/269   train_loss = 4.052\n",
      "Epoch   8 Batch   58/269   train_loss = 4.438\n",
      "Epoch   8 Batch   68/269   train_loss = 4.312\n",
      "Epoch   8 Batch   78/269   train_loss = 4.235\n",
      "Epoch   8 Batch   88/269   train_loss = 4.249\n",
      "Epoch   8 Batch   98/269   train_loss = 4.281\n",
      "Epoch   8 Batch  108/269   train_loss = 4.351\n",
      "Epoch   8 Batch  118/269   train_loss = 4.116\n",
      "Epoch   8 Batch  128/269   train_loss = 4.234\n",
      "Epoch   8 Batch  138/269   train_loss = 4.076\n",
      "Epoch   8 Batch  148/269   train_loss = 4.225\n",
      "Epoch   8 Batch  158/269   train_loss = 3.993\n",
      "Epoch   8 Batch  168/269   train_loss = 3.955\n",
      "Epoch   8 Batch  178/269   train_loss = 4.091\n",
      "Epoch   8 Batch  188/269   train_loss = 4.104\n",
      "Epoch   8 Batch  198/269   train_loss = 3.952\n",
      "Epoch   8 Batch  208/269   train_loss = 3.936\n",
      "Epoch   8 Batch  218/269   train_loss = 4.088\n",
      "Epoch   8 Batch  228/269   train_loss = 4.270\n",
      "Epoch   8 Batch  238/269   train_loss = 3.950\n",
      "Epoch   8 Batch  248/269   train_loss = 4.210\n",
      "Epoch   8 Batch  258/269   train_loss = 4.114\n",
      "Epoch   8 Batch  268/269   train_loss = 3.929\n",
      "Epoch   9 Batch    9/269   train_loss = 4.163\n",
      "Epoch   9 Batch   19/269   train_loss = 3.968\n",
      "Epoch   9 Batch   29/269   train_loss = 4.099\n",
      "Epoch   9 Batch   39/269   train_loss = 3.875\n",
      "Epoch   9 Batch   49/269   train_loss = 4.128\n",
      "Epoch   9 Batch   59/269   train_loss = 4.048\n",
      "Epoch   9 Batch   69/269   train_loss = 4.048\n",
      "Epoch   9 Batch   79/269   train_loss = 4.009\n",
      "Epoch   9 Batch   89/269   train_loss = 4.049\n",
      "Epoch   9 Batch   99/269   train_loss = 4.247\n",
      "Epoch   9 Batch  109/269   train_loss = 4.108\n",
      "Epoch   9 Batch  119/269   train_loss = 3.928\n",
      "Epoch   9 Batch  129/269   train_loss = 4.203\n",
      "Epoch   9 Batch  139/269   train_loss = 4.173\n",
      "Epoch   9 Batch  149/269   train_loss = 3.948\n",
      "Epoch   9 Batch  159/269   train_loss = 4.522\n",
      "Epoch   9 Batch  169/269   train_loss = 3.965\n",
      "Epoch   9 Batch  179/269   train_loss = 3.846\n",
      "Epoch   9 Batch  189/269   train_loss = 3.928\n",
      "Epoch   9 Batch  199/269   train_loss = 4.019\n",
      "Epoch   9 Batch  209/269   train_loss = 3.930\n",
      "Epoch   9 Batch  219/269   train_loss = 3.919\n",
      "Epoch   9 Batch  229/269   train_loss = 3.907\n",
      "Epoch   9 Batch  239/269   train_loss = 3.808\n",
      "Epoch   9 Batch  249/269   train_loss = 4.169\n",
      "Epoch   9 Batch  259/269   train_loss = 4.032\n",
      "Epoch   9 Batch  269/269   train_loss = 4.045\n",
      "Epoch  10 Batch   10/269   train_loss = 3.623\n",
      "Epoch  10 Batch   20/269   train_loss = 4.172\n",
      "Epoch  10 Batch   30/269   train_loss = 3.997\n",
      "Epoch  10 Batch   40/269   train_loss = 3.854\n",
      "Epoch  10 Batch   50/269   train_loss = 4.017\n",
      "Epoch  10 Batch   60/269   train_loss = 4.031\n",
      "Epoch  10 Batch   70/269   train_loss = 4.021\n",
      "Epoch  10 Batch   80/269   train_loss = 4.029\n",
      "Epoch  10 Batch   90/269   train_loss = 3.818\n",
      "Epoch  10 Batch  100/269   train_loss = 3.869\n",
      "Epoch  10 Batch  110/269   train_loss = 3.945\n",
      "Epoch  10 Batch  120/269   train_loss = 3.993\n",
      "Epoch  10 Batch  130/269   train_loss = 3.887\n",
      "Epoch  10 Batch  140/269   train_loss = 3.857\n",
      "Epoch  10 Batch  150/269   train_loss = 4.100\n",
      "Epoch  10 Batch  160/269   train_loss = 3.893\n",
      "Epoch  10 Batch  170/269   train_loss = 3.812\n",
      "Epoch  10 Batch  180/269   train_loss = 3.992\n",
      "Epoch  10 Batch  190/269   train_loss = 3.918\n",
      "Epoch  10 Batch  200/269   train_loss = 3.893\n",
      "Epoch  10 Batch  210/269   train_loss = 3.906\n",
      "Epoch  10 Batch  220/269   train_loss = 3.802\n",
      "Epoch  10 Batch  230/269   train_loss = 4.004\n",
      "Epoch  10 Batch  240/269   train_loss = 3.938\n",
      "Epoch  10 Batch  250/269   train_loss = 3.976\n",
      "Epoch  10 Batch  260/269   train_loss = 3.903\n",
      "Epoch  11 Batch    1/269   train_loss = 3.947\n",
      "Epoch  11 Batch   11/269   train_loss = 3.635\n",
      "Epoch  11 Batch   21/269   train_loss = 3.743\n",
      "Epoch  11 Batch   31/269   train_loss = 3.907\n",
      "Epoch  11 Batch   41/269   train_loss = 3.824\n",
      "Epoch  11 Batch   51/269   train_loss = 3.944\n",
      "Epoch  11 Batch   61/269   train_loss = 3.977\n",
      "Epoch  11 Batch   71/269   train_loss = 3.873\n",
      "Epoch  11 Batch   81/269   train_loss = 3.664\n",
      "Epoch  11 Batch   91/269   train_loss = 3.903\n",
      "Epoch  11 Batch  101/269   train_loss = 4.128\n",
      "Epoch  11 Batch  111/269   train_loss = 3.818\n",
      "Epoch  11 Batch  121/269   train_loss = 3.772\n",
      "Epoch  11 Batch  131/269   train_loss = 3.775\n",
      "Epoch  11 Batch  141/269   train_loss = 3.748\n",
      "Epoch  11 Batch  151/269   train_loss = 3.674\n",
      "Epoch  11 Batch  161/269   train_loss = 3.932\n",
      "Epoch  11 Batch  171/269   train_loss = 4.003\n",
      "Epoch  11 Batch  181/269   train_loss = 3.718\n",
      "Epoch  11 Batch  191/269   train_loss = 3.581\n",
      "Epoch  11 Batch  201/269   train_loss = 4.062\n",
      "Epoch  11 Batch  211/269   train_loss = 3.554\n",
      "Epoch  11 Batch  221/269   train_loss = 3.771\n",
      "Epoch  11 Batch  231/269   train_loss = 3.667\n",
      "Epoch  11 Batch  241/269   train_loss = 3.656\n",
      "Epoch  11 Batch  251/269   train_loss = 3.782\n",
      "Epoch  11 Batch  261/269   train_loss = 3.822\n",
      "Epoch  12 Batch    2/269   train_loss = 3.547\n",
      "Epoch  12 Batch   12/269   train_loss = 3.572\n",
      "Epoch  12 Batch   22/269   train_loss = 3.663\n",
      "Epoch  12 Batch   32/269   train_loss = 3.811\n",
      "Epoch  12 Batch   42/269   train_loss = 3.711\n",
      "Epoch  12 Batch   52/269   train_loss = 3.376\n",
      "Epoch  12 Batch   62/269   train_loss = 3.630\n",
      "Epoch  12 Batch   72/269   train_loss = 3.669\n",
      "Epoch  12 Batch   82/269   train_loss = 3.778\n",
      "Epoch  12 Batch   92/269   train_loss = 3.871\n",
      "Epoch  12 Batch  102/269   train_loss = 3.771\n",
      "Epoch  12 Batch  112/269   train_loss = 3.586\n",
      "Epoch  12 Batch  122/269   train_loss = 3.642\n",
      "Epoch  12 Batch  132/269   train_loss = 3.672\n",
      "Epoch  12 Batch  142/269   train_loss = 3.446\n",
      "Epoch  12 Batch  152/269   train_loss = 3.424\n",
      "Epoch  12 Batch  162/269   train_loss = 3.551\n",
      "Epoch  12 Batch  172/269   train_loss = 3.617\n",
      "Epoch  12 Batch  182/269   train_loss = 3.674\n",
      "Epoch  12 Batch  192/269   train_loss = 3.477\n",
      "Epoch  12 Batch  202/269   train_loss = 3.911\n",
      "Epoch  12 Batch  212/269   train_loss = 3.868\n",
      "Epoch  12 Batch  222/269   train_loss = 3.647\n",
      "Epoch  12 Batch  232/269   train_loss = 3.511\n",
      "Epoch  12 Batch  242/269   train_loss = 3.571\n",
      "Epoch  12 Batch  252/269   train_loss = 3.468\n",
      "Epoch  12 Batch  262/269   train_loss = 3.864\n",
      "Epoch  13 Batch    3/269   train_loss = 3.302\n",
      "Epoch  13 Batch   13/269   train_loss = 3.514\n",
      "Epoch  13 Batch   23/269   train_loss = 3.690\n",
      "Epoch  13 Batch   33/269   train_loss = 3.592\n",
      "Epoch  13 Batch   43/269   train_loss = 3.571\n",
      "Epoch  13 Batch   53/269   train_loss = 3.530\n",
      "Epoch  13 Batch   63/269   train_loss = 3.793\n",
      "Epoch  13 Batch   73/269   train_loss = 3.479\n",
      "Epoch  13 Batch   83/269   train_loss = 3.735\n",
      "Epoch  13 Batch   93/269   train_loss = 3.586\n",
      "Epoch  13 Batch  103/269   train_loss = 3.598\n",
      "Epoch  13 Batch  113/269   train_loss = 3.626\n",
      "Epoch  13 Batch  123/269   train_loss = 3.727\n",
      "Epoch  13 Batch  133/269   train_loss = 3.688\n",
      "Epoch  13 Batch  143/269   train_loss = 3.511\n",
      "Epoch  13 Batch  153/269   train_loss = 3.743\n",
      "Epoch  13 Batch  163/269   train_loss = 3.592\n",
      "Epoch  13 Batch  173/269   train_loss = 3.485\n",
      "Epoch  13 Batch  183/269   train_loss = 3.291\n",
      "Epoch  13 Batch  193/269   train_loss = 3.409\n",
      "Epoch  13 Batch  203/269   train_loss = 3.560\n",
      "Epoch  13 Batch  213/269   train_loss = 3.676\n",
      "Epoch  13 Batch  223/269   train_loss = 3.557\n",
      "Epoch  13 Batch  233/269   train_loss = 3.560\n",
      "Epoch  13 Batch  243/269   train_loss = 3.541\n",
      "Epoch  13 Batch  253/269   train_loss = 3.805\n",
      "Epoch  13 Batch  263/269   train_loss = 3.357\n",
      "Epoch  14 Batch    4/269   train_loss = 3.587\n",
      "Epoch  14 Batch   14/269   train_loss = 3.456\n",
      "Epoch  14 Batch   24/269   train_loss = 3.749\n",
      "Epoch  14 Batch   34/269   train_loss = 3.526\n",
      "Epoch  14 Batch   44/269   train_loss = 3.448\n",
      "Epoch  14 Batch   54/269   train_loss = 3.627\n",
      "Epoch  14 Batch   64/269   train_loss = 3.653\n",
      "Epoch  14 Batch   74/269   train_loss = 3.386\n",
      "Epoch  14 Batch   84/269   train_loss = 3.453\n",
      "Epoch  14 Batch   94/269   train_loss = 3.417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14 Batch  104/269   train_loss = 3.277\n",
      "Epoch  14 Batch  114/269   train_loss = 3.606\n",
      "Epoch  14 Batch  124/269   train_loss = 3.576\n",
      "Epoch  14 Batch  134/269   train_loss = 3.336\n",
      "Epoch  14 Batch  144/269   train_loss = 3.418\n",
      "Epoch  14 Batch  154/269   train_loss = 3.393\n",
      "Epoch  14 Batch  164/269   train_loss = 3.353\n",
      "Epoch  14 Batch  174/269   train_loss = 3.475\n",
      "Epoch  14 Batch  184/269   train_loss = 3.744\n",
      "Epoch  14 Batch  194/269   train_loss = 3.513\n",
      "Epoch  14 Batch  204/269   train_loss = 3.580\n",
      "Epoch  14 Batch  214/269   train_loss = 3.489\n",
      "Epoch  14 Batch  224/269   train_loss = 3.462\n",
      "Epoch  14 Batch  234/269   train_loss = 3.224\n",
      "Epoch  14 Batch  244/269   train_loss = 3.257\n",
      "Epoch  14 Batch  254/269   train_loss = 3.431\n",
      "Epoch  14 Batch  264/269   train_loss = 3.479\n",
      "Epoch  15 Batch    5/269   train_loss = 3.414\n",
      "Epoch  15 Batch   15/269   train_loss = 3.508\n",
      "Epoch  15 Batch   25/269   train_loss = 3.195\n",
      "Epoch  15 Batch   35/269   train_loss = 3.354\n",
      "Epoch  15 Batch   45/269   train_loss = 3.524\n",
      "Epoch  15 Batch   55/269   train_loss = 3.395\n",
      "Epoch  15 Batch   65/269   train_loss = 3.256\n",
      "Epoch  15 Batch   75/269   train_loss = 3.581\n",
      "Epoch  15 Batch   85/269   train_loss = 3.394\n",
      "Epoch  15 Batch   95/269   train_loss = 3.391\n",
      "Epoch  15 Batch  105/269   train_loss = 3.255\n",
      "Epoch  15 Batch  115/269   train_loss = 3.303\n",
      "Epoch  15 Batch  125/269   train_loss = 3.526\n",
      "Epoch  15 Batch  135/269   train_loss = 3.481\n",
      "Epoch  15 Batch  145/269   train_loss = 3.120\n",
      "Epoch  15 Batch  155/269   train_loss = 3.412\n",
      "Epoch  15 Batch  165/269   train_loss = 3.409\n",
      "Epoch  15 Batch  175/269   train_loss = 3.550\n",
      "Epoch  15 Batch  185/269   train_loss = 3.291\n",
      "Epoch  15 Batch  195/269   train_loss = 3.511\n",
      "Epoch  15 Batch  205/269   train_loss = 3.384\n",
      "Epoch  15 Batch  215/269   train_loss = 3.182\n",
      "Epoch  15 Batch  225/269   train_loss = 3.415\n",
      "Epoch  15 Batch  235/269   train_loss = 3.451\n",
      "Epoch  15 Batch  245/269   train_loss = 3.498\n",
      "Epoch  15 Batch  255/269   train_loss = 3.619\n",
      "Epoch  15 Batch  265/269   train_loss = 3.227\n",
      "Epoch  16 Batch    6/269   train_loss = 3.199\n",
      "Epoch  16 Batch   16/269   train_loss = 3.310\n",
      "Epoch  16 Batch   26/269   train_loss = 3.275\n",
      "Epoch  16 Batch   36/269   train_loss = 3.268\n",
      "Epoch  16 Batch   46/269   train_loss = 3.627\n",
      "Epoch  16 Batch   56/269   train_loss = 3.313\n",
      "Epoch  16 Batch   66/269   train_loss = 3.315\n",
      "Epoch  16 Batch   76/269   train_loss = 3.463\n",
      "Epoch  16 Batch   86/269   train_loss = 3.365\n",
      "Epoch  16 Batch   96/269   train_loss = 3.538\n",
      "Epoch  16 Batch  106/269   train_loss = 3.387\n",
      "Epoch  16 Batch  116/269   train_loss = 3.422\n",
      "Epoch  16 Batch  126/269   train_loss = 3.341\n",
      "Epoch  16 Batch  136/269   train_loss = 3.156\n",
      "Epoch  16 Batch  146/269   train_loss = 3.169\n",
      "Epoch  16 Batch  156/269   train_loss = 3.316\n",
      "Epoch  16 Batch  166/269   train_loss = 3.466\n",
      "Epoch  16 Batch  176/269   train_loss = 3.438\n",
      "Epoch  16 Batch  186/269   train_loss = 3.384\n",
      "Epoch  16 Batch  196/269   train_loss = 3.321\n",
      "Epoch  16 Batch  206/269   train_loss = 3.312\n",
      "Epoch  16 Batch  216/269   train_loss = 3.498\n",
      "Epoch  16 Batch  226/269   train_loss = 3.108\n",
      "Epoch  16 Batch  236/269   train_loss = 3.172\n",
      "Epoch  16 Batch  246/269   train_loss = 3.300\n",
      "Epoch  16 Batch  256/269   train_loss = 3.406\n",
      "Epoch  16 Batch  266/269   train_loss = 3.245\n",
      "Epoch  17 Batch    7/269   train_loss = 3.173\n",
      "Epoch  17 Batch   17/269   train_loss = 3.369\n",
      "Epoch  17 Batch   27/269   train_loss = 3.296\n",
      "Epoch  17 Batch   37/269   train_loss = 3.345\n",
      "Epoch  17 Batch   47/269   train_loss = 3.191\n",
      "Epoch  17 Batch   57/269   train_loss = 3.390\n",
      "Epoch  17 Batch   67/269   train_loss = 3.250\n",
      "Epoch  17 Batch   77/269   train_loss = 3.224\n",
      "Epoch  17 Batch   87/269   train_loss = 3.400\n",
      "Epoch  17 Batch   97/269   train_loss = 3.251\n",
      "Epoch  17 Batch  107/269   train_loss = 3.389\n",
      "Epoch  17 Batch  117/269   train_loss = 3.522\n",
      "Epoch  17 Batch  127/269   train_loss = 3.383\n",
      "Epoch  17 Batch  137/269   train_loss = 3.407\n",
      "Epoch  17 Batch  147/269   train_loss = 3.443\n",
      "Epoch  17 Batch  157/269   train_loss = 3.068\n",
      "Epoch  17 Batch  167/269   train_loss = 3.327\n",
      "Epoch  17 Batch  177/269   train_loss = 3.324\n",
      "Epoch  17 Batch  187/269   train_loss = 3.374\n",
      "Epoch  17 Batch  197/269   train_loss = 3.095\n",
      "Epoch  17 Batch  207/269   train_loss = 3.526\n",
      "Epoch  17 Batch  217/269   train_loss = 3.323\n",
      "Epoch  17 Batch  227/269   train_loss = 3.091\n",
      "Epoch  17 Batch  237/269   train_loss = 3.258\n",
      "Epoch  17 Batch  247/269   train_loss = 3.249\n",
      "Epoch  17 Batch  257/269   train_loss = 3.092\n",
      "Epoch  17 Batch  267/269   train_loss = 3.060\n",
      "Epoch  18 Batch    8/269   train_loss = 3.105\n",
      "Epoch  18 Batch   18/269   train_loss = 3.037\n",
      "Epoch  18 Batch   28/269   train_loss = 3.540\n",
      "Epoch  18 Batch   38/269   train_loss = 3.131\n",
      "Epoch  18 Batch   48/269   train_loss = 3.114\n",
      "Epoch  18 Batch   58/269   train_loss = 3.384\n",
      "Epoch  18 Batch   68/269   train_loss = 3.496\n",
      "Epoch  18 Batch   78/269   train_loss = 3.140\n",
      "Epoch  18 Batch   88/269   train_loss = 3.200\n",
      "Epoch  18 Batch   98/269   train_loss = 3.265\n",
      "Epoch  18 Batch  108/269   train_loss = 3.374\n",
      "Epoch  18 Batch  118/269   train_loss = 3.021\n",
      "Epoch  18 Batch  128/269   train_loss = 3.372\n",
      "Epoch  18 Batch  138/269   train_loss = 3.170\n",
      "Epoch  18 Batch  148/269   train_loss = 3.044\n",
      "Epoch  18 Batch  158/269   train_loss = 3.104\n",
      "Epoch  18 Batch  168/269   train_loss = 3.055\n",
      "Epoch  18 Batch  178/269   train_loss = 3.090\n",
      "Epoch  18 Batch  188/269   train_loss = 3.157\n",
      "Epoch  18 Batch  198/269   train_loss = 3.041\n",
      "Epoch  18 Batch  208/269   train_loss = 3.112\n",
      "Epoch  18 Batch  218/269   train_loss = 3.166\n",
      "Epoch  18 Batch  228/269   train_loss = 3.332\n",
      "Epoch  18 Batch  238/269   train_loss = 3.071\n",
      "Epoch  18 Batch  248/269   train_loss = 3.262\n",
      "Epoch  18 Batch  258/269   train_loss = 3.196\n",
      "Epoch  18 Batch  268/269   train_loss = 3.155\n",
      "Epoch  19 Batch    9/269   train_loss = 3.278\n",
      "Epoch  19 Batch   19/269   train_loss = 3.121\n",
      "Epoch  19 Batch   29/269   train_loss = 3.169\n",
      "Epoch  19 Batch   39/269   train_loss = 3.002\n",
      "Epoch  19 Batch   49/269   train_loss = 3.203\n",
      "Epoch  19 Batch   59/269   train_loss = 3.147\n",
      "Epoch  19 Batch   69/269   train_loss = 3.171\n",
      "Epoch  19 Batch   79/269   train_loss = 3.149\n",
      "Epoch  19 Batch   89/269   train_loss = 3.270\n",
      "Epoch  19 Batch   99/269   train_loss = 3.097\n",
      "Epoch  19 Batch  109/269   train_loss = 3.248\n",
      "Epoch  19 Batch  119/269   train_loss = 3.220\n",
      "Epoch  19 Batch  129/269   train_loss = 3.116\n",
      "Epoch  19 Batch  139/269   train_loss = 3.206\n",
      "Epoch  19 Batch  149/269   train_loss = 3.050\n",
      "Epoch  19 Batch  159/269   train_loss = 3.445\n",
      "Epoch  19 Batch  169/269   train_loss = 3.159\n",
      "Epoch  19 Batch  179/269   train_loss = 3.115\n",
      "Epoch  19 Batch  189/269   train_loss = 2.983\n",
      "Epoch  19 Batch  199/269   train_loss = 3.239\n",
      "Epoch  19 Batch  209/269   train_loss = 3.090\n",
      "Epoch  19 Batch  219/269   train_loss = 2.995\n",
      "Epoch  19 Batch  229/269   train_loss = 3.084\n",
      "Epoch  19 Batch  239/269   train_loss = 2.951\n",
      "Epoch  19 Batch  249/269   train_loss = 3.271\n",
      "Epoch  19 Batch  259/269   train_loss = 3.124\n",
      "Epoch  19 Batch  269/269   train_loss = 3.117\n",
      "Epoch  20 Batch   10/269   train_loss = 2.914\n",
      "Epoch  20 Batch   20/269   train_loss = 3.300\n",
      "Epoch  20 Batch   30/269   train_loss = 3.152\n",
      "Epoch  20 Batch   40/269   train_loss = 3.142\n",
      "Epoch  20 Batch   50/269   train_loss = 3.106\n",
      "Epoch  20 Batch   60/269   train_loss = 3.223\n",
      "Epoch  20 Batch   70/269   train_loss = 3.151\n",
      "Epoch  20 Batch   80/269   train_loss = 3.270\n",
      "Epoch  20 Batch   90/269   train_loss = 3.039\n",
      "Epoch  20 Batch  100/269   train_loss = 2.991\n",
      "Epoch  20 Batch  110/269   train_loss = 3.160\n",
      "Epoch  20 Batch  120/269   train_loss = 3.143\n",
      "Epoch  20 Batch  130/269   train_loss = 3.074\n",
      "Epoch  20 Batch  140/269   train_loss = 2.988\n",
      "Epoch  20 Batch  150/269   train_loss = 3.198\n",
      "Epoch  20 Batch  160/269   train_loss = 3.138\n",
      "Epoch  20 Batch  170/269   train_loss = 3.127\n",
      "Epoch  20 Batch  180/269   train_loss = 3.249\n",
      "Epoch  20 Batch  190/269   train_loss = 3.201\n",
      "Epoch  20 Batch  200/269   train_loss = 3.047\n",
      "Epoch  20 Batch  210/269   train_loss = 3.018\n",
      "Epoch  20 Batch  220/269   train_loss = 2.974\n",
      "Epoch  20 Batch  230/269   train_loss = 3.208\n",
      "Epoch  20 Batch  240/269   train_loss = 3.174\n",
      "Epoch  20 Batch  250/269   train_loss = 3.091\n",
      "Epoch  20 Batch  260/269   train_loss = 3.148\n",
      "Epoch  21 Batch    1/269   train_loss = 3.003\n",
      "Epoch  21 Batch   11/269   train_loss = 2.944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  21 Batch   21/269   train_loss = 2.972\n",
      "Epoch  21 Batch   31/269   train_loss = 3.209\n",
      "Epoch  21 Batch   41/269   train_loss = 3.042\n",
      "Epoch  21 Batch   51/269   train_loss = 2.960\n",
      "Epoch  21 Batch   61/269   train_loss = 3.242\n",
      "Epoch  21 Batch   71/269   train_loss = 3.075\n",
      "Epoch  21 Batch   81/269   train_loss = 2.965\n",
      "Epoch  21 Batch   91/269   train_loss = 3.238\n",
      "Epoch  21 Batch  101/269   train_loss = 3.340\n",
      "Epoch  21 Batch  111/269   train_loss = 3.133\n",
      "Epoch  21 Batch  121/269   train_loss = 3.064\n",
      "Epoch  21 Batch  131/269   train_loss = 2.949\n",
      "Epoch  21 Batch  141/269   train_loss = 3.040\n",
      "Epoch  21 Batch  151/269   train_loss = 2.869\n",
      "Epoch  21 Batch  161/269   train_loss = 3.105\n",
      "Epoch  21 Batch  171/269   train_loss = 3.211\n",
      "Epoch  21 Batch  181/269   train_loss = 3.035\n",
      "Epoch  21 Batch  191/269   train_loss = 2.925\n",
      "Epoch  21 Batch  201/269   train_loss = 3.399\n",
      "Epoch  21 Batch  211/269   train_loss = 2.921\n",
      "Epoch  21 Batch  221/269   train_loss = 3.041\n",
      "Epoch  21 Batch  231/269   train_loss = 3.064\n",
      "Epoch  21 Batch  241/269   train_loss = 2.986\n",
      "Epoch  21 Batch  251/269   train_loss = 3.021\n",
      "Epoch  21 Batch  261/269   train_loss = 3.082\n",
      "Epoch  22 Batch    2/269   train_loss = 2.718\n",
      "Epoch  22 Batch   12/269   train_loss = 2.910\n",
      "Epoch  22 Batch   22/269   train_loss = 2.932\n",
      "Epoch  22 Batch   32/269   train_loss = 3.130\n",
      "Epoch  22 Batch   42/269   train_loss = 3.007\n",
      "Epoch  22 Batch   52/269   train_loss = 2.704\n",
      "Epoch  22 Batch   62/269   train_loss = 2.959\n",
      "Epoch  22 Batch   72/269   train_loss = 3.071\n",
      "Epoch  22 Batch   82/269   train_loss = 3.062\n",
      "Epoch  22 Batch   92/269   train_loss = 3.027\n",
      "Epoch  22 Batch  102/269   train_loss = 3.006\n",
      "Epoch  22 Batch  112/269   train_loss = 2.907\n",
      "Epoch  22 Batch  122/269   train_loss = 2.963\n",
      "Epoch  22 Batch  132/269   train_loss = 3.125\n",
      "Epoch  22 Batch  142/269   train_loss = 2.821\n",
      "Epoch  22 Batch  152/269   train_loss = 2.914\n",
      "Epoch  22 Batch  162/269   train_loss = 2.923\n",
      "Epoch  22 Batch  172/269   train_loss = 2.923\n",
      "Epoch  22 Batch  182/269   train_loss = 3.080\n",
      "Epoch  22 Batch  192/269   train_loss = 2.826\n",
      "Epoch  22 Batch  202/269   train_loss = 3.170\n",
      "Epoch  22 Batch  212/269   train_loss = 3.229\n",
      "Epoch  22 Batch  222/269   train_loss = 2.890\n",
      "Epoch  22 Batch  232/269   train_loss = 2.850\n",
      "Epoch  22 Batch  242/269   train_loss = 2.915\n",
      "Epoch  22 Batch  252/269   train_loss = 2.787\n",
      "Epoch  22 Batch  262/269   train_loss = 3.115\n",
      "Epoch  23 Batch    3/269   train_loss = 2.707\n",
      "Epoch  23 Batch   13/269   train_loss = 2.924\n",
      "Epoch  23 Batch   23/269   train_loss = 3.013\n",
      "Epoch  23 Batch   33/269   train_loss = 2.934\n",
      "Epoch  23 Batch   43/269   train_loss = 2.896\n",
      "Epoch  23 Batch   53/269   train_loss = 2.985\n",
      "Epoch  23 Batch   63/269   train_loss = 3.264\n",
      "Epoch  23 Batch   73/269   train_loss = 2.835\n",
      "Epoch  23 Batch   83/269   train_loss = 3.230\n",
      "Epoch  23 Batch   93/269   train_loss = 2.888\n",
      "Epoch  23 Batch  103/269   train_loss = 2.935\n",
      "Epoch  23 Batch  113/269   train_loss = 2.950\n",
      "Epoch  23 Batch  123/269   train_loss = 3.193\n",
      "Epoch  23 Batch  133/269   train_loss = 2.984\n",
      "Epoch  23 Batch  143/269   train_loss = 2.998\n",
      "Epoch  23 Batch  153/269   train_loss = 3.116\n",
      "Epoch  23 Batch  163/269   train_loss = 3.009\n",
      "Epoch  23 Batch  173/269   train_loss = 2.977\n",
      "Epoch  23 Batch  183/269   train_loss = 2.684\n",
      "Epoch  23 Batch  193/269   train_loss = 2.880\n",
      "Epoch  23 Batch  203/269   train_loss = 2.983\n",
      "Epoch  23 Batch  213/269   train_loss = 2.971\n",
      "Epoch  23 Batch  223/269   train_loss = 3.046\n",
      "Epoch  23 Batch  233/269   train_loss = 2.977\n",
      "Epoch  23 Batch  243/269   train_loss = 2.940\n",
      "Epoch  23 Batch  253/269   train_loss = 3.019\n",
      "Epoch  23 Batch  263/269   train_loss = 2.747\n",
      "Epoch  24 Batch    4/269   train_loss = 2.911\n",
      "Epoch  24 Batch   14/269   train_loss = 2.978\n",
      "Epoch  24 Batch   24/269   train_loss = 3.112\n",
      "Epoch  24 Batch   34/269   train_loss = 2.997\n",
      "Epoch  24 Batch   44/269   train_loss = 2.967\n",
      "Epoch  24 Batch   54/269   train_loss = 3.052\n",
      "Epoch  24 Batch   64/269   train_loss = 2.955\n",
      "Epoch  24 Batch   74/269   train_loss = 2.905\n",
      "Epoch  24 Batch   84/269   train_loss = 2.807\n",
      "Epoch  24 Batch   94/269   train_loss = 2.792\n",
      "Epoch  24 Batch  104/269   train_loss = 2.698\n",
      "Epoch  24 Batch  114/269   train_loss = 2.938\n",
      "Epoch  24 Batch  124/269   train_loss = 2.998\n",
      "Epoch  24 Batch  134/269   train_loss = 2.765\n",
      "Epoch  24 Batch  144/269   train_loss = 2.906\n",
      "Epoch  24 Batch  154/269   train_loss = 2.746\n",
      "Epoch  24 Batch  164/269   train_loss = 2.772\n",
      "Epoch  24 Batch  174/269   train_loss = 3.080\n",
      "Epoch  24 Batch  184/269   train_loss = 3.274\n",
      "Epoch  24 Batch  194/269   train_loss = 2.948\n",
      "Epoch  24 Batch  204/269   train_loss = 3.017\n",
      "Epoch  24 Batch  214/269   train_loss = 2.917\n",
      "Epoch  24 Batch  224/269   train_loss = 2.839\n",
      "Epoch  24 Batch  234/269   train_loss = 2.746\n",
      "Epoch  24 Batch  244/269   train_loss = 2.749\n",
      "Epoch  24 Batch  254/269   train_loss = 2.698\n",
      "Epoch  24 Batch  264/269   train_loss = 2.947\n",
      "Epoch  25 Batch    5/269   train_loss = 2.824\n",
      "Epoch  25 Batch   15/269   train_loss = 2.844\n",
      "Epoch  25 Batch   25/269   train_loss = 2.749\n",
      "Epoch  25 Batch   35/269   train_loss = 2.782\n",
      "Epoch  25 Batch   45/269   train_loss = 3.047\n",
      "Epoch  25 Batch   55/269   train_loss = 2.953\n",
      "Epoch  25 Batch   65/269   train_loss = 2.716\n",
      "Epoch  25 Batch   75/269   train_loss = 2.969\n",
      "Epoch  25 Batch   85/269   train_loss = 2.828\n",
      "Epoch  25 Batch   95/269   train_loss = 2.846\n",
      "Epoch  25 Batch  105/269   train_loss = 2.841\n",
      "Epoch  25 Batch  115/269   train_loss = 2.728\n",
      "Epoch  25 Batch  125/269   train_loss = 2.992\n",
      "Epoch  25 Batch  135/269   train_loss = 2.968\n",
      "Epoch  25 Batch  145/269   train_loss = 2.820\n",
      "Epoch  25 Batch  155/269   train_loss = 2.781\n",
      "Epoch  25 Batch  165/269   train_loss = 2.887\n",
      "Epoch  25 Batch  175/269   train_loss = 2.987\n",
      "Epoch  25 Batch  185/269   train_loss = 2.844\n",
      "Epoch  25 Batch  195/269   train_loss = 3.076\n",
      "Epoch  25 Batch  205/269   train_loss = 2.811\n",
      "Epoch  25 Batch  215/269   train_loss = 2.678\n",
      "Epoch  25 Batch  225/269   train_loss = 2.866\n",
      "Epoch  25 Batch  235/269   train_loss = 2.896\n",
      "Epoch  25 Batch  245/269   train_loss = 3.019\n",
      "Epoch  25 Batch  255/269   train_loss = 3.149\n",
      "Epoch  25 Batch  265/269   train_loss = 2.752\n",
      "Epoch  26 Batch    6/269   train_loss = 2.753\n",
      "Epoch  26 Batch   16/269   train_loss = 2.724\n",
      "Epoch  26 Batch   26/269   train_loss = 2.999\n",
      "Epoch  26 Batch   36/269   train_loss = 2.819\n",
      "Epoch  26 Batch   46/269   train_loss = 3.050\n",
      "Epoch  26 Batch   56/269   train_loss = 2.887\n",
      "Epoch  26 Batch   66/269   train_loss = 2.746\n",
      "Epoch  26 Batch   76/269   train_loss = 3.009\n",
      "Epoch  26 Batch   86/269   train_loss = 2.751\n",
      "Epoch  26 Batch   96/269   train_loss = 2.918\n",
      "Epoch  26 Batch  106/269   train_loss = 2.874\n",
      "Epoch  26 Batch  116/269   train_loss = 2.841\n",
      "Epoch  26 Batch  126/269   train_loss = 2.869\n",
      "Epoch  26 Batch  136/269   train_loss = 2.720\n",
      "Epoch  26 Batch  146/269   train_loss = 2.724\n",
      "Epoch  26 Batch  156/269   train_loss = 2.952\n",
      "Epoch  26 Batch  166/269   train_loss = 2.994\n",
      "Epoch  26 Batch  176/269   train_loss = 2.839\n",
      "Epoch  26 Batch  186/269   train_loss = 2.966\n",
      "Epoch  26 Batch  196/269   train_loss = 2.842\n",
      "Epoch  26 Batch  206/269   train_loss = 2.905\n",
      "Epoch  26 Batch  216/269   train_loss = 2.985\n",
      "Epoch  26 Batch  226/269   train_loss = 2.791\n",
      "Epoch  26 Batch  236/269   train_loss = 2.669\n",
      "Epoch  26 Batch  246/269   train_loss = 2.935\n",
      "Epoch  26 Batch  256/269   train_loss = 2.920\n",
      "Epoch  26 Batch  266/269   train_loss = 2.824\n",
      "Epoch  27 Batch    7/269   train_loss = 2.765\n",
      "Epoch  27 Batch   17/269   train_loss = 2.915\n",
      "Epoch  27 Batch   27/269   train_loss = 2.904\n",
      "Epoch  27 Batch   37/269   train_loss = 2.845\n",
      "Epoch  27 Batch   47/269   train_loss = 2.603\n",
      "Epoch  27 Batch   57/269   train_loss = 3.014\n",
      "Epoch  27 Batch   67/269   train_loss = 2.928\n",
      "Epoch  27 Batch   77/269   train_loss = 2.681\n",
      "Epoch  27 Batch   87/269   train_loss = 2.904\n",
      "Epoch  27 Batch   97/269   train_loss = 2.826\n",
      "Epoch  27 Batch  107/269   train_loss = 2.978\n",
      "Epoch  27 Batch  117/269   train_loss = 2.898\n",
      "Epoch  27 Batch  127/269   train_loss = 2.968\n",
      "Epoch  27 Batch  137/269   train_loss = 3.054\n",
      "Epoch  27 Batch  147/269   train_loss = 2.910\n",
      "Epoch  27 Batch  157/269   train_loss = 2.693\n",
      "Epoch  27 Batch  167/269   train_loss = 2.975\n",
      "Epoch  27 Batch  177/269   train_loss = 2.832\n",
      "Epoch  27 Batch  187/269   train_loss = 2.887\n",
      "Epoch  27 Batch  197/269   train_loss = 2.775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  27 Batch  207/269   train_loss = 3.129\n",
      "Epoch  27 Batch  217/269   train_loss = 2.952\n",
      "Epoch  27 Batch  227/269   train_loss = 2.685\n",
      "Epoch  27 Batch  237/269   train_loss = 2.826\n",
      "Epoch  27 Batch  247/269   train_loss = 2.814\n",
      "Epoch  27 Batch  257/269   train_loss = 2.617\n",
      "Epoch  27 Batch  267/269   train_loss = 2.640\n",
      "Epoch  28 Batch    8/269   train_loss = 2.642\n",
      "Epoch  28 Batch   18/269   train_loss = 2.391\n",
      "Epoch  28 Batch   28/269   train_loss = 3.079\n",
      "Epoch  28 Batch   38/269   train_loss = 2.746\n",
      "Epoch  28 Batch   48/269   train_loss = 2.841\n",
      "Epoch  28 Batch   58/269   train_loss = 2.996\n",
      "Epoch  28 Batch   68/269   train_loss = 3.032\n",
      "Epoch  28 Batch   78/269   train_loss = 2.734\n",
      "Epoch  28 Batch   88/269   train_loss = 2.749\n",
      "Epoch  28 Batch   98/269   train_loss = 2.807\n",
      "Epoch  28 Batch  108/269   train_loss = 2.921\n",
      "Epoch  28 Batch  118/269   train_loss = 2.495\n",
      "Epoch  28 Batch  128/269   train_loss = 3.006\n",
      "Epoch  28 Batch  138/269   train_loss = 2.815\n",
      "Epoch  28 Batch  148/269   train_loss = 2.605\n",
      "Epoch  28 Batch  158/269   train_loss = 2.692\n",
      "Epoch  28 Batch  168/269   train_loss = 2.710\n",
      "Epoch  28 Batch  178/269   train_loss = 2.734\n",
      "Epoch  28 Batch  188/269   train_loss = 2.798\n",
      "Epoch  28 Batch  198/269   train_loss = 2.695\n",
      "Epoch  28 Batch  208/269   train_loss = 2.714\n",
      "Epoch  28 Batch  218/269   train_loss = 2.726\n",
      "Epoch  28 Batch  228/269   train_loss = 2.869\n",
      "Epoch  28 Batch  238/269   train_loss = 2.719\n",
      "Epoch  28 Batch  248/269   train_loss = 2.915\n",
      "Epoch  28 Batch  258/269   train_loss = 2.668\n",
      "Epoch  28 Batch  268/269   train_loss = 2.788\n",
      "Epoch  29 Batch    9/269   train_loss = 2.836\n",
      "Epoch  29 Batch   19/269   train_loss = 2.769\n",
      "Epoch  29 Batch   29/269   train_loss = 2.848\n",
      "Epoch  29 Batch   39/269   train_loss = 2.737\n",
      "Epoch  29 Batch   49/269   train_loss = 2.913\n",
      "Epoch  29 Batch   59/269   train_loss = 2.777\n",
      "Epoch  29 Batch   69/269   train_loss = 2.845\n",
      "Epoch  29 Batch   79/269   train_loss = 2.816\n",
      "Epoch  29 Batch   89/269   train_loss = 2.774\n",
      "Epoch  29 Batch   99/269   train_loss = 2.759\n",
      "Epoch  29 Batch  109/269   train_loss = 2.874\n",
      "Epoch  29 Batch  119/269   train_loss = 2.779\n",
      "Epoch  29 Batch  129/269   train_loss = 2.716\n",
      "Epoch  29 Batch  139/269   train_loss = 2.853\n",
      "Epoch  29 Batch  149/269   train_loss = 2.607\n",
      "Epoch  29 Batch  159/269   train_loss = 3.171\n",
      "Epoch  29 Batch  169/269   train_loss = 2.836\n",
      "Epoch  29 Batch  179/269   train_loss = 2.803\n",
      "Epoch  29 Batch  189/269   train_loss = 2.637\n",
      "Epoch  29 Batch  199/269   train_loss = 3.028\n",
      "Epoch  29 Batch  209/269   train_loss = 2.768\n",
      "Epoch  29 Batch  219/269   train_loss = 2.589\n",
      "Epoch  29 Batch  229/269   train_loss = 2.748\n",
      "Epoch  29 Batch  239/269   train_loss = 2.722\n",
      "Epoch  29 Batch  249/269   train_loss = 2.858\n",
      "Epoch  29 Batch  259/269   train_loss = 2.752\n",
      "Epoch  29 Batch  269/269   train_loss = 2.840\n",
      "Epoch  30 Batch   10/269   train_loss = 2.481\n",
      "Epoch  30 Batch   20/269   train_loss = 2.988\n",
      "Epoch  30 Batch   30/269   train_loss = 2.875\n",
      "Epoch  30 Batch   40/269   train_loss = 2.786\n",
      "Epoch  30 Batch   50/269   train_loss = 2.713\n",
      "Epoch  30 Batch   60/269   train_loss = 2.882\n",
      "Epoch  30 Batch   70/269   train_loss = 2.791\n",
      "Epoch  30 Batch   80/269   train_loss = 2.849\n",
      "Epoch  30 Batch   90/269   train_loss = 2.720\n",
      "Epoch  30 Batch  100/269   train_loss = 2.592\n",
      "Epoch  30 Batch  110/269   train_loss = 2.774\n",
      "Epoch  30 Batch  120/269   train_loss = 2.742\n",
      "Epoch  30 Batch  130/269   train_loss = 2.752\n",
      "Epoch  30 Batch  140/269   train_loss = 2.740\n",
      "Epoch  30 Batch  150/269   train_loss = 2.869\n",
      "Epoch  30 Batch  160/269   train_loss = 2.798\n",
      "Epoch  30 Batch  170/269   train_loss = 2.815\n",
      "Epoch  30 Batch  180/269   train_loss = 2.886\n",
      "Epoch  30 Batch  190/269   train_loss = 2.786\n",
      "Epoch  30 Batch  200/269   train_loss = 2.635\n",
      "Epoch  30 Batch  210/269   train_loss = 2.695\n",
      "Epoch  30 Batch  220/269   train_loss = 2.570\n",
      "Epoch  30 Batch  230/269   train_loss = 2.875\n",
      "Epoch  30 Batch  240/269   train_loss = 2.844\n",
      "Epoch  30 Batch  250/269   train_loss = 2.716\n",
      "Epoch  30 Batch  260/269   train_loss = 2.786\n",
      "Epoch  31 Batch    1/269   train_loss = 2.665\n",
      "Epoch  31 Batch   11/269   train_loss = 2.577\n",
      "Epoch  31 Batch   21/269   train_loss = 2.669\n",
      "Epoch  31 Batch   31/269   train_loss = 2.980\n",
      "Epoch  31 Batch   41/269   train_loss = 2.781\n",
      "Epoch  31 Batch   51/269   train_loss = 2.689\n",
      "Epoch  31 Batch   61/269   train_loss = 2.859\n",
      "Epoch  31 Batch   71/269   train_loss = 2.834\n",
      "Epoch  31 Batch   81/269   train_loss = 2.637\n",
      "Epoch  31 Batch   91/269   train_loss = 2.817\n",
      "Epoch  31 Batch  101/269   train_loss = 2.925\n",
      "Epoch  31 Batch  111/269   train_loss = 2.886\n",
      "Epoch  31 Batch  121/269   train_loss = 2.655\n",
      "Epoch  31 Batch  131/269   train_loss = 2.716\n",
      "Epoch  31 Batch  141/269   train_loss = 2.646\n",
      "Epoch  31 Batch  151/269   train_loss = 2.531\n",
      "Epoch  31 Batch  161/269   train_loss = 2.676\n",
      "Epoch  31 Batch  171/269   train_loss = 2.855\n",
      "Epoch  31 Batch  181/269   train_loss = 2.720\n",
      "Epoch  31 Batch  191/269   train_loss = 2.608\n",
      "Epoch  31 Batch  201/269   train_loss = 2.961\n",
      "Epoch  31 Batch  211/269   train_loss = 2.556\n",
      "Epoch  31 Batch  221/269   train_loss = 2.752\n",
      "Epoch  31 Batch  231/269   train_loss = 2.761\n",
      "Epoch  31 Batch  241/269   train_loss = 2.648\n",
      "Epoch  31 Batch  251/269   train_loss = 2.604\n",
      "Epoch  31 Batch  261/269   train_loss = 2.828\n",
      "Epoch  32 Batch    2/269   train_loss = 2.464\n",
      "Epoch  32 Batch   12/269   train_loss = 2.645\n",
      "Epoch  32 Batch   22/269   train_loss = 2.660\n",
      "Epoch  32 Batch   32/269   train_loss = 2.843\n",
      "Epoch  32 Batch   42/269   train_loss = 2.765\n",
      "Epoch  32 Batch   52/269   train_loss = 2.442\n",
      "Epoch  32 Batch   62/269   train_loss = 2.721\n",
      "Epoch  32 Batch   72/269   train_loss = 2.754\n",
      "Epoch  32 Batch   82/269   train_loss = 2.775\n",
      "Epoch  32 Batch   92/269   train_loss = 2.735\n",
      "Epoch  32 Batch  102/269   train_loss = 2.570\n",
      "Epoch  32 Batch  112/269   train_loss = 2.649\n",
      "Epoch  32 Batch  122/269   train_loss = 2.600\n",
      "Epoch  32 Batch  132/269   train_loss = 2.786\n",
      "Epoch  32 Batch  142/269   train_loss = 2.491\n",
      "Epoch  32 Batch  152/269   train_loss = 2.567\n",
      "Epoch  32 Batch  162/269   train_loss = 2.703\n",
      "Epoch  32 Batch  172/269   train_loss = 2.561\n",
      "Epoch  32 Batch  182/269   train_loss = 2.763\n",
      "Epoch  32 Batch  192/269   train_loss = 2.669\n",
      "Epoch  32 Batch  202/269   train_loss = 2.893\n",
      "Epoch  32 Batch  212/269   train_loss = 2.932\n",
      "Epoch  32 Batch  222/269   train_loss = 2.628\n",
      "Epoch  32 Batch  232/269   train_loss = 2.639\n",
      "Epoch  32 Batch  242/269   train_loss = 2.565\n",
      "Epoch  32 Batch  252/269   train_loss = 2.590\n",
      "Epoch  32 Batch  262/269   train_loss = 2.770\n",
      "Epoch  33 Batch    3/269   train_loss = 2.482\n",
      "Epoch  33 Batch   13/269   train_loss = 2.551\n",
      "Epoch  33 Batch   23/269   train_loss = 2.573\n",
      "Epoch  33 Batch   33/269   train_loss = 2.617\n",
      "Epoch  33 Batch   43/269   train_loss = 2.702\n",
      "Epoch  33 Batch   53/269   train_loss = 2.778\n",
      "Epoch  33 Batch   63/269   train_loss = 2.890\n",
      "Epoch  33 Batch   73/269   train_loss = 2.585\n",
      "Epoch  33 Batch   83/269   train_loss = 2.985\n",
      "Epoch  33 Batch   93/269   train_loss = 2.606\n",
      "Epoch  33 Batch  103/269   train_loss = 2.681\n",
      "Epoch  33 Batch  113/269   train_loss = 2.648\n",
      "Epoch  33 Batch  123/269   train_loss = 2.985\n",
      "Epoch  33 Batch  133/269   train_loss = 2.688\n",
      "Epoch  33 Batch  143/269   train_loss = 2.743\n",
      "Epoch  33 Batch  153/269   train_loss = 2.875\n",
      "Epoch  33 Batch  163/269   train_loss = 2.716\n",
      "Epoch  33 Batch  173/269   train_loss = 2.631\n",
      "Epoch  33 Batch  183/269   train_loss = 2.419\n",
      "Epoch  33 Batch  193/269   train_loss = 2.600\n",
      "Epoch  33 Batch  203/269   train_loss = 2.693\n",
      "Epoch  33 Batch  213/269   train_loss = 2.686\n",
      "Epoch  33 Batch  223/269   train_loss = 2.740\n",
      "Epoch  33 Batch  233/269   train_loss = 2.818\n",
      "Epoch  33 Batch  243/269   train_loss = 2.628\n",
      "Epoch  33 Batch  253/269   train_loss = 2.667\n",
      "Epoch  33 Batch  263/269   train_loss = 2.484\n",
      "Epoch  34 Batch    4/269   train_loss = 2.720\n",
      "Epoch  34 Batch   14/269   train_loss = 2.758\n",
      "Epoch  34 Batch   24/269   train_loss = 2.758\n",
      "Epoch  34 Batch   34/269   train_loss = 2.716\n",
      "Epoch  34 Batch   44/269   train_loss = 2.703\n",
      "Epoch  34 Batch   54/269   train_loss = 2.854\n",
      "Epoch  34 Batch   64/269   train_loss = 2.617\n",
      "Epoch  34 Batch   74/269   train_loss = 2.685\n",
      "Epoch  34 Batch   84/269   train_loss = 2.624\n",
      "Epoch  34 Batch   94/269   train_loss = 2.568\n",
      "Epoch  34 Batch  104/269   train_loss = 2.543\n",
      "Epoch  34 Batch  114/269   train_loss = 2.740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  34 Batch  124/269   train_loss = 2.688\n",
      "Epoch  34 Batch  134/269   train_loss = 2.514\n",
      "Epoch  34 Batch  144/269   train_loss = 2.660\n",
      "Epoch  34 Batch  154/269   train_loss = 2.478\n",
      "Epoch  34 Batch  164/269   train_loss = 2.539\n",
      "Epoch  34 Batch  174/269   train_loss = 2.899\n",
      "Epoch  34 Batch  184/269   train_loss = 3.013\n",
      "Epoch  34 Batch  194/269   train_loss = 2.761\n",
      "Epoch  34 Batch  204/269   train_loss = 2.830\n",
      "Epoch  34 Batch  214/269   train_loss = 2.667\n",
      "Epoch  34 Batch  224/269   train_loss = 2.582\n",
      "Epoch  34 Batch  234/269   train_loss = 2.565\n",
      "Epoch  34 Batch  244/269   train_loss = 2.563\n",
      "Epoch  34 Batch  254/269   train_loss = 2.461\n",
      "Epoch  34 Batch  264/269   train_loss = 2.752\n",
      "Epoch  35 Batch    5/269   train_loss = 2.643\n",
      "Epoch  35 Batch   15/269   train_loss = 2.693\n",
      "Epoch  35 Batch   25/269   train_loss = 2.507\n",
      "Epoch  35 Batch   35/269   train_loss = 2.621\n",
      "Epoch  35 Batch   45/269   train_loss = 2.852\n",
      "Epoch  35 Batch   55/269   train_loss = 2.653\n",
      "Epoch  35 Batch   65/269   train_loss = 2.483\n",
      "Epoch  35 Batch   75/269   train_loss = 2.751\n",
      "Epoch  35 Batch   85/269   train_loss = 2.571\n",
      "Epoch  35 Batch   95/269   train_loss = 2.698\n",
      "Epoch  35 Batch  105/269   train_loss = 2.666\n",
      "Epoch  35 Batch  115/269   train_loss = 2.477\n",
      "Epoch  35 Batch  125/269   train_loss = 2.755\n",
      "Epoch  35 Batch  135/269   train_loss = 2.695\n",
      "Epoch  35 Batch  145/269   train_loss = 2.605\n",
      "Epoch  35 Batch  155/269   train_loss = 2.551\n",
      "Epoch  35 Batch  165/269   train_loss = 2.668\n",
      "Epoch  35 Batch  175/269   train_loss = 2.711\n",
      "Epoch  35 Batch  185/269   train_loss = 2.570\n",
      "Epoch  35 Batch  195/269   train_loss = 2.806\n",
      "Epoch  35 Batch  205/269   train_loss = 2.644\n",
      "Epoch  35 Batch  215/269   train_loss = 2.457\n",
      "Epoch  35 Batch  225/269   train_loss = 2.634\n",
      "Epoch  35 Batch  235/269   train_loss = 2.743\n",
      "Epoch  35 Batch  245/269   train_loss = 2.864\n",
      "Epoch  35 Batch  255/269   train_loss = 2.917\n",
      "Epoch  35 Batch  265/269   train_loss = 2.596\n",
      "Epoch  36 Batch    6/269   train_loss = 2.492\n",
      "Epoch  36 Batch   16/269   train_loss = 2.613\n",
      "Epoch  36 Batch   26/269   train_loss = 2.782\n",
      "Epoch  36 Batch   36/269   train_loss = 2.633\n",
      "Epoch  36 Batch   46/269   train_loss = 2.654\n",
      "Epoch  36 Batch   56/269   train_loss = 2.592\n",
      "Epoch  36 Batch   66/269   train_loss = 2.532\n",
      "Epoch  36 Batch   76/269   train_loss = 2.762\n",
      "Epoch  36 Batch   86/269   train_loss = 2.543\n",
      "Epoch  36 Batch   96/269   train_loss = 2.742\n",
      "Epoch  36 Batch  106/269   train_loss = 2.656\n",
      "Epoch  36 Batch  116/269   train_loss = 2.597\n",
      "Epoch  36 Batch  126/269   train_loss = 2.675\n",
      "Epoch  36 Batch  136/269   train_loss = 2.618\n",
      "Epoch  36 Batch  146/269   train_loss = 2.557\n",
      "Epoch  36 Batch  156/269   train_loss = 2.570\n",
      "Epoch  36 Batch  166/269   train_loss = 2.882\n",
      "Epoch  36 Batch  176/269   train_loss = 2.651\n",
      "Epoch  36 Batch  186/269   train_loss = 2.758\n",
      "Epoch  36 Batch  196/269   train_loss = 2.673\n",
      "Epoch  36 Batch  206/269   train_loss = 2.816\n",
      "Epoch  36 Batch  216/269   train_loss = 2.871\n",
      "Epoch  36 Batch  226/269   train_loss = 2.552\n",
      "Epoch  36 Batch  236/269   train_loss = 2.513\n",
      "Epoch  36 Batch  246/269   train_loss = 2.728\n",
      "Epoch  36 Batch  256/269   train_loss = 2.653\n",
      "Epoch  36 Batch  266/269   train_loss = 2.555\n",
      "Epoch  37 Batch    7/269   train_loss = 2.610\n",
      "Epoch  37 Batch   17/269   train_loss = 2.763\n",
      "Epoch  37 Batch   27/269   train_loss = 2.607\n",
      "Epoch  37 Batch   37/269   train_loss = 2.635\n",
      "Epoch  37 Batch   47/269   train_loss = 2.368\n",
      "Epoch  37 Batch   57/269   train_loss = 2.708\n",
      "Epoch  37 Batch   67/269   train_loss = 2.605\n",
      "Epoch  37 Batch   77/269   train_loss = 2.620\n",
      "Epoch  37 Batch   87/269   train_loss = 2.742\n",
      "Epoch  37 Batch   97/269   train_loss = 2.704\n",
      "Epoch  37 Batch  107/269   train_loss = 2.735\n",
      "Epoch  37 Batch  117/269   train_loss = 2.651\n",
      "Epoch  37 Batch  127/269   train_loss = 2.776\n",
      "Epoch  37 Batch  137/269   train_loss = 2.785\n",
      "Epoch  37 Batch  147/269   train_loss = 2.675\n",
      "Epoch  37 Batch  157/269   train_loss = 2.630\n",
      "Epoch  37 Batch  167/269   train_loss = 2.800\n",
      "Epoch  37 Batch  177/269   train_loss = 2.715\n",
      "Epoch  37 Batch  187/269   train_loss = 2.646\n",
      "Epoch  37 Batch  197/269   train_loss = 2.515\n",
      "Epoch  37 Batch  207/269   train_loss = 2.894\n",
      "Epoch  37 Batch  217/269   train_loss = 2.747\n",
      "Epoch  37 Batch  227/269   train_loss = 2.465\n",
      "Epoch  37 Batch  237/269   train_loss = 2.604\n",
      "Epoch  37 Batch  247/269   train_loss = 2.745\n",
      "Epoch  37 Batch  257/269   train_loss = 2.447\n",
      "Epoch  37 Batch  267/269   train_loss = 2.516\n",
      "Epoch  38 Batch    8/269   train_loss = 2.431\n",
      "Epoch  38 Batch   18/269   train_loss = 2.125\n",
      "Epoch  38 Batch   28/269   train_loss = 2.755\n",
      "Epoch  38 Batch   38/269   train_loss = 2.486\n",
      "Epoch  38 Batch   48/269   train_loss = 2.688\n",
      "Epoch  38 Batch   58/269   train_loss = 2.630\n",
      "Epoch  38 Batch   68/269   train_loss = 2.836\n",
      "Epoch  38 Batch   78/269   train_loss = 2.549\n",
      "Epoch  38 Batch   88/269   train_loss = 2.457\n",
      "Epoch  38 Batch   98/269   train_loss = 2.517\n",
      "Epoch  38 Batch  108/269   train_loss = 2.767\n",
      "Epoch  38 Batch  118/269   train_loss = 2.420\n",
      "Epoch  38 Batch  128/269   train_loss = 2.735\n",
      "Epoch  38 Batch  138/269   train_loss = 2.537\n",
      "Epoch  38 Batch  148/269   train_loss = 2.430\n",
      "Epoch  38 Batch  158/269   train_loss = 2.561\n",
      "Epoch  38 Batch  168/269   train_loss = 2.529\n",
      "Epoch  38 Batch  178/269   train_loss = 2.621\n",
      "Epoch  38 Batch  188/269   train_loss = 2.538\n",
      "Epoch  38 Batch  198/269   train_loss = 2.578\n",
      "Epoch  38 Batch  208/269   train_loss = 2.521\n",
      "Epoch  38 Batch  218/269   train_loss = 2.613\n",
      "Epoch  38 Batch  228/269   train_loss = 2.748\n",
      "Epoch  38 Batch  238/269   train_loss = 2.629\n",
      "Epoch  38 Batch  248/269   train_loss = 2.724\n",
      "Epoch  38 Batch  258/269   train_loss = 2.462\n",
      "Epoch  38 Batch  268/269   train_loss = 2.659\n",
      "Epoch  39 Batch    9/269   train_loss = 2.678\n",
      "Epoch  39 Batch   19/269   train_loss = 2.617\n",
      "Epoch  39 Batch   29/269   train_loss = 2.649\n",
      "Epoch  39 Batch   39/269   train_loss = 2.505\n",
      "Epoch  39 Batch   49/269   train_loss = 2.757\n",
      "Epoch  39 Batch   59/269   train_loss = 2.680\n",
      "Epoch  39 Batch   69/269   train_loss = 2.566\n",
      "Epoch  39 Batch   79/269   train_loss = 2.598\n",
      "Epoch  39 Batch   89/269   train_loss = 2.634\n",
      "Epoch  39 Batch   99/269   train_loss = 2.643\n",
      "Epoch  39 Batch  109/269   train_loss = 2.671\n",
      "Epoch  39 Batch  119/269   train_loss = 2.595\n",
      "Epoch  39 Batch  129/269   train_loss = 2.498\n",
      "Epoch  39 Batch  139/269   train_loss = 2.610\n",
      "Epoch  39 Batch  149/269   train_loss = 2.487\n",
      "Epoch  39 Batch  159/269   train_loss = 2.868\n",
      "Epoch  39 Batch  169/269   train_loss = 2.553\n",
      "Epoch  39 Batch  179/269   train_loss = 2.584\n",
      "Epoch  39 Batch  189/269   train_loss = 2.471\n",
      "Epoch  39 Batch  199/269   train_loss = 2.752\n",
      "Epoch  39 Batch  209/269   train_loss = 2.618\n",
      "Epoch  39 Batch  219/269   train_loss = 2.434\n",
      "Epoch  39 Batch  229/269   train_loss = 2.527\n",
      "Epoch  39 Batch  239/269   train_loss = 2.545\n",
      "Epoch  39 Batch  249/269   train_loss = 2.678\n",
      "Epoch  39 Batch  259/269   train_loss = 2.603\n",
      "Epoch  39 Batch  269/269   train_loss = 2.505\n",
      "Epoch  40 Batch   10/269   train_loss = 2.351\n",
      "Epoch  40 Batch   20/269   train_loss = 2.782\n",
      "Epoch  40 Batch   30/269   train_loss = 2.656\n",
      "Epoch  40 Batch   40/269   train_loss = 2.641\n",
      "Epoch  40 Batch   50/269   train_loss = 2.616\n",
      "Epoch  40 Batch   60/269   train_loss = 2.703\n",
      "Epoch  40 Batch   70/269   train_loss = 2.645\n",
      "Epoch  40 Batch   80/269   train_loss = 2.755\n",
      "Epoch  40 Batch   90/269   train_loss = 2.556\n",
      "Epoch  40 Batch  100/269   train_loss = 2.480\n",
      "Epoch  40 Batch  110/269   train_loss = 2.539\n",
      "Epoch  40 Batch  120/269   train_loss = 2.627\n",
      "Epoch  40 Batch  130/269   train_loss = 2.587\n",
      "Epoch  40 Batch  140/269   train_loss = 2.510\n",
      "Epoch  40 Batch  150/269   train_loss = 2.687\n",
      "Epoch  40 Batch  160/269   train_loss = 2.581\n",
      "Epoch  40 Batch  170/269   train_loss = 2.544\n",
      "Epoch  40 Batch  180/269   train_loss = 2.602\n",
      "Epoch  40 Batch  190/269   train_loss = 2.682\n",
      "Epoch  40 Batch  200/269   train_loss = 2.436\n",
      "Epoch  40 Batch  210/269   train_loss = 2.459\n",
      "Epoch  40 Batch  220/269   train_loss = 2.452\n",
      "Epoch  40 Batch  230/269   train_loss = 2.595\n",
      "Epoch  40 Batch  240/269   train_loss = 2.676\n",
      "Epoch  40 Batch  250/269   train_loss = 2.551\n",
      "Epoch  40 Batch  260/269   train_loss = 2.667\n",
      "Epoch  41 Batch    1/269   train_loss = 2.521\n",
      "Epoch  41 Batch   11/269   train_loss = 2.362\n",
      "Epoch  41 Batch   21/269   train_loss = 2.547\n",
      "Epoch  41 Batch   31/269   train_loss = 2.757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  41 Batch   41/269   train_loss = 2.635\n",
      "Epoch  41 Batch   51/269   train_loss = 2.540\n",
      "Epoch  41 Batch   61/269   train_loss = 2.771\n",
      "Epoch  41 Batch   71/269   train_loss = 2.662\n",
      "Epoch  41 Batch   81/269   train_loss = 2.505\n",
      "Epoch  41 Batch   91/269   train_loss = 2.646\n",
      "Epoch  41 Batch  101/269   train_loss = 2.795\n",
      "Epoch  41 Batch  111/269   train_loss = 2.712\n",
      "Epoch  41 Batch  121/269   train_loss = 2.536\n",
      "Epoch  41 Batch  131/269   train_loss = 2.628\n",
      "Epoch  41 Batch  141/269   train_loss = 2.603\n",
      "Epoch  41 Batch  151/269   train_loss = 2.505\n",
      "Epoch  41 Batch  161/269   train_loss = 2.434\n",
      "Epoch  41 Batch  171/269   train_loss = 2.564\n",
      "Epoch  41 Batch  181/269   train_loss = 2.546\n",
      "Epoch  41 Batch  191/269   train_loss = 2.524\n",
      "Epoch  41 Batch  201/269   train_loss = 2.776\n",
      "Epoch  41 Batch  211/269   train_loss = 2.556\n",
      "Epoch  41 Batch  221/269   train_loss = 2.625\n",
      "Epoch  41 Batch  231/269   train_loss = 2.622\n",
      "Epoch  41 Batch  241/269   train_loss = 2.500\n",
      "Epoch  41 Batch  251/269   train_loss = 2.466\n",
      "Epoch  41 Batch  261/269   train_loss = 2.685\n",
      "Epoch  42 Batch    2/269   train_loss = 2.303\n",
      "Epoch  42 Batch   12/269   train_loss = 2.512\n",
      "Epoch  42 Batch   22/269   train_loss = 2.468\n",
      "Epoch  42 Batch   32/269   train_loss = 2.610\n",
      "Epoch  42 Batch   42/269   train_loss = 2.509\n",
      "Epoch  42 Batch   52/269   train_loss = 2.261\n",
      "Epoch  42 Batch   62/269   train_loss = 2.544\n",
      "Epoch  42 Batch   72/269   train_loss = 2.760\n",
      "Epoch  42 Batch   82/269   train_loss = 2.570\n",
      "Epoch  42 Batch   92/269   train_loss = 2.648\n",
      "Epoch  42 Batch  102/269   train_loss = 2.502\n",
      "Epoch  42 Batch  112/269   train_loss = 2.489\n",
      "Epoch  42 Batch  122/269   train_loss = 2.507\n",
      "Epoch  42 Batch  132/269   train_loss = 2.716\n",
      "Epoch  42 Batch  142/269   train_loss = 2.309\n",
      "Epoch  42 Batch  152/269   train_loss = 2.452\n",
      "Epoch  42 Batch  162/269   train_loss = 2.530\n",
      "Epoch  42 Batch  172/269   train_loss = 2.434\n",
      "Epoch  42 Batch  182/269   train_loss = 2.597\n",
      "Epoch  42 Batch  192/269   train_loss = 2.444\n",
      "Epoch  42 Batch  202/269   train_loss = 2.762\n",
      "Epoch  42 Batch  212/269   train_loss = 2.786\n",
      "Epoch  42 Batch  222/269   train_loss = 2.606\n",
      "Epoch  42 Batch  232/269   train_loss = 2.547\n",
      "Epoch  42 Batch  242/269   train_loss = 2.369\n",
      "Epoch  42 Batch  252/269   train_loss = 2.417\n",
      "Epoch  42 Batch  262/269   train_loss = 2.619\n",
      "Epoch  43 Batch    3/269   train_loss = 2.391\n",
      "Epoch  43 Batch   13/269   train_loss = 2.504\n",
      "Epoch  43 Batch   23/269   train_loss = 2.553\n",
      "Epoch  43 Batch   33/269   train_loss = 2.409\n",
      "Epoch  43 Batch   43/269   train_loss = 2.450\n",
      "Epoch  43 Batch   53/269   train_loss = 2.629\n",
      "Epoch  43 Batch   63/269   train_loss = 2.807\n",
      "Epoch  43 Batch   73/269   train_loss = 2.430\n",
      "Epoch  43 Batch   83/269   train_loss = 2.725\n",
      "Epoch  43 Batch   93/269   train_loss = 2.463\n",
      "Epoch  43 Batch  103/269   train_loss = 2.527\n",
      "Epoch  43 Batch  113/269   train_loss = 2.680\n",
      "Epoch  43 Batch  123/269   train_loss = 2.804\n",
      "Epoch  43 Batch  133/269   train_loss = 2.586\n",
      "Epoch  43 Batch  143/269   train_loss = 2.635\n",
      "Epoch  43 Batch  153/269   train_loss = 2.771\n",
      "Epoch  43 Batch  163/269   train_loss = 2.649\n",
      "Epoch  43 Batch  173/269   train_loss = 2.613\n",
      "Epoch  43 Batch  183/269   train_loss = 2.356\n",
      "Epoch  43 Batch  193/269   train_loss = 2.536\n",
      "Epoch  43 Batch  203/269   train_loss = 2.520\n",
      "Epoch  43 Batch  213/269   train_loss = 2.476\n",
      "Epoch  43 Batch  223/269   train_loss = 2.628\n",
      "Epoch  43 Batch  233/269   train_loss = 2.634\n",
      "Epoch  43 Batch  243/269   train_loss = 2.467\n",
      "Epoch  43 Batch  253/269   train_loss = 2.595\n",
      "Epoch  43 Batch  263/269   train_loss = 2.461\n",
      "Epoch  44 Batch    4/269   train_loss = 2.555\n",
      "Epoch  44 Batch   14/269   train_loss = 2.689\n",
      "Epoch  44 Batch   24/269   train_loss = 2.602\n",
      "Epoch  44 Batch   34/269   train_loss = 2.525\n",
      "Epoch  44 Batch   44/269   train_loss = 2.635\n",
      "Epoch  44 Batch   54/269   train_loss = 2.662\n",
      "Epoch  44 Batch   64/269   train_loss = 2.517\n",
      "Epoch  44 Batch   74/269   train_loss = 2.604\n",
      "Epoch  44 Batch   84/269   train_loss = 2.589\n",
      "Epoch  44 Batch   94/269   train_loss = 2.368\n",
      "Epoch  44 Batch  104/269   train_loss = 2.471\n",
      "Epoch  44 Batch  114/269   train_loss = 2.616\n",
      "Epoch  44 Batch  124/269   train_loss = 2.524\n",
      "Epoch  44 Batch  134/269   train_loss = 2.416\n",
      "Epoch  44 Batch  144/269   train_loss = 2.513\n",
      "Epoch  44 Batch  154/269   train_loss = 2.303\n",
      "Epoch  44 Batch  164/269   train_loss = 2.481\n",
      "Epoch  44 Batch  174/269   train_loss = 2.687\n",
      "Epoch  44 Batch  184/269   train_loss = 2.892\n",
      "Epoch  44 Batch  194/269   train_loss = 2.672\n",
      "Epoch  44 Batch  204/269   train_loss = 2.644\n",
      "Epoch  44 Batch  214/269   train_loss = 2.573\n",
      "Epoch  44 Batch  224/269   train_loss = 2.543\n",
      "Epoch  44 Batch  234/269   train_loss = 2.447\n",
      "Epoch  44 Batch  244/269   train_loss = 2.449\n",
      "Epoch  44 Batch  254/269   train_loss = 2.277\n",
      "Epoch  44 Batch  264/269   train_loss = 2.664\n",
      "Epoch  45 Batch    5/269   train_loss = 2.544\n",
      "Epoch  45 Batch   15/269   train_loss = 2.592\n",
      "Epoch  45 Batch   25/269   train_loss = 2.345\n",
      "Epoch  45 Batch   35/269   train_loss = 2.512\n",
      "Epoch  45 Batch   45/269   train_loss = 2.600\n",
      "Epoch  45 Batch   55/269   train_loss = 2.669\n",
      "Epoch  45 Batch   65/269   train_loss = 2.352\n",
      "Epoch  45 Batch   75/269   train_loss = 2.649\n",
      "Epoch  45 Batch   85/269   train_loss = 2.511\n",
      "Epoch  45 Batch   95/269   train_loss = 2.534\n",
      "Epoch  45 Batch  105/269   train_loss = 2.585\n",
      "Epoch  45 Batch  115/269   train_loss = 2.449\n",
      "Epoch  45 Batch  125/269   train_loss = 2.658\n",
      "Epoch  45 Batch  135/269   train_loss = 2.594\n",
      "Epoch  45 Batch  145/269   train_loss = 2.567\n",
      "Epoch  45 Batch  155/269   train_loss = 2.393\n",
      "Epoch  45 Batch  165/269   train_loss = 2.610\n",
      "Epoch  45 Batch  175/269   train_loss = 2.532\n",
      "Epoch  45 Batch  185/269   train_loss = 2.374\n",
      "Epoch  45 Batch  195/269   train_loss = 2.742\n",
      "Epoch  45 Batch  205/269   train_loss = 2.443\n",
      "Epoch  45 Batch  215/269   train_loss = 2.347\n",
      "Epoch  45 Batch  225/269   train_loss = 2.461\n",
      "Epoch  45 Batch  235/269   train_loss = 2.684\n",
      "Epoch  45 Batch  245/269   train_loss = 2.702\n",
      "Epoch  45 Batch  255/269   train_loss = 2.757\n",
      "Epoch  45 Batch  265/269   train_loss = 2.480\n",
      "Epoch  46 Batch    6/269   train_loss = 2.341\n",
      "Epoch  46 Batch   16/269   train_loss = 2.515\n",
      "Epoch  46 Batch   26/269   train_loss = 2.767\n",
      "Epoch  46 Batch   36/269   train_loss = 2.460\n",
      "Epoch  46 Batch   46/269   train_loss = 2.558\n",
      "Epoch  46 Batch   56/269   train_loss = 2.533\n",
      "Epoch  46 Batch   66/269   train_loss = 2.497\n",
      "Epoch  46 Batch   76/269   train_loss = 2.655\n",
      "Epoch  46 Batch   86/269   train_loss = 2.520\n",
      "Epoch  46 Batch   96/269   train_loss = 2.655\n",
      "Epoch  46 Batch  106/269   train_loss = 2.534\n",
      "Epoch  46 Batch  116/269   train_loss = 2.503\n",
      "Epoch  46 Batch  126/269   train_loss = 2.620\n",
      "Epoch  46 Batch  136/269   train_loss = 2.483\n",
      "Epoch  46 Batch  146/269   train_loss = 2.561\n",
      "Epoch  46 Batch  156/269   train_loss = 2.490\n",
      "Epoch  46 Batch  166/269   train_loss = 2.759\n",
      "Epoch  46 Batch  176/269   train_loss = 2.483\n",
      "Epoch  46 Batch  186/269   train_loss = 2.654\n",
      "Epoch  46 Batch  196/269   train_loss = 2.591\n",
      "Epoch  46 Batch  206/269   train_loss = 2.677\n",
      "Epoch  46 Batch  216/269   train_loss = 2.619\n",
      "Epoch  46 Batch  226/269   train_loss = 2.494\n",
      "Epoch  46 Batch  236/269   train_loss = 2.460\n",
      "Epoch  46 Batch  246/269   train_loss = 2.538\n",
      "Epoch  46 Batch  256/269   train_loss = 2.471\n",
      "Epoch  46 Batch  266/269   train_loss = 2.478\n",
      "Epoch  47 Batch    7/269   train_loss = 2.535\n",
      "Epoch  47 Batch   17/269   train_loss = 2.659\n",
      "Epoch  47 Batch   27/269   train_loss = 2.602\n",
      "Epoch  47 Batch   37/269   train_loss = 2.427\n",
      "Epoch  47 Batch   47/269   train_loss = 2.275\n",
      "Epoch  47 Batch   57/269   train_loss = 2.698\n",
      "Epoch  47 Batch   67/269   train_loss = 2.574\n",
      "Epoch  47 Batch   77/269   train_loss = 2.481\n",
      "Epoch  47 Batch   87/269   train_loss = 2.727\n",
      "Epoch  47 Batch   97/269   train_loss = 2.624\n",
      "Epoch  47 Batch  107/269   train_loss = 2.697\n",
      "Epoch  47 Batch  117/269   train_loss = 2.680\n",
      "Epoch  47 Batch  127/269   train_loss = 2.669\n",
      "Epoch  47 Batch  137/269   train_loss = 2.775\n",
      "Epoch  47 Batch  147/269   train_loss = 2.570\n",
      "Epoch  47 Batch  157/269   train_loss = 2.523\n",
      "Epoch  47 Batch  167/269   train_loss = 2.709\n",
      "Epoch  47 Batch  177/269   train_loss = 2.542\n",
      "Epoch  47 Batch  187/269   train_loss = 2.528\n",
      "Epoch  47 Batch  197/269   train_loss = 2.497\n",
      "Epoch  47 Batch  207/269   train_loss = 2.730\n",
      "Epoch  47 Batch  217/269   train_loss = 2.629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  47 Batch  227/269   train_loss = 2.323\n",
      "Epoch  47 Batch  237/269   train_loss = 2.492\n",
      "Epoch  47 Batch  247/269   train_loss = 2.640\n",
      "Epoch  47 Batch  257/269   train_loss = 2.355\n",
      "Epoch  47 Batch  267/269   train_loss = 2.477\n",
      "Epoch  48 Batch    8/269   train_loss = 2.375\n",
      "Epoch  48 Batch   18/269   train_loss = 2.070\n",
      "Epoch  48 Batch   28/269   train_loss = 2.699\n",
      "Epoch  48 Batch   38/269   train_loss = 2.446\n",
      "Epoch  48 Batch   48/269   train_loss = 2.532\n",
      "Epoch  48 Batch   58/269   train_loss = 2.524\n",
      "Epoch  48 Batch   68/269   train_loss = 2.745\n",
      "Epoch  48 Batch   78/269   train_loss = 2.495\n",
      "Epoch  48 Batch   88/269   train_loss = 2.373\n",
      "Epoch  48 Batch   98/269   train_loss = 2.455\n",
      "Epoch  48 Batch  108/269   train_loss = 2.583\n",
      "Epoch  48 Batch  118/269   train_loss = 2.300\n",
      "Epoch  48 Batch  128/269   train_loss = 2.722\n",
      "Epoch  48 Batch  138/269   train_loss = 2.474\n",
      "Epoch  48 Batch  148/269   train_loss = 2.366\n",
      "Epoch  48 Batch  158/269   train_loss = 2.444\n",
      "Epoch  48 Batch  168/269   train_loss = 2.512\n",
      "Epoch  48 Batch  178/269   train_loss = 2.480\n",
      "Epoch  48 Batch  188/269   train_loss = 2.445\n",
      "Epoch  48 Batch  198/269   train_loss = 2.527\n",
      "Epoch  48 Batch  208/269   train_loss = 2.404\n",
      "Epoch  48 Batch  218/269   train_loss = 2.567\n",
      "Epoch  48 Batch  228/269   train_loss = 2.653\n",
      "Epoch  48 Batch  238/269   train_loss = 2.530\n",
      "Epoch  48 Batch  248/269   train_loss = 2.569\n",
      "Epoch  48 Batch  258/269   train_loss = 2.407\n",
      "Epoch  48 Batch  268/269   train_loss = 2.528\n",
      "Epoch  49 Batch    9/269   train_loss = 2.769\n",
      "Epoch  49 Batch   19/269   train_loss = 2.399\n",
      "Epoch  49 Batch   29/269   train_loss = 2.526\n",
      "Epoch  49 Batch   39/269   train_loss = 2.406\n",
      "Epoch  49 Batch   49/269   train_loss = 2.685\n",
      "Epoch  49 Batch   59/269   train_loss = 2.483\n",
      "Epoch  49 Batch   69/269   train_loss = 2.540\n",
      "Epoch  49 Batch   79/269   train_loss = 2.560\n",
      "Epoch  49 Batch   89/269   train_loss = 2.589\n",
      "Epoch  49 Batch   99/269   train_loss = 2.527\n",
      "Epoch  49 Batch  109/269   train_loss = 2.592\n",
      "Epoch  49 Batch  119/269   train_loss = 2.574\n",
      "Epoch  49 Batch  129/269   train_loss = 2.374\n",
      "Epoch  49 Batch  139/269   train_loss = 2.596\n",
      "Epoch  49 Batch  149/269   train_loss = 2.317\n",
      "Epoch  49 Batch  159/269   train_loss = 2.725\n",
      "Epoch  49 Batch  169/269   train_loss = 2.509\n",
      "Epoch  49 Batch  179/269   train_loss = 2.560\n",
      "Epoch  49 Batch  189/269   train_loss = 2.313\n",
      "Epoch  49 Batch  199/269   train_loss = 2.790\n",
      "Epoch  49 Batch  209/269   train_loss = 2.541\n",
      "Epoch  49 Batch  219/269   train_loss = 2.324\n",
      "Epoch  49 Batch  229/269   train_loss = 2.562\n",
      "Epoch  49 Batch  239/269   train_loss = 2.454\n",
      "Epoch  49 Batch  249/269   train_loss = 2.508\n",
      "Epoch  49 Batch  259/269   train_loss = 2.542\n",
      "Epoch  49 Batch  269/269   train_loss = 2.519\n",
      "Epoch  50 Batch   10/269   train_loss = 2.372\n",
      "Epoch  50 Batch   20/269   train_loss = 2.678\n",
      "Epoch  50 Batch   30/269   train_loss = 2.523\n",
      "Epoch  50 Batch   40/269   train_loss = 2.596\n",
      "Epoch  50 Batch   50/269   train_loss = 2.454\n",
      "Epoch  50 Batch   60/269   train_loss = 2.524\n",
      "Epoch  50 Batch   70/269   train_loss = 2.515\n",
      "Epoch  50 Batch   80/269   train_loss = 2.561\n",
      "Epoch  50 Batch   90/269   train_loss = 2.395\n",
      "Epoch  50 Batch  100/269   train_loss = 2.302\n",
      "Epoch  50 Batch  110/269   train_loss = 2.594\n",
      "Epoch  50 Batch  120/269   train_loss = 2.429\n",
      "Epoch  50 Batch  130/269   train_loss = 2.407\n",
      "Epoch  50 Batch  140/269   train_loss = 2.485\n",
      "Epoch  50 Batch  150/269   train_loss = 2.614\n",
      "Epoch  50 Batch  160/269   train_loss = 2.511\n",
      "Epoch  50 Batch  170/269   train_loss = 2.487\n",
      "Epoch  50 Batch  180/269   train_loss = 2.530\n",
      "Epoch  50 Batch  190/269   train_loss = 2.627\n",
      "Epoch  50 Batch  200/269   train_loss = 2.351\n",
      "Epoch  50 Batch  210/269   train_loss = 2.408\n",
      "Epoch  50 Batch  220/269   train_loss = 2.523\n",
      "Epoch  50 Batch  230/269   train_loss = 2.552\n",
      "Epoch  50 Batch  240/269   train_loss = 2.574\n",
      "Epoch  50 Batch  250/269   train_loss = 2.475\n",
      "Epoch  50 Batch  260/269   train_loss = 2.564\n",
      "\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "rnnTrainer = RNNTrainer()\n",
    "\n",
    "rnnTrainer.train_rnn(rnn, train_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickleHelper = PickleHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickleHelper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, vocab_to_int, int_to_vocab, punc_dict = pickleHelper.load_preprocessed_data()\n",
    "seq_length, load_dir = pickleHelper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating TV Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TVScriptGenerator:\n",
    "    \n",
    "    def generate_tv_script(self, gen_length, prime_word):\n",
    "        \"\"\"\n",
    "        Generate TV script using the trainde RNN model.\n",
    "        \n",
    "        :param gen_length: Generation length\n",
    "        :param prime_word: Prime word to use \n",
    "        :return: Generated TV script\n",
    "        \"\"\"\n",
    "        \n",
    "        loaded_graph = tf.Graph()\n",
    "\n",
    "        tensorLoader = TensorLoader()\n",
    "        wordSelector = WordSelector()\n",
    "        \n",
    "        with tf.Session(graph=loaded_graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Load saved model\n",
    "            loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "            loader.restore(sess, load_dir)\n",
    "\n",
    "            # Get Tensors from loaded model\n",
    "            input_text, initial_state, final_state, probs = \\\n",
    "                tensorLoader.get_tensors(loaded_graph)\n",
    "\n",
    "            # Sentence generation setup\n",
    "            gen_sentences = [prime_word + ':']\n",
    "            feed = {input_text: np.array([[1]])}\n",
    "            prev_state = sess.run(initial_state, feed_dict=feed)\n",
    "\n",
    "            # Generate sentences\n",
    "            for n in range(gen_length):\n",
    "                # Dynamic Input\n",
    "                dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "                dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "                # Get Prediction\n",
    "                feed = {input_text: dyn_input, initial_state: prev_state}\n",
    "                probabilities, prev_state = sess.run([probs, final_state],\n",
    "                                                      feed_dict=feed)\n",
    "\n",
    "                pred_word = wordSelector.pick_word(probabilities[dyn_seq_length-1], \n",
    "                                                   int_to_vocab)\n",
    "                gen_sentences.append(pred_word)\n",
    "\n",
    "            # Remove punctuation tokens\n",
    "            tv_script = ' '.join(gen_sentences)\n",
    "            for key, token in punc_dict.items():\n",
    "                ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "                tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "            tv_script = tv_script.replace('\\n ', '\\n')\n",
    "            tv_script = tv_script.replace('( ', '(')\n",
    "\n",
    "            return tv_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorLoader:\n",
    "\n",
    "    def get_tensors(self, loaded_graph):\n",
    "        \"\"\"\n",
    "        Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "        \n",
    "        :param loaded_graph: TensorFlow graph loaded from file\n",
    "        :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.Session(graph=loaded_graph) as sess:\n",
    "            inputs = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "            initial_state = tf.get_default_graph().get_tensor_by_name(\"initial_state:0\")\n",
    "            final_state = tf.get_default_graph().get_tensor_by_name(\"final_state:0\")\n",
    "            probs = tf.get_default_graph().get_tensor_by_name(\"probs:0\")\n",
    "\n",
    "        return (inputs, initial_state, final_state, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordSelector:\n",
    "    \n",
    "    def pick_word(self, probabilities, int_to_vocab):\n",
    "        \"\"\"\n",
    "        Pick the next word in the generated text.\n",
    "        \n",
    "        :param probabilities: Probabilites of the next word\n",
    "        :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "        :return: String of the predicted word\n",
    "        \"\"\"\n",
    "    #     max_idx = np.argmax(probabilities)\n",
    "    #     return int_to_vocab[max_idx]\n",
    "\n",
    "        top_n = 5\n",
    "        p = np.squeeze(probabilities)\n",
    "        p[np.argsort(p)[:-top_n]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        i = np.random.choice(len(int_to_vocab), 1, p=p)[0]\n",
    "        \n",
    "        return int_to_vocab[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "moe_szyslak: oh, cold out barney, you know what i laid early with a drawing.\n",
      "moe_szyslak: yeah, i don't think. and you ever felt, huh?(determined) i've been working in the summer... what, i swear!\n",
      "moe_szyslak:(à page jerry) you gotta hit the head on.\n",
      "homer_simpson:(awkwardly) you know, i got...\n",
      "moe_szyslak:(rueful) why can't they be back.\n",
      "homer_simpson: hey, watch those are my flashbacks.\n",
      "homer_simpson:(awkwardly) you callin' beer on the wall..\n",
      "\n",
      "homer_simpson: and marge.\n",
      "moe_szyslak: yeah, edna's never! i own the game! it's worse with her hair for each.\n",
      "\n",
      "\n",
      "homer_simpson:(ringing) mistresses?\n",
      "moe_szyslak:(to moe) i want you to pay for it?(scary sigh) i can't believe homer.\n",
      "moe_szyslak: uh, what's wrong with seymour at this filth. i think, marge.\n",
      "moe_szyslak:(turning up a girl body that was philip glass. my wife's fall\n"
     ]
    }
   ],
   "source": [
    "tvScriptGenerator = TVScriptGenerator()\n",
    "\n",
    "tv_script = tvScriptGenerator.generate_tv_script(gen_length, prime_word)\n",
    "\n",
    "print(tv_script)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
