{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def load_data(self, data_dir):\n",
    "        \"\"\"\n",
    "        Load dataset from file.\n",
    "        \n",
    "        :param data_dir: Directory where the dataset is located\n",
    "        \"\"\"\n",
    " \n",
    "        input_file = os.path.join(data_dir)\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './data/simpsons/moes_tavern_lines.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataLoader = DataLoader()\n",
    "\n",
    "text = dataLoader.load_data(data_dir)\n",
    "\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataExplorer:\n",
    "    \n",
    "    def explore_data(self, text, view_sentence_range):\n",
    "        \"\"\"\n",
    "        Explore input data text.\n",
    "        \n",
    "        :param text: Input text to explore\n",
    "        :param view_sentence_range: Range of sentences to display\n",
    "        \"\"\"\n",
    "        \n",
    "        print('Dataset Stats')\n",
    "        print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "        scenes = text.split('\\n\\n')\n",
    "        print('Number of scenes: {}'.format(len(scenes)))\n",
    "        sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "        print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "        sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "        print('Number of lines: {}'.format(len(sentences)))\n",
    "        word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "        print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "        print()\n",
    "        print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "        print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "view_sentence_range = (0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 10:\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataExplorer = DataExplorer()\n",
    "\n",
    "dataExplorer.explore_data(text, view_sentence_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \n",
    "    def preprocess_and_save_data(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess and save text data.\n",
    "        \n",
    "        :param text: The text of tv scripts split into words\n",
    "        \"\"\"\n",
    "\n",
    "        punc_dict = self.create_punc_lookup_table()\n",
    "        \n",
    "        for key, token in punc_dict.items():\n",
    "            text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "        text = text.lower()\n",
    "        text = text.split()\n",
    "\n",
    "        vocab_to_int, int_to_vocab = self.create_lookup_tables(text)\n",
    "        \n",
    "        int_text = [vocab_to_int[word] for word in text]\n",
    "        \n",
    "        PickleHelper().save_preprocessed_data((int_text, vocab_to_int, int_to_vocab, punc_dict))\n",
    "    \n",
    "    \n",
    "    def create_punc_lookup_table(self):\n",
    "        \"\"\"\n",
    "        Generate a dict to turn punctuations into tokens.\n",
    "        \n",
    "        :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "        \"\"\"\n",
    "        \n",
    "        punc_dict = {}\n",
    "        punc_dict['.']  = \"||Period||\"\n",
    "        punc_dict[',']  = \"||Comma||\"\n",
    "        punc_dict['\"']  = \"||Quotation_Mark||\"\n",
    "        punc_dict[';']  = \"||Semicolon||\"\n",
    "        punc_dict['!']  = \"||Exclamation_Mark||\"\n",
    "        punc_dict['?']  = \"||Question_Mark\"\n",
    "        punc_dict['(']  = \"||Left_Parenthesis||\"\n",
    "        punc_dict[')']  = \"||Right_Parenthesis||\"\n",
    "        punc_dict['--'] = \"||Dash||\"\n",
    "        punc_dict['\\n'] = \"||Return||\"\n",
    "\n",
    "        return punc_dict\n",
    "    \n",
    "    \n",
    "    def create_lookup_tables(self, text):\n",
    "        \"\"\"\n",
    "        Create lookup tables for vocabulary.\n",
    "        \n",
    "        :param text: The text of tv scripts split into words\n",
    "        :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "        \"\"\"\n",
    "        words = set(text)\n",
    "        vocab_to_int = {word: ii for ii, word in enumerate(words)}\n",
    "        int_to_vocab = dict(enumerate(words))\n",
    "        return (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PickleHelper:\n",
    "    \n",
    "    def save_preprocessed_data(self, data):\n",
    "        \"\"\"\n",
    "        Save preprocessed training data.\n",
    "        \"\"\"\n",
    "        pickle.dump(data, open('preprocess.p', 'wb'))\n",
    "        \n",
    "    def load_preprocessed_data(self):\n",
    "        \"\"\"\n",
    "        Load the Preprocessed training data and return them in batches of <batch_size> or less.\n",
    "        \"\"\"\n",
    "        return pickle.load(open('preprocess.p', mode='rb'))\n",
    "    \n",
    "    def save_params(self, params):\n",
    "        \"\"\"\n",
    "        Save parameters to file.\n",
    "        \"\"\"\n",
    "        pickle.dump(params, open('params.p', 'wb'))\n",
    "    \n",
    "    def load_params(self):\n",
    "        \"\"\"\n",
    "        Load parameters from file.\n",
    "        \"\"\"\n",
    "        return pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPreprocessor = DataPreprocessor()\n",
    "\n",
    "dataPreprocessor.preprocess_and_save_data(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = PickleHelper().load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Tensorflow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorflowVersionChecker:\n",
    "    \n",
    "    def check_version(self):\n",
    "        # Check tensorflow version\n",
    "        assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), \\\n",
    "            'Please use TensorFlow version 1.0 or newer'\n",
    "        print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "        # Check for a GPU\n",
    "        if not tf.test.gpu_device_name():\n",
    "            warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "        else:\n",
    "            print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "tfVersionChecker = TensorflowVersionChecker()\n",
    "\n",
    "tfVersionChecker.check_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_text = None\n",
    "        self.targets = None\n",
    "        self.lr = None\n",
    "        self.initial_state = None\n",
    "        self.final_state = None\n",
    "        self.cost = None\n",
    "        self.train_op = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNBuilder:\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        \"\"\"\n",
    "        Create TF Placeholders for input, targets, and learning rate.\n",
    "        \n",
    "        :return: Tuple (input, targets, learning_rate)\n",
    "        \"\"\"\n",
    "        inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "        learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        return (inputs, targets, learning_rate)\n",
    "    \n",
    "    \n",
    "    def build_init_cell(self, batch_size, rnn_size, num_rnn_layers, keep_prob):\n",
    "        \"\"\"\n",
    "        Create an RNN Cell and initialize it.\n",
    "        \n",
    "        :param batch_size: Size of batches\n",
    "        :param rnn_size: Size of RNNs\n",
    "        :param num_rnn_layers: Number of RNN (LSTM) layers\n",
    "        :param keep_prob: Keep probability value\n",
    "        :return: Tuple (cell, initial_state)\n",
    "        \"\"\"\n",
    "        \n",
    "        cell = tf.contrib.rnn.MultiRNNCell(\n",
    "                [self.build_lstm_cell(rnn_size, keep_prob) for _ in range(num_rnn_layers)])\n",
    "\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        initial_state = tf.identity(initial_state, name='initial_state')\n",
    "\n",
    "        return (cell, initial_state)\n",
    "    \n",
    "    \n",
    "    def build_lstm_cell(self, rnn_size, keep_prob):\n",
    "        \"\"\" \n",
    "        Build LSTM cell and apply dropout.\n",
    "        \n",
    "        :param rnn_size: Size of RNNs\n",
    "        :param keep_prob: Keep probability value\n",
    "        :return: LSTM cell (with dropout applied)\n",
    "        \"\"\"\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    def get_embed(self, input_data, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Create embedding for <input_data>.\n",
    "        \n",
    "        :param input_data: TF placeholder for text input\n",
    "        :param vocab_size: Number of words in vocabulary\n",
    "        :param embed_dim: Number of embedding dimensions\n",
    "        :return: Embedded input\n",
    "        \"\"\"        \n",
    "        return tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n",
    "    \n",
    "    \n",
    "    def build_rnn(self, cell, inputs):\n",
    "        \"\"\"\n",
    "        Create a RNN using a RNN Cell.\n",
    "        \n",
    "        :param cell: RNN Cell\n",
    "        :param inputs: Input text data\n",
    "        :return: Tuple (outputs, final_state)\n",
    "        \"\"\"\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "        final_state = tf.identity(final_state, name='final_state')\n",
    "        return (outputs, final_state)\n",
    "    \n",
    "    \n",
    "    def build_nn(self, cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Build part of the neural network.\n",
    "        \n",
    "        :param cell: RNN cell\n",
    "        :param rnn_size: Size of rnns\n",
    "        :param input_data: Input data\n",
    "        :param vocab_size: Vocabulary size\n",
    "        :param embed_dim: Number of embedding dimensions\n",
    "        :return: Tuple (logits, final_state)\n",
    "        \"\"\"\n",
    "        \n",
    "        embed = self.get_embed(input_data, vocab_size, embed_dim)\n",
    "        outputs, final_state = self.build_rnn(cell, embed)\n",
    "        \n",
    "        logits = tf.contrib.layers.fully_connected(inputs=outputs,\n",
    "                                                   num_outputs=vocab_size,\n",
    "                                                   activation_fn=None)\n",
    "        \n",
    "        return (logits, final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNGraphBuilder:\n",
    "    \n",
    "    def build_rnn_graph(self, \n",
    "                        int_to_vocab, \n",
    "                        rnn_size,\n",
    "                        num_rnn_layers,\n",
    "                        keep_prob,\n",
    "                        embed_dim):\n",
    "        \n",
    "        \"\"\"\n",
    "        Build RNN graph.\n",
    "        \n",
    "        :param int_to_vocab: Integer mapping of input text words\n",
    "        :param rnn_size: Size of rnns\n",
    "        :param num_rnn_layers: Number of RNN (LSTM) layers\n",
    "        :param keep_prob: Keep probability value\n",
    "        :param embed_dim: Number of embedding dimensions\n",
    "        :return: Tuple (rnn, train_graph, probs) \n",
    "        \"\"\"\n",
    "        \n",
    "        train_graph = tf.Graph()\n",
    "        \n",
    "        rnn = RNN()\n",
    "        rnnBuilder = RNNBuilder()\n",
    "        optimizerTuner = OptimizerTuner()\n",
    "\n",
    "        with train_graph.as_default():\n",
    "            # Placeholders\n",
    "            input_text, targets, lr = rnnBuilder.create_placeholders()\n",
    "            rnn.input_text, rnn.targets, rnn.lr = input_text, targets, lr\n",
    "            \n",
    "            # Cell, Initial State\n",
    "            input_data_shape = tf.shape(input_text)\n",
    "            cell, initial_state = rnnBuilder.build_init_cell(input_data_shape[0], \n",
    "                                                             rnn_size, \n",
    "                                                             num_rnn_layers, \n",
    "                                                             keep_prob)\n",
    "            rnn.initial_state = initial_state\n",
    " \n",
    "            # Logits, Final State\n",
    "            vocab_size = len(int_to_vocab)\n",
    "            logits, final_state = rnnBuilder.build_nn(cell, \n",
    "                                                      rnn_size, \n",
    "                                                      input_text, \n",
    "                                                      vocab_size, \n",
    "                                                      embed_dim)\n",
    "            rnn.final_state = final_state\n",
    "\n",
    "            # Probabilities for generating words\n",
    "            probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "            # Loss function\n",
    "            cost = seq2seq.sequence_loss(logits, \n",
    "                                         targets, \n",
    "                                         tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "            # Optimizer\n",
    "            optimizer = tf.train.AdamOptimizer(lr)\n",
    "            train_op = optimizerTuner.get_gradient_clipped_optimizer(optimizer, cost)\n",
    "            rnn.cost, rnn.train_op = cost, train_op\n",
    "            \n",
    "            return (rnn, train_graph, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OptimizerTuner:\n",
    "    \n",
    "    def get_gradient_clipped_optimizer(self, optimizer, cost):\n",
    "        \"\"\"\n",
    "        Apply gradient clipping to optimizer.\n",
    "        \n",
    "        :param optimizer: Optimizer to apply gradient clipping to\n",
    "        :param cost: Loss function\n",
    "        :return: Optimizer with gradient clipping\n",
    "        \"\"\"\n",
    "        \n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) \\\n",
    "                            for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        \n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_size = 1024 # [25, 1024]\n",
    "num_rnn_layers = 2 \n",
    "keep_prob = 0.75\n",
    "embed_dim = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnGraphBuilder = RNNGraphBuilder()\n",
    "\n",
    "rnn, train_graph, probs = rnnGraphBuilder.build_rnn_graph(int_to_vocab, \n",
    "                                                          rnn_size,\n",
    "                                                          num_rnn_layers,\n",
    "                                                          keep_prob,\n",
    "                                                          embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    \n",
    "    def train_rnn(self, rnn, train_graph):\n",
    "        \"\"\"\n",
    "        Train and save RNN model.\n",
    "        \n",
    "        :param rnn: RNN to train and save\n",
    "        :param train_graph: RNN graph\n",
    "        \"\"\"\n",
    "            \n",
    "        with tf.Session(graph=train_graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            dataBatchGenerator = DataBatchGenerator()\n",
    "            batches = dataBatchGenerator.get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "            for epoch_i in range(num_epochs):\n",
    "                feed = {rnn.input_text: batches[0][0]}\n",
    "                \n",
    "                state = sess.run(rnn.initial_state, \n",
    "                                 feed_dict=feed)\n",
    "\n",
    "                for batch_i, (x, y) in enumerate(batches):\n",
    "                    feed = {rnn.input_text: x,\n",
    "                            rnn.targets: y,\n",
    "                            rnn.initial_state: state,\n",
    "                            rnn.lr: learning_rate}\n",
    "                    \n",
    "                    train_loss, state, _ = sess.run([rnn.cost, rnn.final_state, rnn.train_op],\n",
    "                                                     feed_dict=feed)\n",
    "\n",
    "                    # Show every <show_every_n_batches> batches\n",
    "                    if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                        print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                                epoch_i+1,\n",
    "                                batch_i+1,\n",
    "                                len(batches),\n",
    "                                train_loss))\n",
    "\n",
    "            # Save model\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, save_dir)\n",
    "            \n",
    "            print('\\nModel Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataBatchGenerator:\n",
    "\n",
    "    def get_batches(self, int_text, batch_size, seq_len):\n",
    "        \"\"\"\n",
    "        Return batches of input and target.\n",
    "        \n",
    "        :param int_text: Text with the words replaced by their ids\n",
    "        :param batch_size: The size of batch\n",
    "        :param seq_len: The length of sequence\n",
    "        :return: Batches as a Numpy array\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Example Input:\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)\n",
    "        \n",
    "        Example Output:\n",
    "        [\n",
    "          # First Batch\n",
    "          [\n",
    "            # Batch of Input\n",
    "            [[ 1  2], [ 7  8], [13 14]]\n",
    "            # Batch of targets\n",
    "            [[ 2  3], [ 8  9], [14 15]]\n",
    "          ]\n",
    "\n",
    "          # Second Batch\n",
    "          [\n",
    "            # Batch of Input\n",
    "            [[ 3  4], [ 9 10], [15 16]]\n",
    "            # Batch of targets\n",
    "            [[ 4  5], [10 11], [16 17]]\n",
    "          ]\n",
    "\n",
    "          # Third Batch\n",
    "          [\n",
    "            # Batch of Input\n",
    "            [[ 5  6], [11 12], [17 18]]\n",
    "            # Batch of targets\n",
    "            [[ 6  7], [12 13], [18  1]]\n",
    "          ]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        \n",
    "        n_chars_per_batch = batch_size * seq_len\n",
    "        n_batches = int(len(int_text) / n_chars_per_batch)\n",
    "        full_batch_size = n_batches * n_chars_per_batch\n",
    "\n",
    "        # Drop the last few characters to make only full batches\n",
    "        x_data = np.array(int_text[: full_batch_size])\n",
    "        y_data = np.array(int_text[1: full_batch_size+1])\n",
    "\n",
    "        x_batches = np.split(x_data.reshape(batch_size, -1), n_batches, 1)\n",
    "        y_batches = np.split(y_data.reshape(batch_size, -1), n_batches, 1)\n",
    "\n",
    "        first_x_char = x_batches[0][0][0]\n",
    "        y_batches[n_batches-1][batch_size-1][-1] = first_x_char\n",
    "\n",
    "        return np.array(list(zip(x_batches, y_batches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01 # [0.001, 0.7]\n",
    "num_epochs = 100 # [20, 400]\n",
    "batch_size = 128\n",
    "seq_length = 10 # [5, 50]\n",
    "show_every_n_batches = 10\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch    1/53   train_loss = 8.822\n",
      "Epoch   1 Batch   11/53   train_loss = 6.425\n",
      "Epoch   1 Batch   21/53   train_loss = 6.513\n",
      "Epoch   1 Batch   31/53   train_loss = 6.319\n",
      "Epoch   1 Batch   41/53   train_loss = 6.287\n",
      "Epoch   1 Batch   51/53   train_loss = 6.169\n",
      "Epoch   2 Batch    8/53   train_loss = 6.253\n",
      "Epoch   2 Batch   18/53   train_loss = 6.164\n",
      "Epoch   2 Batch   28/53   train_loss = 6.144\n",
      "Epoch   2 Batch   38/53   train_loss = 6.021\n",
      "Epoch   2 Batch   48/53   train_loss = 6.000\n",
      "Epoch   3 Batch    5/53   train_loss = 5.970\n",
      "Epoch   3 Batch   15/53   train_loss = 6.050\n",
      "Epoch   3 Batch   25/53   train_loss = 5.998\n",
      "Epoch   3 Batch   35/53   train_loss = 5.965\n",
      "Epoch   3 Batch   45/53   train_loss = 5.996\n",
      "Epoch   4 Batch    2/53   train_loss = 5.961\n",
      "Epoch   4 Batch   12/53   train_loss = 5.807\n",
      "Epoch   4 Batch   22/53   train_loss = 6.078\n",
      "Epoch   4 Batch   32/53   train_loss = 5.739\n",
      "Epoch   4 Batch   42/53   train_loss = 5.571\n",
      "Epoch   4 Batch   52/53   train_loss = 5.612\n",
      "Epoch   5 Batch    9/53   train_loss = 5.438\n",
      "Epoch   5 Batch   19/53   train_loss = 5.499\n",
      "Epoch   5 Batch   29/53   train_loss = 5.223\n",
      "Epoch   5 Batch   39/53   train_loss = 5.163\n",
      "Epoch   5 Batch   49/53   train_loss = 5.230\n",
      "Epoch   6 Batch    6/53   train_loss = 5.149\n",
      "Epoch   6 Batch   16/53   train_loss = 5.229\n",
      "Epoch   6 Batch   26/53   train_loss = 5.190\n",
      "Epoch   6 Batch   36/53   train_loss = 5.011\n",
      "Epoch   6 Batch   46/53   train_loss = 4.781\n",
      "Epoch   7 Batch    3/53   train_loss = 4.744\n",
      "Epoch   7 Batch   13/53   train_loss = 4.817\n",
      "Epoch   7 Batch   23/53   train_loss = 4.796\n",
      "Epoch   7 Batch   33/53   train_loss = 4.764\n",
      "Epoch   7 Batch   43/53   train_loss = 4.765\n",
      "Epoch   7 Batch   53/53   train_loss = 4.531\n",
      "Epoch   8 Batch   10/53   train_loss = 4.838\n",
      "Epoch   8 Batch   20/53   train_loss = 4.491\n",
      "Epoch   8 Batch   30/53   train_loss = 4.731\n",
      "Epoch   8 Batch   40/53   train_loss = 4.609\n",
      "Epoch   8 Batch   50/53   train_loss = 4.483\n",
      "Epoch   9 Batch    7/53   train_loss = 4.423\n",
      "Epoch   9 Batch   17/53   train_loss = 4.498\n",
      "Epoch   9 Batch   27/53   train_loss = 4.667\n",
      "Epoch   9 Batch   37/53   train_loss = 4.415\n",
      "Epoch   9 Batch   47/53   train_loss = 4.458\n",
      "Epoch  10 Batch    4/53   train_loss = 4.354\n",
      "Epoch  10 Batch   14/53   train_loss = 4.455\n",
      "Epoch  10 Batch   24/53   train_loss = 4.515\n",
      "Epoch  10 Batch   34/53   train_loss = 4.317\n",
      "Epoch  10 Batch   44/53   train_loss = 4.207\n",
      "Epoch  11 Batch    1/53   train_loss = 4.476\n",
      "Epoch  11 Batch   11/53   train_loss = 4.238\n",
      "Epoch  11 Batch   21/53   train_loss = 4.420\n",
      "Epoch  11 Batch   31/53   train_loss = 4.278\n",
      "Epoch  11 Batch   41/53   train_loss = 4.279\n",
      "Epoch  11 Batch   51/53   train_loss = 4.284\n",
      "Epoch  12 Batch    8/53   train_loss = 4.369\n",
      "Epoch  12 Batch   18/53   train_loss = 4.114\n",
      "Epoch  12 Batch   28/53   train_loss = 4.129\n",
      "Epoch  12 Batch   38/53   train_loss = 4.096\n",
      "Epoch  12 Batch   48/53   train_loss = 4.043\n",
      "Epoch  13 Batch    5/53   train_loss = 4.063\n",
      "Epoch  13 Batch   15/53   train_loss = 4.095\n",
      "Epoch  13 Batch   25/53   train_loss = 3.963\n",
      "Epoch  13 Batch   35/53   train_loss = 3.863\n",
      "Epoch  13 Batch   45/53   train_loss = 3.972\n",
      "Epoch  14 Batch    2/53   train_loss = 3.866\n",
      "Epoch  14 Batch   12/53   train_loss = 3.783\n",
      "Epoch  14 Batch   22/53   train_loss = 4.070\n",
      "Epoch  14 Batch   32/53   train_loss = 3.715\n",
      "Epoch  14 Batch   42/53   train_loss = 3.725\n",
      "Epoch  14 Batch   52/53   train_loss = 3.888\n",
      "Epoch  15 Batch    9/53   train_loss = 3.770\n",
      "Epoch  15 Batch   19/53   train_loss = 3.837\n",
      "Epoch  15 Batch   29/53   train_loss = 3.707\n",
      "Epoch  15 Batch   39/53   train_loss = 3.594\n",
      "Epoch  15 Batch   49/53   train_loss = 3.781\n",
      "Epoch  16 Batch    6/53   train_loss = 3.766\n",
      "Epoch  16 Batch   16/53   train_loss = 3.830\n",
      "Epoch  16 Batch   26/53   train_loss = 3.762\n",
      "Epoch  16 Batch   36/53   train_loss = 3.544\n",
      "Epoch  16 Batch   46/53   train_loss = 3.474\n",
      "Epoch  17 Batch    3/53   train_loss = 3.586\n",
      "Epoch  17 Batch   13/53   train_loss = 3.591\n",
      "Epoch  17 Batch   23/53   train_loss = 3.549\n",
      "Epoch  17 Batch   33/53   train_loss = 3.432\n",
      "Epoch  17 Batch   43/53   train_loss = 3.323\n",
      "Epoch  17 Batch   53/53   train_loss = 3.423\n",
      "Epoch  18 Batch   10/53   train_loss = 3.476\n",
      "Epoch  18 Batch   20/53   train_loss = 3.320\n",
      "Epoch  18 Batch   30/53   train_loss = 3.457\n",
      "Epoch  18 Batch   40/53   train_loss = 3.256\n",
      "Epoch  18 Batch   50/53   train_loss = 3.316\n",
      "Epoch  19 Batch    7/53   train_loss = 3.348\n",
      "Epoch  19 Batch   17/53   train_loss = 3.139\n",
      "Epoch  19 Batch   27/53   train_loss = 3.243\n",
      "Epoch  19 Batch   37/53   train_loss = 3.230\n",
      "Epoch  19 Batch   47/53   train_loss = 3.142\n",
      "Epoch  20 Batch    4/53   train_loss = 3.148\n",
      "Epoch  20 Batch   14/53   train_loss = 3.224\n",
      "Epoch  20 Batch   24/53   train_loss = 3.161\n",
      "Epoch  20 Batch   34/53   train_loss = 3.097\n",
      "Epoch  20 Batch   44/53   train_loss = 3.005\n",
      "Epoch  21 Batch    1/53   train_loss = 3.132\n",
      "Epoch  21 Batch   11/53   train_loss = 3.005\n",
      "Epoch  21 Batch   21/53   train_loss = 3.035\n",
      "Epoch  21 Batch   31/53   train_loss = 3.034\n",
      "Epoch  21 Batch   41/53   train_loss = 3.021\n",
      "Epoch  21 Batch   51/53   train_loss = 3.000\n",
      "Epoch  22 Batch    8/53   train_loss = 3.062\n",
      "Epoch  22 Batch   18/53   train_loss = 2.878\n",
      "Epoch  22 Batch   28/53   train_loss = 2.953\n",
      "Epoch  22 Batch   38/53   train_loss = 2.915\n",
      "Epoch  22 Batch   48/53   train_loss = 2.889\n",
      "Epoch  23 Batch    5/53   train_loss = 2.871\n",
      "Epoch  23 Batch   15/53   train_loss = 2.904\n",
      "Epoch  23 Batch   25/53   train_loss = 2.649\n",
      "Epoch  23 Batch   35/53   train_loss = 2.710\n",
      "Epoch  23 Batch   45/53   train_loss = 2.777\n",
      "Epoch  24 Batch    2/53   train_loss = 2.576\n",
      "Epoch  24 Batch   12/53   train_loss = 2.673\n",
      "Epoch  24 Batch   22/53   train_loss = 2.614\n",
      "Epoch  24 Batch   32/53   train_loss = 2.604\n",
      "Epoch  24 Batch   42/53   train_loss = 2.606\n",
      "Epoch  24 Batch   52/53   train_loss = 2.574\n",
      "Epoch  25 Batch    9/53   train_loss = 2.498\n",
      "Epoch  25 Batch   19/53   train_loss = 2.615\n",
      "Epoch  25 Batch   29/53   train_loss = 2.559\n",
      "Epoch  25 Batch   39/53   train_loss = 2.483\n",
      "Epoch  25 Batch   49/53   train_loss = 2.472\n",
      "Epoch  26 Batch    6/53   train_loss = 2.572\n",
      "Epoch  26 Batch   16/53   train_loss = 2.502\n",
      "Epoch  26 Batch   26/53   train_loss = 2.494\n",
      "Epoch  26 Batch   36/53   train_loss = 2.412\n",
      "Epoch  26 Batch   46/53   train_loss = 2.375\n",
      "Epoch  27 Batch    3/53   train_loss = 2.447\n",
      "Epoch  27 Batch   13/53   train_loss = 2.342\n",
      "Epoch  27 Batch   23/53   train_loss = 2.320\n",
      "Epoch  27 Batch   33/53   train_loss = 2.191\n",
      "Epoch  27 Batch   43/53   train_loss = 2.219\n",
      "Epoch  27 Batch   53/53   train_loss = 2.307\n",
      "Epoch  28 Batch   10/53   train_loss = 2.213\n",
      "Epoch  28 Batch   20/53   train_loss = 2.230\n",
      "Epoch  28 Batch   30/53   train_loss = 2.262\n",
      "Epoch  28 Batch   40/53   train_loss = 2.158\n",
      "Epoch  28 Batch   50/53   train_loss = 2.158\n",
      "Epoch  29 Batch    7/53   train_loss = 2.270\n",
      "Epoch  29 Batch   17/53   train_loss = 1.987\n",
      "Epoch  29 Batch   27/53   train_loss = 2.029\n",
      "Epoch  29 Batch   37/53   train_loss = 2.212\n",
      "Epoch  29 Batch   47/53   train_loss = 2.091\n",
      "Epoch  30 Batch    4/53   train_loss = 2.066\n",
      "Epoch  30 Batch   14/53   train_loss = 2.187\n",
      "Epoch  30 Batch   24/53   train_loss = 2.120\n",
      "Epoch  30 Batch   34/53   train_loss = 2.098\n",
      "Epoch  30 Batch   44/53   train_loss = 2.080\n",
      "Epoch  31 Batch    1/53   train_loss = 2.110\n",
      "Epoch  31 Batch   11/53   train_loss = 2.004\n",
      "Epoch  31 Batch   21/53   train_loss = 1.981\n",
      "Epoch  31 Batch   31/53   train_loss = 2.012\n",
      "Epoch  31 Batch   41/53   train_loss = 2.006\n",
      "Epoch  31 Batch   51/53   train_loss = 2.043\n",
      "Epoch  32 Batch    8/53   train_loss = 1.991\n",
      "Epoch  32 Batch   18/53   train_loss = 1.950\n",
      "Epoch  32 Batch   28/53   train_loss = 1.941\n",
      "Epoch  32 Batch   38/53   train_loss = 1.932\n",
      "Epoch  32 Batch   48/53   train_loss = 1.972\n",
      "Epoch  33 Batch    5/53   train_loss = 1.930\n",
      "Epoch  33 Batch   15/53   train_loss = 1.904\n",
      "Epoch  33 Batch   25/53   train_loss = 1.776\n",
      "Epoch  33 Batch   35/53   train_loss = 1.793\n",
      "Epoch  33 Batch   45/53   train_loss = 1.868\n",
      "Epoch  34 Batch    2/53   train_loss = 1.769\n",
      "Epoch  34 Batch   12/53   train_loss = 1.861\n",
      "Epoch  34 Batch   22/53   train_loss = 1.738\n",
      "Epoch  34 Batch   32/53   train_loss = 1.807\n",
      "Epoch  34 Batch   42/53   train_loss = 1.692\n",
      "Epoch  34 Batch   52/53   train_loss = 1.750\n",
      "Epoch  35 Batch    9/53   train_loss = 1.654\n",
      "Epoch  35 Batch   19/53   train_loss = 1.751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35 Batch   29/53   train_loss = 1.676\n",
      "Epoch  35 Batch   39/53   train_loss = 1.713\n",
      "Epoch  35 Batch   49/53   train_loss = 1.764\n",
      "Epoch  36 Batch    6/53   train_loss = 1.769\n",
      "Epoch  36 Batch   16/53   train_loss = 1.705\n",
      "Epoch  36 Batch   26/53   train_loss = 1.727\n",
      "Epoch  36 Batch   36/53   train_loss = 1.581\n",
      "Epoch  36 Batch   46/53   train_loss = 1.726\n",
      "Epoch  37 Batch    3/53   train_loss = 1.780\n",
      "Epoch  37 Batch   13/53   train_loss = 1.678\n",
      "Epoch  37 Batch   23/53   train_loss = 1.596\n",
      "Epoch  37 Batch   33/53   train_loss = 1.562\n",
      "Epoch  37 Batch   43/53   train_loss = 1.526\n",
      "Epoch  37 Batch   53/53   train_loss = 1.669\n",
      "Epoch  38 Batch   10/53   train_loss = 1.507\n",
      "Epoch  38 Batch   20/53   train_loss = 1.607\n",
      "Epoch  38 Batch   30/53   train_loss = 1.593\n",
      "Epoch  38 Batch   40/53   train_loss = 1.574\n",
      "Epoch  38 Batch   50/53   train_loss = 1.536\n",
      "Epoch  39 Batch    7/53   train_loss = 1.560\n",
      "Epoch  39 Batch   17/53   train_loss = 1.481\n",
      "Epoch  39 Batch   27/53   train_loss = 1.466\n",
      "Epoch  39 Batch   37/53   train_loss = 1.622\n",
      "Epoch  39 Batch   47/53   train_loss = 1.525\n",
      "Epoch  40 Batch    4/53   train_loss = 1.478\n",
      "Epoch  40 Batch   14/53   train_loss = 1.566\n",
      "Epoch  40 Batch   24/53   train_loss = 1.512\n",
      "Epoch  40 Batch   34/53   train_loss = 1.480\n",
      "Epoch  40 Batch   44/53   train_loss = 1.523\n",
      "Epoch  41 Batch    1/53   train_loss = 1.529\n",
      "Epoch  41 Batch   11/53   train_loss = 1.458\n",
      "Epoch  41 Batch   21/53   train_loss = 1.395\n",
      "Epoch  41 Batch   31/53   train_loss = 1.433\n",
      "Epoch  41 Batch   41/53   train_loss = 1.413\n",
      "Epoch  41 Batch   51/53   train_loss = 1.405\n",
      "Epoch  42 Batch    8/53   train_loss = 1.438\n",
      "Epoch  42 Batch   18/53   train_loss = 1.391\n",
      "Epoch  42 Batch   28/53   train_loss = 1.417\n",
      "Epoch  42 Batch   38/53   train_loss = 1.434\n",
      "Epoch  42 Batch   48/53   train_loss = 1.425\n",
      "Epoch  43 Batch    5/53   train_loss = 1.494\n",
      "Epoch  43 Batch   15/53   train_loss = 1.473\n",
      "Epoch  43 Batch   25/53   train_loss = 1.389\n",
      "Epoch  43 Batch   35/53   train_loss = 1.387\n",
      "Epoch  43 Batch   45/53   train_loss = 1.421\n",
      "Epoch  44 Batch    2/53   train_loss = 1.328\n",
      "Epoch  44 Batch   12/53   train_loss = 1.421\n",
      "Epoch  44 Batch   22/53   train_loss = 1.297\n",
      "Epoch  44 Batch   32/53   train_loss = 1.400\n",
      "Epoch  44 Batch   42/53   train_loss = 1.317\n",
      "Epoch  44 Batch   52/53   train_loss = 1.333\n",
      "Epoch  45 Batch    9/53   train_loss = 1.294\n",
      "Epoch  45 Batch   19/53   train_loss = 1.410\n",
      "Epoch  45 Batch   29/53   train_loss = 1.345\n",
      "Epoch  45 Batch   39/53   train_loss = 1.342\n",
      "Epoch  45 Batch   49/53   train_loss = 1.315\n",
      "Epoch  46 Batch    6/53   train_loss = 1.427\n",
      "Epoch  46 Batch   16/53   train_loss = 1.383\n",
      "Epoch  46 Batch   26/53   train_loss = 1.429\n",
      "Epoch  46 Batch   36/53   train_loss = 1.267\n",
      "Epoch  46 Batch   46/53   train_loss = 1.320\n",
      "Epoch  47 Batch    3/53   train_loss = 1.370\n",
      "Epoch  47 Batch   13/53   train_loss = 1.350\n",
      "Epoch  47 Batch   23/53   train_loss = 1.341\n",
      "Epoch  47 Batch   33/53   train_loss = 1.234\n",
      "Epoch  47 Batch   43/53   train_loss = 1.303\n",
      "Epoch  47 Batch   53/53   train_loss = 1.418\n",
      "Epoch  48 Batch   10/53   train_loss = 1.208\n",
      "Epoch  48 Batch   20/53   train_loss = 1.293\n",
      "Epoch  48 Batch   30/53   train_loss = 1.245\n",
      "Epoch  48 Batch   40/53   train_loss = 1.249\n",
      "Epoch  48 Batch   50/53   train_loss = 1.278\n",
      "Epoch  49 Batch    7/53   train_loss = 1.315\n",
      "Epoch  49 Batch   17/53   train_loss = 1.230\n",
      "Epoch  49 Batch   27/53   train_loss = 1.211\n",
      "Epoch  49 Batch   37/53   train_loss = 1.299\n",
      "Epoch  49 Batch   47/53   train_loss = 1.271\n",
      "Epoch  50 Batch    4/53   train_loss = 1.192\n",
      "Epoch  50 Batch   14/53   train_loss = 1.344\n",
      "Epoch  50 Batch   24/53   train_loss = 1.215\n",
      "Epoch  50 Batch   34/53   train_loss = 1.265\n",
      "Epoch  50 Batch   44/53   train_loss = 1.239\n",
      "Epoch  51 Batch    1/53   train_loss = 1.240\n",
      "Epoch  51 Batch   11/53   train_loss = 1.208\n",
      "Epoch  51 Batch   21/53   train_loss = 1.242\n",
      "Epoch  51 Batch   31/53   train_loss = 1.187\n",
      "Epoch  51 Batch   41/53   train_loss = 1.231\n",
      "Epoch  51 Batch   51/53   train_loss = 1.214\n",
      "Epoch  52 Batch    8/53   train_loss = 1.212\n",
      "Epoch  52 Batch   18/53   train_loss = 1.184\n",
      "Epoch  52 Batch   28/53   train_loss = 1.212\n",
      "Epoch  52 Batch   38/53   train_loss = 1.163\n",
      "Epoch  52 Batch   48/53   train_loss = 1.274\n",
      "Epoch  53 Batch    5/53   train_loss = 1.213\n",
      "Epoch  53 Batch   15/53   train_loss = 1.191\n",
      "Epoch  53 Batch   25/53   train_loss = 1.143\n",
      "Epoch  53 Batch   35/53   train_loss = 1.149\n",
      "Epoch  53 Batch   45/53   train_loss = 1.222\n",
      "Epoch  54 Batch    2/53   train_loss = 1.156\n",
      "Epoch  54 Batch   12/53   train_loss = 1.193\n",
      "Epoch  54 Batch   22/53   train_loss = 1.154\n",
      "Epoch  54 Batch   32/53   train_loss = 1.184\n",
      "Epoch  54 Batch   42/53   train_loss = 1.114\n",
      "Epoch  54 Batch   52/53   train_loss = 1.185\n",
      "Epoch  55 Batch    9/53   train_loss = 1.139\n",
      "Epoch  55 Batch   19/53   train_loss = 1.159\n",
      "Epoch  55 Batch   29/53   train_loss = 1.169\n",
      "Epoch  55 Batch   39/53   train_loss = 1.227\n",
      "Epoch  55 Batch   49/53   train_loss = 1.196\n",
      "Epoch  56 Batch    6/53   train_loss = 1.207\n",
      "Epoch  56 Batch   16/53   train_loss = 1.164\n",
      "Epoch  56 Batch   26/53   train_loss = 1.217\n",
      "Epoch  56 Batch   36/53   train_loss = 1.098\n",
      "Epoch  56 Batch   46/53   train_loss = 1.170\n",
      "Epoch  57 Batch    3/53   train_loss = 1.184\n",
      "Epoch  57 Batch   13/53   train_loss = 1.170\n",
      "Epoch  57 Batch   23/53   train_loss = 1.151\n",
      "Epoch  57 Batch   33/53   train_loss = 1.139\n",
      "Epoch  57 Batch   43/53   train_loss = 1.090\n",
      "Epoch  57 Batch   53/53   train_loss = 1.110\n",
      "Epoch  58 Batch   10/53   train_loss = 1.144\n",
      "Epoch  58 Batch   20/53   train_loss = 1.217\n",
      "Epoch  58 Batch   30/53   train_loss = 1.110\n",
      "Epoch  58 Batch   40/53   train_loss = 1.108\n",
      "Epoch  58 Batch   50/53   train_loss = 1.092\n",
      "Epoch  59 Batch    7/53   train_loss = 1.166\n",
      "Epoch  59 Batch   17/53   train_loss = 1.045\n",
      "Epoch  59 Batch   27/53   train_loss = 1.043\n",
      "Epoch  59 Batch   37/53   train_loss = 1.143\n",
      "Epoch  59 Batch   47/53   train_loss = 1.134\n",
      "Epoch  60 Batch    4/53   train_loss = 1.114\n",
      "Epoch  60 Batch   14/53   train_loss = 1.135\n",
      "Epoch  60 Batch   24/53   train_loss = 1.118\n",
      "Epoch  60 Batch   34/53   train_loss = 1.082\n",
      "Epoch  60 Batch   44/53   train_loss = 1.080\n",
      "Epoch  61 Batch    1/53   train_loss = 1.063\n",
      "Epoch  61 Batch   11/53   train_loss = 1.067\n",
      "Epoch  61 Batch   21/53   train_loss = 1.146\n",
      "Epoch  61 Batch   31/53   train_loss = 1.077\n",
      "Epoch  61 Batch   41/53   train_loss = 1.109\n",
      "Epoch  61 Batch   51/53   train_loss = 1.110\n",
      "Epoch  62 Batch    8/53   train_loss = 1.120\n",
      "Epoch  62 Batch   18/53   train_loss = 1.141\n",
      "Epoch  62 Batch   28/53   train_loss = 1.131\n",
      "Epoch  62 Batch   38/53   train_loss = 1.089\n",
      "Epoch  62 Batch   48/53   train_loss = 1.083\n",
      "Epoch  63 Batch    5/53   train_loss = 1.117\n",
      "Epoch  63 Batch   15/53   train_loss = 1.063\n",
      "Epoch  63 Batch   25/53   train_loss = 1.070\n",
      "Epoch  63 Batch   35/53   train_loss = 1.060\n",
      "Epoch  63 Batch   45/53   train_loss = 1.161\n",
      "Epoch  64 Batch    2/53   train_loss = 1.135\n",
      "Epoch  64 Batch   12/53   train_loss = 1.118\n",
      "Epoch  64 Batch   22/53   train_loss = 1.016\n",
      "Epoch  64 Batch   32/53   train_loss = 1.096\n",
      "Epoch  64 Batch   42/53   train_loss = 1.056\n",
      "Epoch  64 Batch   52/53   train_loss = 1.051\n",
      "Epoch  65 Batch    9/53   train_loss = 1.071\n",
      "Epoch  65 Batch   19/53   train_loss = 1.124\n",
      "Epoch  65 Batch   29/53   train_loss = 1.087\n",
      "Epoch  65 Batch   39/53   train_loss = 1.099\n",
      "Epoch  65 Batch   49/53   train_loss = 1.082\n",
      "Epoch  66 Batch    6/53   train_loss = 1.120\n",
      "Epoch  66 Batch   16/53   train_loss = 1.069\n",
      "Epoch  66 Batch   26/53   train_loss = 1.099\n",
      "Epoch  66 Batch   36/53   train_loss = 1.045\n",
      "Epoch  66 Batch   46/53   train_loss = 1.012\n",
      "Epoch  67 Batch    3/53   train_loss = 1.110\n",
      "Epoch  67 Batch   13/53   train_loss = 1.015\n",
      "Epoch  67 Batch   23/53   train_loss = 1.114\n",
      "Epoch  67 Batch   33/53   train_loss = 1.022\n",
      "Epoch  67 Batch   43/53   train_loss = 1.000\n",
      "Epoch  67 Batch   53/53   train_loss = 1.091\n",
      "Epoch  68 Batch   10/53   train_loss = 0.985\n",
      "Epoch  68 Batch   20/53   train_loss = 1.049\n",
      "Epoch  68 Batch   30/53   train_loss = 1.027\n",
      "Epoch  68 Batch   40/53   train_loss = 1.039\n",
      "Epoch  68 Batch   50/53   train_loss = 1.006\n",
      "Epoch  69 Batch    7/53   train_loss = 1.147\n",
      "Epoch  69 Batch   17/53   train_loss = 1.072\n",
      "Epoch  69 Batch   27/53   train_loss = 0.997\n",
      "Epoch  69 Batch   37/53   train_loss = 1.083\n",
      "Epoch  69 Batch   47/53   train_loss = 1.011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  70 Batch    4/53   train_loss = 1.019\n",
      "Epoch  70 Batch   14/53   train_loss = 1.028\n",
      "Epoch  70 Batch   24/53   train_loss = 1.071\n",
      "Epoch  70 Batch   34/53   train_loss = 1.029\n",
      "Epoch  70 Batch   44/53   train_loss = 1.060\n",
      "Epoch  71 Batch    1/53   train_loss = 1.067\n",
      "Epoch  71 Batch   11/53   train_loss = 1.068\n",
      "Epoch  71 Batch   21/53   train_loss = 1.016\n",
      "Epoch  71 Batch   31/53   train_loss = 1.056\n",
      "Epoch  71 Batch   41/53   train_loss = 0.995\n",
      "Epoch  71 Batch   51/53   train_loss = 1.035\n",
      "Epoch  72 Batch    8/53   train_loss = 1.026\n",
      "Epoch  72 Batch   18/53   train_loss = 0.985\n",
      "Epoch  72 Batch   28/53   train_loss = 1.020\n",
      "Epoch  72 Batch   38/53   train_loss = 1.047\n",
      "Epoch  72 Batch   48/53   train_loss = 1.025\n",
      "Epoch  73 Batch    5/53   train_loss = 1.037\n",
      "Epoch  73 Batch   15/53   train_loss = 1.021\n",
      "Epoch  73 Batch   25/53   train_loss = 0.998\n",
      "Epoch  73 Batch   35/53   train_loss = 1.027\n",
      "Epoch  73 Batch   45/53   train_loss = 1.017\n",
      "Epoch  74 Batch    2/53   train_loss = 1.018\n",
      "Epoch  74 Batch   12/53   train_loss = 1.019\n",
      "Epoch  74 Batch   22/53   train_loss = 0.923\n",
      "Epoch  74 Batch   32/53   train_loss = 0.983\n",
      "Epoch  74 Batch   42/53   train_loss = 1.086\n",
      "Epoch  74 Batch   52/53   train_loss = 0.990\n",
      "Epoch  75 Batch    9/53   train_loss = 0.921\n",
      "Epoch  75 Batch   19/53   train_loss = 1.054\n",
      "Epoch  75 Batch   29/53   train_loss = 1.068\n",
      "Epoch  75 Batch   39/53   train_loss = 1.076\n",
      "Epoch  75 Batch   49/53   train_loss = 1.034\n",
      "Epoch  76 Batch    6/53   train_loss = 1.040\n",
      "Epoch  76 Batch   16/53   train_loss = 1.010\n",
      "Epoch  76 Batch   26/53   train_loss = 1.069\n",
      "Epoch  76 Batch   36/53   train_loss = 0.984\n",
      "Epoch  76 Batch   46/53   train_loss = 1.031\n",
      "Epoch  77 Batch    3/53   train_loss = 1.000\n",
      "Epoch  77 Batch   13/53   train_loss = 1.111\n",
      "Epoch  77 Batch   23/53   train_loss = 1.037\n",
      "Epoch  77 Batch   33/53   train_loss = 1.012\n",
      "Epoch  77 Batch   43/53   train_loss = 1.023\n",
      "Epoch  77 Batch   53/53   train_loss = 0.993\n",
      "Epoch  78 Batch   10/53   train_loss = 0.909\n",
      "Epoch  78 Batch   20/53   train_loss = 1.024\n",
      "Epoch  78 Batch   30/53   train_loss = 1.001\n",
      "Epoch  78 Batch   40/53   train_loss = 0.914\n",
      "Epoch  78 Batch   50/53   train_loss = 1.021\n",
      "Epoch  79 Batch    7/53   train_loss = 1.003\n",
      "Epoch  79 Batch   17/53   train_loss = 0.976\n",
      "Epoch  79 Batch   27/53   train_loss = 0.856\n",
      "Epoch  79 Batch   37/53   train_loss = 1.082\n",
      "Epoch  79 Batch   47/53   train_loss = 0.972\n",
      "Epoch  80 Batch    4/53   train_loss = 0.934\n",
      "Epoch  80 Batch   14/53   train_loss = 0.998\n",
      "Epoch  80 Batch   24/53   train_loss = 1.062\n",
      "Epoch  80 Batch   34/53   train_loss = 1.003\n",
      "Epoch  80 Batch   44/53   train_loss = 1.038\n",
      "Epoch  81 Batch    1/53   train_loss = 0.966\n",
      "Epoch  81 Batch   11/53   train_loss = 0.992\n",
      "Epoch  81 Batch   21/53   train_loss = 1.005\n",
      "Epoch  81 Batch   31/53   train_loss = 1.021\n",
      "Epoch  81 Batch   41/53   train_loss = 1.043\n",
      "Epoch  81 Batch   51/53   train_loss = 1.015\n",
      "Epoch  82 Batch    8/53   train_loss = 1.002\n",
      "Epoch  82 Batch   18/53   train_loss = 0.927\n",
      "Epoch  82 Batch   28/53   train_loss = 1.019\n",
      "Epoch  82 Batch   38/53   train_loss = 1.024\n",
      "Epoch  82 Batch   48/53   train_loss = 0.996\n",
      "Epoch  83 Batch    5/53   train_loss = 1.006\n",
      "Epoch  83 Batch   15/53   train_loss = 1.004\n",
      "Epoch  83 Batch   25/53   train_loss = 0.933\n",
      "Epoch  83 Batch   35/53   train_loss = 1.012\n",
      "Epoch  83 Batch   45/53   train_loss = 1.009\n",
      "Epoch  84 Batch    2/53   train_loss = 0.973\n",
      "Epoch  84 Batch   12/53   train_loss = 0.983\n",
      "Epoch  84 Batch   22/53   train_loss = 0.957\n",
      "Epoch  84 Batch   32/53   train_loss = 0.947\n",
      "Epoch  84 Batch   42/53   train_loss = 0.954\n",
      "Epoch  84 Batch   52/53   train_loss = 1.001\n",
      "Epoch  85 Batch    9/53   train_loss = 0.949\n",
      "Epoch  85 Batch   19/53   train_loss = 0.988\n",
      "Epoch  85 Batch   29/53   train_loss = 1.016\n",
      "Epoch  85 Batch   39/53   train_loss = 1.063\n",
      "Epoch  85 Batch   49/53   train_loss = 1.022\n",
      "Epoch  86 Batch    6/53   train_loss = 0.991\n",
      "Epoch  86 Batch   16/53   train_loss = 1.024\n",
      "Epoch  86 Batch   26/53   train_loss = 1.057\n",
      "Epoch  86 Batch   36/53   train_loss = 1.024\n",
      "Epoch  86 Batch   46/53   train_loss = 0.958\n",
      "Epoch  87 Batch    3/53   train_loss = 1.009\n",
      "Epoch  87 Batch   13/53   train_loss = 0.923\n",
      "Epoch  87 Batch   23/53   train_loss = 0.940\n",
      "Epoch  87 Batch   33/53   train_loss = 0.960\n",
      "Epoch  87 Batch   43/53   train_loss = 1.000\n",
      "Epoch  87 Batch   53/53   train_loss = 1.071\n",
      "Epoch  88 Batch   10/53   train_loss = 0.952\n",
      "Epoch  88 Batch   20/53   train_loss = 1.035\n",
      "Epoch  88 Batch   30/53   train_loss = 1.036\n",
      "Epoch  88 Batch   40/53   train_loss = 0.986\n",
      "Epoch  88 Batch   50/53   train_loss = 0.974\n",
      "Epoch  89 Batch    7/53   train_loss = 1.004\n",
      "Epoch  89 Batch   17/53   train_loss = 0.971\n",
      "Epoch  89 Batch   27/53   train_loss = 0.932\n",
      "Epoch  89 Batch   37/53   train_loss = 1.016\n",
      "Epoch  89 Batch   47/53   train_loss = 0.996\n",
      "Epoch  90 Batch    4/53   train_loss = 0.974\n",
      "Epoch  90 Batch   14/53   train_loss = 1.011\n",
      "Epoch  90 Batch   24/53   train_loss = 0.997\n",
      "Epoch  90 Batch   34/53   train_loss = 0.955\n",
      "Epoch  90 Batch   44/53   train_loss = 0.966\n",
      "Epoch  91 Batch    1/53   train_loss = 1.026\n",
      "Epoch  91 Batch   11/53   train_loss = 0.965\n",
      "Epoch  91 Batch   21/53   train_loss = 0.961\n",
      "Epoch  91 Batch   31/53   train_loss = 0.984\n",
      "Epoch  91 Batch   41/53   train_loss = 0.943\n",
      "Epoch  91 Batch   51/53   train_loss = 1.008\n",
      "Epoch  92 Batch    8/53   train_loss = 1.024\n",
      "Epoch  92 Batch   18/53   train_loss = 0.962\n",
      "Epoch  92 Batch   28/53   train_loss = 0.963\n",
      "Epoch  92 Batch   38/53   train_loss = 0.944\n",
      "Epoch  92 Batch   48/53   train_loss = 0.984\n",
      "Epoch  93 Batch    5/53   train_loss = 0.992\n",
      "Epoch  93 Batch   15/53   train_loss = 0.981\n",
      "Epoch  93 Batch   25/53   train_loss = 0.952\n",
      "Epoch  93 Batch   35/53   train_loss = 0.997\n",
      "Epoch  93 Batch   45/53   train_loss = 0.986\n",
      "Epoch  94 Batch    2/53   train_loss = 0.975\n",
      "Epoch  94 Batch   12/53   train_loss = 0.995\n",
      "Epoch  94 Batch   22/53   train_loss = 0.926\n",
      "Epoch  94 Batch   32/53   train_loss = 1.004\n",
      "Epoch  94 Batch   42/53   train_loss = 1.032\n",
      "Epoch  94 Batch   52/53   train_loss = 0.973\n",
      "Epoch  95 Batch    9/53   train_loss = 0.955\n",
      "Epoch  95 Batch   19/53   train_loss = 1.006\n",
      "Epoch  95 Batch   29/53   train_loss = 0.963\n",
      "Epoch  95 Batch   39/53   train_loss = 1.044\n",
      "Epoch  95 Batch   49/53   train_loss = 0.983\n",
      "Epoch  96 Batch    6/53   train_loss = 1.062\n",
      "Epoch  96 Batch   16/53   train_loss = 0.986\n",
      "Epoch  96 Batch   26/53   train_loss = 0.990\n",
      "Epoch  96 Batch   36/53   train_loss = 0.955\n",
      "Epoch  96 Batch   46/53   train_loss = 0.992\n",
      "Epoch  97 Batch    3/53   train_loss = 0.961\n",
      "Epoch  97 Batch   13/53   train_loss = 0.932\n",
      "Epoch  97 Batch   23/53   train_loss = 1.040\n",
      "Epoch  97 Batch   33/53   train_loss = 0.937\n",
      "Epoch  97 Batch   43/53   train_loss = 0.960\n",
      "Epoch  97 Batch   53/53   train_loss = 1.022\n",
      "Epoch  98 Batch   10/53   train_loss = 0.907\n",
      "Epoch  98 Batch   20/53   train_loss = 0.933\n",
      "Epoch  98 Batch   30/53   train_loss = 0.950\n",
      "Epoch  98 Batch   40/53   train_loss = 0.976\n",
      "Epoch  98 Batch   50/53   train_loss = 1.007\n",
      "Epoch  99 Batch    7/53   train_loss = 0.986\n",
      "Epoch  99 Batch   17/53   train_loss = 0.961\n",
      "Epoch  99 Batch   27/53   train_loss = 0.874\n",
      "Epoch  99 Batch   37/53   train_loss = 0.979\n",
      "Epoch  99 Batch   47/53   train_loss = 0.946\n",
      "Epoch 100 Batch    4/53   train_loss = 1.042\n",
      "Epoch 100 Batch   14/53   train_loss = 0.998\n",
      "Epoch 100 Batch   24/53   train_loss = 0.963\n",
      "Epoch 100 Batch   34/53   train_loss = 1.023\n",
      "Epoch 100 Batch   44/53   train_loss = 0.963\n",
      "\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "rnnTrainer = RNNTrainer()\n",
    "\n",
    "rnnTrainer.train_rnn(rnn, train_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickleHelper = PickleHelper()\n",
    "\n",
    "pickleHelper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, vocab_to_int, int_to_vocab, punc_dict = pickleHelper.load_preprocessed_data()\n",
    "seq_length, load_dir = pickleHelper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating TV Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TVScriptGenerator:\n",
    "    \n",
    "    def generate_tv_script(self, gen_length, prime_word):\n",
    "        \"\"\"\n",
    "        Generate TV script using the trainded RNN model.\n",
    "        \n",
    "        :param gen_length: Generation length\n",
    "        :param prime_word: Prime word to use \n",
    "        :return: Generated TV script\n",
    "        \"\"\"\n",
    "        \n",
    "        loaded_graph = tf.Graph()\n",
    "\n",
    "        tensorLoader = TensorLoader()\n",
    "        wordSelector = WordSelector()\n",
    "        sentenceFormatter = SentenceFormatter()\n",
    "        \n",
    "        with tf.Session(graph=loaded_graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Load saved model\n",
    "            loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "            loader.restore(sess, load_dir)\n",
    "\n",
    "            # Get Tensors from loaded model\n",
    "            input_text, initial_state, final_state, probs = \\\n",
    "                tensorLoader.get_tensors(loaded_graph)\n",
    "\n",
    "            # Sentence generation setup\n",
    "            gen_sentences = [prime_word + ':']\n",
    "            feed = {input_text: np.array([[1]])}\n",
    "            prev_state = sess.run(initial_state, \n",
    "                                  feed_dict=feed)\n",
    "\n",
    "            # Generate sentences\n",
    "            for n in range(gen_length):\n",
    "                # Dynamic input\n",
    "                dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "                dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "                # Get prediction\n",
    "                feed = {input_text: dyn_input, \n",
    "                        initial_state: prev_state}\n",
    "                probabilities, prev_state = sess.run([probs, final_state],\n",
    "                                                      feed_dict=feed)\n",
    "\n",
    "                pred_word = wordSelector.pick_word(probabilities[dyn_seq_length-1], \n",
    "                                                   int_to_vocab)\n",
    "                gen_sentences.append(pred_word)\n",
    "\n",
    "            # Format generated sentences\n",
    "            tv_script = sentenceFormatter.format_generated_sentences(gen_sentences)\n",
    "            \n",
    "            return tv_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorLoader:\n",
    "\n",
    "    def get_tensors(self, loaded_graph):\n",
    "        \"\"\"\n",
    "        Get input, initial_state, final_state, and pros tensor from <loaded_graph>.\n",
    "        \n",
    "        :param loaded_graph: TensorFlow graph loaded from file\n",
    "        :return: Tuple (input, initial_state, final_state, probs)\n",
    "        \"\"\"\n",
    "        \n",
    "        with loaded_graph.as_default():\n",
    "            inputs = loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "            initial_state = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "            final_state = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "            probs = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "            \n",
    "        return (inputs, initial_state, final_state, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordSelector:\n",
    "    \n",
    "    def pick_word(self, probabilities, int_to_vocab):\n",
    "        \"\"\"\n",
    "        Pick the next word in the generated text.\n",
    "        \n",
    "        :param probabilities: Probabilites of the next word\n",
    "        :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "        :return: String of the predicted word\n",
    "        \"\"\"\n",
    "        top_n = 5\n",
    "        p = np.squeeze(probabilities)\n",
    "        p[np.argsort(p)[:-top_n]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        i = np.random.choice(len(int_to_vocab), 1, p=p)[0]\n",
    "        return int_to_vocab[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentenceFormatter:\n",
    "    \n",
    "    def format_generated_sentences(self, gen_sentences):\n",
    "        \"\"\"\n",
    "        Remove punctuation tokens from generated sentences.\n",
    "        \n",
    "        :param gen_sentences: Generated sentences\n",
    "        :return: Formatted sentences\n",
    "        \"\"\"\n",
    "        \n",
    "        tv_script = ' '.join(gen_sentences)\n",
    "        \n",
    "        for key, token in punc_dict.items():\n",
    "            ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "            tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "        \n",
    "        tv_script = tv_script.replace('\\n ', '\\n')\n",
    "        tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "        return tv_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "moe_szyslak:(lying) no, i can't let you where we know.\n",
      "moe_szyslak: well i'm not your friends could i sell that?\n",
      "homer_simpson: how about you. you have internet robbin' the bar...\n",
      "homer_simpson:(contemplates century) huh...(then) that was before, he anybody seen me no collateral, 'cause i was just like a night.\n",
      "homer_simpson:(sobs) but a lot... i'll knit it you've got dinner in the music.\n",
      "\n",
      "\n",
      "homer_simpson: i'm an idiot. like that?\n",
      "moe_szyslak: how many girls put off over and every place of us.\n",
      "moe_szyslak:(counting out, to the little more sensitivity out, i don't want to be no bad in the card... apulina.\n",
      "moe_szyslak: the drunk that in the world won't have to stand for three shut up.\n",
      "moe_szyslak: well, you...\n",
      "homer_simpson:(to moe) you said is that dog the fire could don't do a name-- oh, you're taking plotz.\n",
      "homer_simpson:(excited\n"
     ]
    }
   ],
   "source": [
    "tvScriptGenerator = TVScriptGenerator()\n",
    "\n",
    "tv_script = tvScriptGenerator.generate_tv_script(gen_length, prime_word)\n",
    "\n",
    "print(tv_script)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
