{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Prediction RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \n",
    "    def load_and_preprocess_data(self, \n",
    "                                 reviews_file='reviews.txt', \n",
    "                                 labels_file='labels.txt', \n",
    "                                 seq_len=200):\n",
    "        data = Data()\n",
    "        \n",
    "        dataLoader = DataLoader()\n",
    "        data.reviews, data.labels = dataLoader.load_data(reviews_file, labels_file)\n",
    "        \n",
    "        dataExtractor = DataExtractor()\n",
    "        data.reviews = dataExtractor.remove_punctuation(data.reviews)\n",
    "        data.words = dataExtractor.extract_words(data.reviews)\n",
    "        \n",
    "        dataEncoder = DataEncoder()\n",
    "        data.words_to_ints = dataEncoder.map_words_to_ints(data.words)\n",
    "        data.reviews_to_ints = dataEncoder.map_reviews_to_ints(data.reviews, data.words_to_ints)\n",
    "        data.labels_to_ints = dataEncoder.map_labels_to_ints(data.labels)\n",
    "        \n",
    "        dataFilterer = DataFilterer()\n",
    "        data.reviews_to_ints, data.labels_to_ints = dataFilterer.filter_out_zero_len_entries(\n",
    "            data.reviews_to_ints, data.labels_to_ints)\n",
    "        \n",
    "        dataFormatter = DataFormatter()\n",
    "        data.features = dataFormatter.format_features(data.reviews_to_ints, seq_len)\n",
    "        data.targets = data.labels_to_ints\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reviews = None\n",
    "        self.labels = None\n",
    "        self.words = None\n",
    "        self.words_to_ints = None\n",
    "        self.reviews_to_ints = None\n",
    "        self.labels_to_ints = None\n",
    "        self.features = None\n",
    "        self.targets = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def load_data(self, reviews_file, labels_file):\n",
    "        with open(reviews_file, 'r') as f:\n",
    "            reviews = f.read()\n",
    "        with open(labels_file, 'r') as f:\n",
    "            labels = f.read()\n",
    "        self.log_data(reviews, labels)\n",
    "        return reviews, labels\n",
    "    \n",
    "    def log_data(self, reviews, labels):\n",
    "        print(\"Loaded data\\n\")\n",
    "        print(\"reviews[:100]:\\n{}\\n\".format(reviews[:100]))\n",
    "        print(\"labels[:100]:\\n{}\\n\\n\".format(labels[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "\n",
    "    def remove_punctuation(self, review_text):\n",
    "        all_text = ''.join([c for c in review_text if c not in punctuation])\n",
    "        reviews = all_text.split('\\n')\n",
    "        self.log_reviews(reviews)\n",
    "        return reviews\n",
    "    \n",
    "    def extract_words(self, reviews):\n",
    "        all_text = ' '.join(reviews)\n",
    "        words = all_text.split()\n",
    "        self.log_words(words)\n",
    "        return words\n",
    "    \n",
    "    def log_reviews(self, reviews):\n",
    "        print(\"Removed punctuation from review text\\n\")\n",
    "        print(\"reviews[:1]:\\n{}\\n\\n\".format(reviews[:1]))\n",
    "    \n",
    "    def log_words(self, words):\n",
    "        print(\"Extracted words\\n\")\n",
    "        print(\"words[:100]:\\n{}\\n\\n\".format(words[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataEncoder:\n",
    "    \n",
    "    def map_words_to_ints(self, words):\n",
    "        counts = Counter(words)\n",
    "        vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "        words_to_ints = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "        self.log_words_to_ints(words, words_to_ints)\n",
    "        return words_to_ints\n",
    "    \n",
    "    def map_reviews_to_ints(self, reviews, words_to_ints):\n",
    "        reviews_to_ints = []\n",
    "        for review in reviews:\n",
    "            reviews_to_ints.append([words_to_ints[word] for word in review.split()])\n",
    "        self.log_reviews_to_ints(reviews_to_ints)\n",
    "        return reviews_to_ints\n",
    "    \n",
    "    def map_labels_to_ints(self, labels):\n",
    "        labels = labels.split('\\n')\n",
    "        labels_to_ints = np.array([1 if each == 'positive' else 0 for each in labels])\n",
    "        self.log_labels_to_ints(labels_to_ints)\n",
    "        return labels_to_ints\n",
    "  \n",
    "    def log_words_to_ints(self, words, words_to_ints):\n",
    "        print(\"Mapped words to ints\\n\")\n",
    "        for word in words[:10]:\n",
    "            print('{}: {}'.format(word, words_to_ints[word]))\n",
    "        print(\"\\n\")    \n",
    "            \n",
    "    def log_reviews_to_ints(self, reviews_to_ints):\n",
    "        print(\"Mapped reviews to ints\\n\")\n",
    "        print(\"reviews_to_ints[0]:\\n{}\\n\\n\".format(reviews_to_ints[0]))\n",
    "        \n",
    "    def log_labels_to_ints(self, labels_to_ints):\n",
    "        print(\"Mapped labels to ints\\n\")\n",
    "        print(\"labels_to_ints[:10]:\\n{}\\n\\n\".format(labels_to_ints[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFilterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFilterer:\n",
    "    \n",
    "    def filter_out_zero_len_entries(self, reviews_to_ints, labels_to_ints):\n",
    "        non_zero_len_indices = \\\n",
    "            [ii for ii, review in enumerate(reviews_to_ints) if len(review) > 0]\n",
    "        reviews_to_ints = [reviews_to_ints[ii] for ii in non_zero_len_indices]\n",
    "        labels_to_ints = np.array([labels_to_ints[ii] for ii in non_zero_len_indices])\n",
    "        self.log()\n",
    "        return reviews_to_ints, labels_to_ints\n",
    "    \n",
    "    def log(self):\n",
    "        print(\"Filtered out zero-len entries\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFormatter:\n",
    "    \n",
    "    def format_features(self, reviews_to_ints, seq_len):\n",
    "        \"\"\"\n",
    "        truncate each review to seq_len\n",
    "        add zero-padding to the left if review_len < seq_len\n",
    "        \"\"\"\n",
    "        \n",
    "        features = np.zeros((len(reviews_to_ints), seq_len), dtype=int)\n",
    "        \n",
    "        #for i, row in enumerate(reviews_ints):\n",
    "        #    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "        \n",
    "        for i, review in enumerate(reviews_to_ints):\n",
    "            review_len = len(review)\n",
    "            \n",
    "            if review_len >= seq_len:\n",
    "                features[i] = review[:seq_len]\n",
    "            else:\n",
    "                offset = seq_len - review_len\n",
    "                features[i,:offset] = 0\n",
    "                features[i,offset:] = review\n",
    "        \n",
    "        self.log_features(features)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def log_features(self, features):\n",
    "        print(\"Formatted features\\n\")\n",
    "        print(\"features[:5,:100]:\\n{}\\n\\n\".format(features[:5,:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSetCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSetCreator:\n",
    "    \n",
    "    def create_training_validation_and_testing_sets(self, features, targets, split_frac=0.8):\n",
    "        split_idx = int(len(features)*split_frac)\n",
    "        \n",
    "        train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "        train_y, val_y = targets[:split_idx], targets[split_idx:]\n",
    "        \n",
    "        test_idx = int(len(val_x)*0.5)\n",
    "        \n",
    "        val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "        val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "        \n",
    "        self.log_feature_shapes(train_x, val_x, test_x)\n",
    "        \n",
    "        return DataSets(train_x, train_y, val_x, val_y, test_x, test_y)\n",
    "\n",
    "    def log_feature_shapes(self, train_x, val_x, test_x):\n",
    "        print(\"Created training, validation, and testing sets\\n\")\n",
    "        print(\"Training set features shape: \\t{}\".format(train_x.shape))\n",
    "        print(\"Validation set features shape: \\t{}\".format(val_x.shape))\n",
    "        print(\"Testing set features shape: \\t{}\\n\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSets:\n",
    "    \n",
    "    def __init__(self, train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.val_x = val_x\n",
    "        self.val_y = val_y\n",
    "        self.test_x = test_x\n",
    "        self.test_y = test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        self.log_graph()\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        with self.graph.as_default():\n",
    "            self.inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "            self.labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "            self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "            self.log_placeholders()\n",
    "    \n",
    "    def add_embedding_layer(self, n_words, embed_size=300):\n",
    "        with self.graph.as_default():\n",
    "            self.embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "            self.embed = tf.nn.embedding_lookup(self.embedding, self.inputs_)\n",
    "            self.log_embedding_layer()\n",
    "\n",
    "    def add_lstm_layers(self, lstm_size=256, lstm_layers=1, batch_size=500):\n",
    "        with self.graph.as_default():\n",
    "            self.lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "            self.drop = tf.contrib.rnn.DropoutWrapper(self.lstm, output_keep_prob=self.keep_prob)\n",
    "            self.cell = tf.contrib.rnn.MultiRNNCell([self.drop]*lstm_layers)\n",
    "            self.init_state = self.cell.zero_state(batch_size, tf.float32)\n",
    "            self.log_lstm_layers()\n",
    "    \n",
    "    def add_forward_pass(self):\n",
    "        with self.graph.as_default():\n",
    "            self.outputs, self.final_state = tf.nn.dynamic_rnn(self.cell, \n",
    "                                                               self.embed, \n",
    "                                                               initial_state=self.init_state)\n",
    "        self.log_forward_pass()\n",
    "        \n",
    "    def add_train_loss_computation(self, learning_rate=0.001):\n",
    "        with self.graph.as_default():\n",
    "            self.predictions = tf.contrib.layers.fully_connected(self.outputs[:, -1], \n",
    "                                                                 1, \n",
    "                                                                 activation_fn=tf.sigmoid)\n",
    "            self.cost = tf.losses.mean_squared_error(self.labels_, self.predictions)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "        self.log_train_loss_computation() \n",
    "    \n",
    "    def add_validation_accuracy_computation(self):\n",
    "        with self.graph.as_default():\n",
    "            self.correct_pred = tf.equal(tf.cast(tf.round(self.predictions), tf.int32), \n",
    "                                         self.labels_)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "        self.log_validation_accuracy_computation()    \n",
    "    \n",
    "    def log_graph(self):\n",
    "        print(\"Created graph object\\n\")\n",
    "    \n",
    "    def log_placeholders(self):\n",
    "        print(\"Created placeholders\\n\")\n",
    "        print(\"inputs_: {}\".format(self.inputs_))\n",
    "        print(\"labels_: {}\".format(self.labels_))\n",
    "        print(\"keep_prob_: {}\\n\".format(self.keep_prob))\n",
    "        \n",
    "    def log_embedding_layer(self):\n",
    "        print(\"Added embedding layer\\n\")\n",
    "        \n",
    "    def log_lstm_layers(self):\n",
    "        print(\"Added LSTM layers\\n\")\n",
    "        \n",
    "    def log_forward_pass(self):\n",
    "        print(\"Added forward pass\\n\")\n",
    "        \n",
    "    def log_train_loss_computation(self):\n",
    "        print(\"Added training loss computation\\n\")\n",
    "\n",
    "    def log_validation_accuracy_computation(self):\n",
    "        print(\"Added validation accuracy computation\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetworkTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NetworkTrainer:\n",
    "    \n",
    "    def __init__(self, network, datasets, epochs=10, batch_size=500):\n",
    "        self.network = network\n",
    "        self.datasets = datasets\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def train_network(self):\n",
    "        with self.network.graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "        with tf.Session(graph=self.network.graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            iteration = 1\n",
    "            \n",
    "            for e in range(self.epochs):\n",
    "                state = sess.run(self.network.init_state)\n",
    "                \n",
    "                for ii, (x, y) in enumerate(self.get_batches(\n",
    "                    self.datasets.train_x, self.datasets.train_y, self.batch_size), 1):\n",
    "                    \n",
    "                    feed = {self.network.inputs_: x,\n",
    "                            self.network.labels_: y[:, None],\n",
    "                            self.network.keep_prob: 0.5,\n",
    "                            self.network.init_state: state}\n",
    "                    \n",
    "                    loss, state, _ = sess.run([self.network.cost, \n",
    "                                               self.network.final_state, \n",
    "                                               self.network.optimizer],\n",
    "                                               feed_dict=feed)\n",
    "                    \n",
    "                    if iteration%5==0:\n",
    "                        print(\"Epoch: {}/{}\".format(e+1, self.epochs),\n",
    "                              \"Iteration: {}\".format(iteration),\n",
    "                              \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "                    if iteration%25==0:\n",
    "                        val_acc = []\n",
    "                        val_state = sess.run(self.network.cell.zero_state(self.batch_size, \n",
    "                                                                          tf.float32))\n",
    "                        \n",
    "                        for x, y in self.get_batches(self.datasets.val_x, \n",
    "                                                     self.datasets.val_y, \n",
    "                                                     self.batch_size):\n",
    "                            feed = {self.network.inputs_: x,\n",
    "                                    self.network.labels_: y[:, None],\n",
    "                                    self.network.keep_prob: 1,\n",
    "                                    self.network.init_state: val_state}\n",
    "                            \n",
    "                            batch_acc, val_state = sess.run([self.network.accuracy, \n",
    "                                                             self.network.final_state], \n",
    "                                                            feed_dict=feed)\n",
    "                            val_acc.append(batch_acc)\n",
    "                            \n",
    "                        print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "\n",
    "                    iteration +=1\n",
    "            \n",
    "            saver.save(sess, \"checkpoints/sentiment.ckpt\")\n",
    "            \n",
    "        return saver\n",
    "            \n",
    "    def get_batches(self, x, y, batch_size=100):\n",
    "        n_batches = len(x)//batch_size\n",
    "        x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "\n",
    "        for ii in range(0, len(x), batch_size):\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetworkTester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NetworkTester:\n",
    "    \n",
    "    def __init__(self, network, datasets, batch_size=500):\n",
    "        self.network = network\n",
    "        self.datasets = datasets\n",
    "        self.batch_size = batch_size\n",
    "        self.test_acc = []\n",
    "        \n",
    "    def test_network(self, saver):\n",
    "        with tf.Session(graph=self.network.graph) as sess:\n",
    "            saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "            test_state = sess.run(self.network.cell.zero_state(self.batch_size, tf.float32))\n",
    "            \n",
    "            for ii, (x, y) in enumerate(self.get_batches(\n",
    "                self.datasets.test_x, self.datasets.test_y, self.batch_size), 1):\n",
    "                \n",
    "                feed = {self.network.inputs_: x,\n",
    "                        self.network.labels_: y[:, None],\n",
    "                        self.network.keep_prob: 1,\n",
    "                        self.network.init_state: test_state}\n",
    "                \n",
    "                batch_acc, test_state = sess.run([self.network.accuracy, \n",
    "                                                  self.network.final_state], \n",
    "                                                 feed_dict=feed)\n",
    "                self.test_acc.append(batch_acc)\n",
    "                \n",
    "            print(\"Test accuracy: {:.3f}\".format(np.mean(self.test_acc)))\n",
    "\n",
    "    def get_batches(self, x, y, batch_size=100):\n",
    "        n_batches = len(x)//batch_size\n",
    "        x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "\n",
    "        for ii in range(0, len(x), batch_size):\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentimentRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentimentRNN:\n",
    "    \n",
    "    def load_and_prepare_data(self):\n",
    "        print('DATA PREPROCESSING STEP\\n')\n",
    "        dataPreprocessor = DataPreprocessor()\n",
    "        self.data = dataPreprocessor.load_and_preprocess_data()\n",
    "        print('DATA PROCESSING COMPLETE\\n')\n",
    "        \n",
    "    def create_train_val_test_sets(self):\n",
    "        print('DATA SET CREATION STEP\\n')\n",
    "        dataSetCreator = DataSetCreator()\n",
    "        self.datasets = dataSetCreator.create_training_validation_and_testing_sets(\n",
    "            self.data.features, self.data.targets)\n",
    "        print('DATA SET CREATION COMPLETE\\n')\n",
    "    \n",
    "    def build_network(self):\n",
    "        print('NETWORK BUILDING STEP\\n')\n",
    "        self.network = RNNetwork()\n",
    "        self.network.create_placeholders()\n",
    "        self.network.add_embedding_layer(len(self.data.words_to_ints)+1)\n",
    "        self.network.add_lstm_layers()\n",
    "        self.network.add_forward_pass()\n",
    "        self.network.add_train_loss_computation()\n",
    "        self.network.add_validation_accuracy_computation()\n",
    "        print('NETWORK BUILDING COMPLETE\\n')\n",
    "        \n",
    "    def train_network(self):\n",
    "        print('TRAINING STEP\\n')\n",
    "        networkTrainer = NetworkTrainer(self.network, self.datasets)\n",
    "        self.saver = networkTrainer.train_network()\n",
    "        print('\\nTRAINING STEP COMPLETE\\n')\n",
    "        \n",
    "    def test_network(self):\n",
    "        print('TESTING STEP\\n')\n",
    "        networkTester = NetworkTester(self.network, self.datasets)\n",
    "        networkTester.test_network(self.saver)\n",
    "        print('\\nTESTING COMPLETE\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = SentimentRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPROCESSING STEP\n",
      "\n",
      "Loaded data\n",
      "\n",
      "reviews[:100]:\n",
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life\n",
      "\n",
      "labels[:100]:\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "n\n",
      "\n",
      "\n",
      "Removed punctuation from review text\n",
      "\n",
      "reviews[:1]:\n",
      "['bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   ']\n",
      "\n",
      "\n",
      "Extracted words\n",
      "\n",
      "words[:100]:\n",
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'high', 's', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i', 'immediately', 'recalled', 'at', 'high']\n",
      "\n",
      "\n",
      "Mapped words to ints\n",
      "\n",
      "bromwell: 21025\n",
      "high: 308\n",
      "is: 6\n",
      "a: 3\n",
      "cartoon: 1050\n",
      "comedy: 207\n",
      "it: 8\n",
      "ran: 2138\n",
      "at: 32\n",
      "the: 1\n",
      "\n",
      "\n",
      "Mapped reviews to ints\n",
      "\n",
      "reviews_to_ints[0]:\n",
      "[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]\n",
      "\n",
      "\n",
      "Mapped labels to ints\n",
      "\n",
      "labels_to_ints[:10]:\n",
      "[1 0 1 0 1 0 1 0 1 0]\n",
      "\n",
      "\n",
      "Filtered out zero-len entries\n",
      "\n",
      "\n",
      "Formatted features\n",
      "\n",
      "features[:5,:100]:\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  21025   308     6     3  1050   207     8  2138    32     1   171    57\n",
      "     15    49    81  5785    44   382   110   140    15  5194    60   154\n",
      "      9     1  4975  5852   475    71     5   260    12 21025   308    13\n",
      "   1978     6    74  2395]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0    63     4     3   125    36    47  7472  1395    16     3\n",
      "   4181   505    45    17]\n",
      " [22382    42 46418    15   706 17139  3389    47    77    35  1819    16\n",
      "    154    19   114     3  1305     5   336   147    22     1   857    12\n",
      "     70   281  1168   399    36   120   283    38   169     5   382   158\n",
      "     42  2269    16     1   541    90    78   102     4     1  3244    15\n",
      "     43     3   407  1068   136  8055    44   182   140    15  3043     1\n",
      "    320    22  4818 26224   346     5  3090  2092     1 18839 17939    42\n",
      "   8055    46    33   236    29   370     5   130    56    22     1  1928\n",
      "      7     7    19    48    46    21    70   344     3  2099     5   408\n",
      "     22     1  1928    16]\n",
      " [ 4505   505    15     3  3342   162  8312  1652     6  4819    56    17\n",
      "   4504  5616   140 11725     5   996  4919  2933  4462   566  1201    36\n",
      "      6  1518    96     3   744     4 26225    13     5    27  3461     9\n",
      "  10625     4     8   111  3013     5     1  1027    15     3  4390    82\n",
      "     22  2049     6  4462   538  2764  7073 37443    41   463     1  8312\n",
      "  46419   302   123    15  4221    19  1667   922     1  1652     6  6129\n",
      "  19871    34     1   980  1751 22383   646 24104    27   106 11726    13\n",
      "  14045 15097 17940  2457   466 21027    36  3266     1  6365  1020    45\n",
      "     17  2695  2499    33]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0   520   119   113    34 16372  1816  3737   117   885 21030\n",
      "    721    10    28   124   108     2   115   137     9  1623  7691    26\n",
      "    330     5   589     1  6130    22   386     6     3   349    15    50\n",
      "     15   231     9  7473 11399     1   191    22  8966     6    82   880\n",
      "    101   111  3584     4]]\n",
      "\n",
      "\n",
      "DATA PROCESSING COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn.load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SET CREATION STEP\n",
      "\n",
      "Created training, validation, and testing sets\n",
      "\n",
      "Training set features shape: \t(20000, 200)\n",
      "Validation set features shape: \t(2500, 200)\n",
      "Testing set features shape: \t(2500, 200)\n",
      "\n",
      "DATA SET CREATION COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn.create_train_val_test_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK BUILDING STEP\n",
      "\n",
      "Created graph object\n",
      "\n",
      "Created placeholders\n",
      "\n",
      "inputs_: Tensor(\"inputs:0\", shape=(?, ?), dtype=int32)\n",
      "labels_: Tensor(\"labels:0\", shape=(?, ?), dtype=int32)\n",
      "keep_prob_: Tensor(\"keep_prob:0\", dtype=float32)\n",
      "\n",
      "Added embedding layer\n",
      "\n",
      "Added LSTM layers\n",
      "\n",
      "Added forward pass\n",
      "\n",
      "Added training loss computation\n",
      "\n",
      "Added validation accuracy computation\n",
      "\n",
      "NETWORK BUILDING COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn.build_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING STEP\n",
      "\n",
      "Epoch: 1/10 Iteration: 5 Train loss: 0.237\n",
      "Epoch: 1/10 Iteration: 10 Train loss: 0.241\n",
      "Epoch: 1/10 Iteration: 15 Train loss: 0.222\n",
      "Epoch: 1/10 Iteration: 20 Train loss: 0.220\n",
      "Epoch: 1/10 Iteration: 25 Train loss: 0.191\n",
      "Val acc: 0.726\n",
      "Epoch: 1/10 Iteration: 30 Train loss: 0.231\n",
      "Epoch: 1/10 Iteration: 35 Train loss: 0.177\n",
      "Epoch: 1/10 Iteration: 40 Train loss: 0.205\n",
      "Epoch: 2/10 Iteration: 45 Train loss: 0.166\n",
      "Epoch: 2/10 Iteration: 50 Train loss: 0.172\n",
      "Val acc: 0.769\n",
      "Epoch: 2/10 Iteration: 55 Train loss: 0.163\n",
      "Epoch: 2/10 Iteration: 60 Train loss: 0.130\n",
      "Epoch: 2/10 Iteration: 65 Train loss: 0.128\n",
      "Epoch: 2/10 Iteration: 70 Train loss: 0.163\n",
      "Epoch: 2/10 Iteration: 75 Train loss: 0.129\n",
      "Val acc: 0.780\n",
      "Epoch: 2/10 Iteration: 80 Train loss: 0.168\n",
      "Epoch: 3/10 Iteration: 85 Train loss: 0.143\n",
      "Epoch: 3/10 Iteration: 90 Train loss: 0.133\n",
      "Epoch: 3/10 Iteration: 95 Train loss: 0.137\n",
      "Epoch: 3/10 Iteration: 100 Train loss: 0.165\n",
      "Val acc: 0.690\n",
      "Epoch: 3/10 Iteration: 105 Train loss: 0.192\n",
      "Epoch: 3/10 Iteration: 110 Train loss: 0.182\n",
      "Epoch: 3/10 Iteration: 115 Train loss: 0.184\n",
      "Epoch: 3/10 Iteration: 120 Train loss: 0.173\n",
      "Epoch: 4/10 Iteration: 125 Train loss: 0.162\n",
      "Val acc: 0.680\n",
      "Epoch: 4/10 Iteration: 130 Train loss: 0.185\n",
      "Epoch: 4/10 Iteration: 135 Train loss: 0.146\n",
      "Epoch: 4/10 Iteration: 140 Train loss: 0.124\n",
      "Epoch: 4/10 Iteration: 145 Train loss: 0.123\n",
      "Epoch: 4/10 Iteration: 150 Train loss: 0.139\n",
      "Val acc: 0.752\n",
      "Epoch: 4/10 Iteration: 155 Train loss: 0.130\n",
      "Epoch: 4/10 Iteration: 160 Train loss: 0.124\n",
      "Epoch: 5/10 Iteration: 165 Train loss: 0.096\n",
      "Epoch: 5/10 Iteration: 170 Train loss: 0.105\n",
      "Epoch: 5/10 Iteration: 175 Train loss: 0.103\n",
      "Val acc: 0.777\n",
      "Epoch: 5/10 Iteration: 180 Train loss: 0.076\n",
      "Epoch: 5/10 Iteration: 185 Train loss: 0.069\n",
      "Epoch: 5/10 Iteration: 190 Train loss: 0.087\n",
      "Epoch: 5/10 Iteration: 195 Train loss: 0.064\n",
      "Epoch: 5/10 Iteration: 200 Train loss: 0.079\n",
      "Val acc: 0.808\n",
      "Epoch: 6/10 Iteration: 205 Train loss: 0.113\n",
      "Epoch: 6/10 Iteration: 210 Train loss: 0.101\n",
      "Epoch: 6/10 Iteration: 215 Train loss: 0.101\n",
      "Epoch: 6/10 Iteration: 220 Train loss: 0.088\n",
      "Epoch: 6/10 Iteration: 225 Train loss: 0.095\n",
      "Val acc: 0.775\n",
      "Epoch: 6/10 Iteration: 230 Train loss: 0.081\n",
      "Epoch: 6/10 Iteration: 235 Train loss: 0.036\n",
      "Epoch: 6/10 Iteration: 240 Train loss: 0.087\n",
      "Epoch: 7/10 Iteration: 245 Train loss: 0.119\n",
      "Epoch: 7/10 Iteration: 250 Train loss: 0.111\n",
      "Val acc: 0.766\n",
      "Epoch: 7/10 Iteration: 255 Train loss: 0.088\n",
      "Epoch: 7/10 Iteration: 260 Train loss: 0.078\n",
      "Epoch: 7/10 Iteration: 265 Train loss: 0.058\n",
      "Epoch: 7/10 Iteration: 270 Train loss: 0.082\n",
      "Epoch: 7/10 Iteration: 275 Train loss: 0.091\n",
      "Val acc: 0.798\n",
      "Epoch: 7/10 Iteration: 280 Train loss: 0.098\n",
      "Epoch: 8/10 Iteration: 285 Train loss: 0.076\n",
      "Epoch: 8/10 Iteration: 290 Train loss: 0.054\n",
      "Epoch: 8/10 Iteration: 295 Train loss: 0.060\n",
      "Epoch: 8/10 Iteration: 300 Train loss: 0.070\n",
      "Val acc: 0.827\n",
      "Epoch: 8/10 Iteration: 305 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 310 Train loss: 0.061\n",
      "Epoch: 8/10 Iteration: 315 Train loss: 0.057\n",
      "Epoch: 8/10 Iteration: 320 Train loss: 0.064\n",
      "Epoch: 9/10 Iteration: 325 Train loss: 0.046\n",
      "Val acc: 0.828\n",
      "Epoch: 9/10 Iteration: 330 Train loss: 0.045\n",
      "Epoch: 9/10 Iteration: 335 Train loss: 0.051\n",
      "Epoch: 9/10 Iteration: 340 Train loss: 0.051\n",
      "Epoch: 9/10 Iteration: 345 Train loss: 0.031\n",
      "Epoch: 9/10 Iteration: 350 Train loss: 0.043\n",
      "Val acc: 0.850\n",
      "Epoch: 9/10 Iteration: 355 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 360 Train loss: 0.041\n",
      "Epoch: 10/10 Iteration: 365 Train loss: 0.042\n",
      "Epoch: 10/10 Iteration: 370 Train loss: 0.030\n",
      "Epoch: 10/10 Iteration: 375 Train loss: 0.036\n",
      "Val acc: 0.805\n",
      "Epoch: 10/10 Iteration: 380 Train loss: 0.047\n",
      "Epoch: 10/10 Iteration: 385 Train loss: 0.051\n",
      "Epoch: 10/10 Iteration: 390 Train loss: 0.038\n",
      "Epoch: 10/10 Iteration: 395 Train loss: 0.035\n",
      "Epoch: 10/10 Iteration: 400 Train loss: 0.041\n",
      "Val acc: 0.829\n",
      "\n",
      "TRAINING STEP COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn.train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING STEP\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/sentiment.ckpt\n",
      "Test accuracy: 0.839\n",
      "\n",
      "TESTING COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn.test_network()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
