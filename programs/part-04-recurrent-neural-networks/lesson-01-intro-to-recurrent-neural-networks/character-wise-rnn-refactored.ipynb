{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-wise Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \n",
    "    def __init__(self):\n",
    "        text = None\n",
    "        chars = None\n",
    "        chars_to_ints = None\n",
    "        ints_to_chars = None\n",
    "        encoded = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \n",
    "    def load_and_preprocess_data(self, input_file):\n",
    "        \"\"\"\n",
    "        Load and preprocess data.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        : input_file: Input file name\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\nLoading and preprocesing data ...\\n\")\n",
    "        \n",
    "        data = Data()\n",
    "        \n",
    "        with open(input_file, 'r') as f:\n",
    "            data.text = f.read()\n",
    "                \n",
    "        data.chars = sorted(set(data.text))\n",
    "        data.chars_to_ints = {c: i for i, c in enumerate(data.chars)}\n",
    "        data.ints_to_chars = dict(enumerate(data.chars))\n",
    "        data.encoded = np.array([data.chars_to_ints[c] for c in data.text], dtype=np.int32)\n",
    "        self.log_data(data)\n",
    "        \n",
    "        print(\"Loaded and preprocessed data\\n\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def log_data(self, data):\n",
    "        txt = \"\"\n",
    "        for ii in range(0, 100):\n",
    "            ch = data.text[ii]\n",
    "            ch = '(NEWLINE)' if ch == '\\n' else ch\n",
    "            txt += ch\n",
    "        print(\"text[:100]:\\n{}\\n\".format(txt))    \n",
    "        \n",
    "        print(\"len(chars):\\n{}\\n\".format(len(data.chars)))\n",
    "        print(\"chars[:50]:\\n{}\\n\".format(data.chars[:50]))\n",
    "        print(\"chars_to_ints:\\n\")\n",
    "        for ii in range(0, 10):\n",
    "            ch = data.chars[ii]\n",
    "            ch = 'NEWLINE' if ch == '\\n' else 'SPACE' if ch == ' ' else ch\n",
    "            print(\"chars_to_ints[{}]: {}\".format(ch, ii))\n",
    "        print(\"\")\n",
    "        print(\"ints_to_chars:\\n\")\n",
    "        for ii in range(0, 10):\n",
    "            ch = data.ints_to_chars[ii]\n",
    "            ch = 'NEWLINE' if ch == '\\n' else 'SPACE' if ch == ' ' else ch\n",
    "            print(\"ints_to_chars[{}]: {}\".format(ii, ch))\n",
    "        print(\"\")\n",
    "        print(\"encoded.shape:\\n{}\\n\".format(data.encoded.shape))\n",
    "        print(\"encoded[:100]:\\n{}\\n\".format(data.encoded[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'anna.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and preprocesing data ...\n",
      "\n",
      "text[:100]:\n",
      "Chapter 1(NEWLINE)(NEWLINE)(NEWLINE)Happy families are all alike; every unhappy family is unhappy in its own(NEWLINE)way.(NEWLINE)(NEWLINE)Everythin\n",
      "\n",
      "len(chars):\n",
      "83\n",
      "\n",
      "chars[:50]:\n",
      "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U']\n",
      "\n",
      "chars_to_ints:\n",
      "\n",
      "chars_to_ints[NEWLINE]: 0\n",
      "chars_to_ints[SPACE]: 1\n",
      "chars_to_ints[!]: 2\n",
      "chars_to_ints[\"]: 3\n",
      "chars_to_ints[$]: 4\n",
      "chars_to_ints[%]: 5\n",
      "chars_to_ints[&]: 6\n",
      "chars_to_ints[']: 7\n",
      "chars_to_ints[(]: 8\n",
      "chars_to_ints[)]: 9\n",
      "\n",
      "ints_to_chars:\n",
      "\n",
      "ints_to_chars[0]: NEWLINE\n",
      "ints_to_chars[1]: SPACE\n",
      "ints_to_chars[2]: !\n",
      "ints_to_chars[3]: \"\n",
      "ints_to_chars[4]: $\n",
      "ints_to_chars[5]: %\n",
      "ints_to_chars[6]: &\n",
      "ints_to_chars[7]: '\n",
      "ints_to_chars[8]: (\n",
      "ints_to_chars[9]: )\n",
      "\n",
      "encoded.shape:\n",
      "(1985223,)\n",
      "\n",
      "encoded[:100]:\n",
      "[31 64 57 72 76 61 74  1 16  0  0  0 36 57 72 72 81  1 62 57 69 65 68 65 61\n",
      " 75  1 57 74 61  1 57 68 68  1 57 68 65 67 61 26  1 61 78 61 74 81  1 77 70\n",
      " 64 57 72 72 81  1 62 57 69 65 68 81  1 65 75  1 77 70 64 57 72 72 81  1 65\n",
      " 70  1 65 76 75  1 71 79 70  0 79 57 81 13  0  0 33 78 61 74 81 76 64 65 70]\n",
      "\n",
      "Loaded and preprocessed data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataPreprocessor = DataPreprocessor()\n",
    "data = dataPreprocessor.load_and_preprocess_data(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Character-wise RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNetwork:\n",
    "    \n",
    "    def create_placeholders(self, batch_size, num_steps):\n",
    "        \"\"\" \n",
    "        Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        : batch_size: Batch size, number of sequences per batch\n",
    "        : num_steps: Number of sequence steps in a batch\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        \n",
    "        print(\"Created placeholders\\n\")\n",
    "        \n",
    "        return inputs, targets, keep_prob\n",
    "    \n",
    "    \n",
    "    def build_lstm_layers(self, keep_prob, lstm_size, num_layers, batch_size):\n",
    "        \"\"\"\n",
    "        Build LSTM layers.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        : keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        : lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        : num_layers: Number of LSTM layers\n",
    "        : batch_size: Batch size\n",
    "        \"\"\"\n",
    "        \n",
    "        cell = tf.contrib.rnn.MultiRNNCell(\n",
    "            [self.build_lstm_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        print(\"Built LSTM layers\\n\")\n",
    "        \n",
    "        return cell, initial_state\n",
    "    \n",
    "        \n",
    "    def build_lstm_cell(self, lstm_size, keep_prob):\n",
    "        \"\"\"\n",
    "        Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        : lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        : keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        \"\"\"\n",
    "        \n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        \n",
    "        print(\"Built LSTM cell\")\n",
    "        \n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    def build_output_layer(self, lstm_output, in_size, out_size):\n",
    "        \"\"\"\n",
    "        Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        : lstm_output: List of output tensors from the LSTM layer\n",
    "        : in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        : out_size: Size of this softmax layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "        # That is, the shape should be batch_size*num_steps rows by lstm_size columns\n",
    "        \n",
    "        # Concatenate lstm_output over axis 1 (the columns)\n",
    "        seq_output = tf.concat(lstm_output, axis=1)\n",
    "        \n",
    "        # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "        x = tf.reshape(seq_output, [-1, in_size])\n",
    "        \n",
    "        # Connect the RNN outputs to a softmax layer\n",
    "        with tf.variable_scope('softmax'):\n",
    "            softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "            softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "            \n",
    "        # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "        # of rows of logit outputs, one for each step and sequence    \n",
    "        logits = tf.nn.bias_add(tf.matmul(x, softmax_w), softmax_b)\n",
    "        \n",
    "        # Use softmax to get the probabilities for predicted characters\n",
    "        out = tf.nn.softmax(logits, name='predictions')\n",
    "        \n",
    "        print(\"Built output layer\\n\")\n",
    "        \n",
    "        return out, logits\n",
    "    \n",
    "\n",
    "    def add_training_loss_computation(self, logits, targets, lstm_size, num_classes):\n",
    "        \"\"\"\n",
    "        Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        : logits: Logits from final fully connected layer\n",
    "        : targets: Targets for supervised learning\n",
    "        : lstm_size: Number of LSTM hidden units\n",
    "        : num_classes: Number of classes in targets\n",
    "        \"\"\"\n",
    "        \n",
    "        # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "        y_one_hot = tf.one_hot(targets, num_classes)\n",
    "        y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "        \n",
    "        # Softmax cross entropy loss\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "        print(\"Added training loss computation\\n\")\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def build_optimizer(self, loss, learning_rate, grad_clip):\n",
    "        \"\"\"\n",
    "        Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        ---------\n",
    "        : loss: Network loss\n",
    "        : learning_rate: Learning rate for optimizer\n",
    "        : grad_clip: For gradient clipping \n",
    "        \"\"\"\n",
    "        \n",
    "        # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "        print(\"Built optimizer\\n\")\n",
    "        \n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharRNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNModel:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_classes, \n",
    "                 batch_size, \n",
    "                 num_steps,\n",
    "                 lstm_size, \n",
    "                 num_layers, \n",
    "                 learning_rate,\n",
    "                 grad_clip, \n",
    "                 sampling=False):\n",
    "\n",
    "        \"\"\"\n",
    "        Build CharRNN model.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        : num_classes: Number of classes in targets\n",
    "        : batch_size: Batch size, number of sequences per batch\n",
    "        : num_steps: Number of sequence steps in a batch\n",
    "        : lstm_size: Number of LSTM hidden units\n",
    "        : num_layers: Number of LSTM layers\n",
    "        : learning_rate: Learning rate\n",
    "        : grad_clip: For gradient clipping\n",
    "        : sampling: Whether or not the model is used for sampling\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\nBuilding CharRNN model ...\\n\")\n",
    "        \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        self.lstm_size = lstm_size\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Create RNNetwork object\n",
    "        network = RNNetwork()\n",
    "        \n",
    "        # Build the placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = network.create_placeholders(self.batch_size, \n",
    "                                                                                self.num_steps)\n",
    "        \n",
    "        # Build the LSTM layers\n",
    "        cell, self.initial_state = network.build_lstm_layers(self.keep_prob, \n",
    "                                                             self.lstm_size, \n",
    "                                                             num_layers,\n",
    "                                                             batch_size)\n",
    "        \n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = network.build_output_layer(outputs, \n",
    "                                                                  self.lstm_size,\n",
    "                                                                  num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = network.add_training_loss_computation(self.logits, \n",
    "                                                          self.targets, \n",
    "                                                          self.lstm_size, \n",
    "                                                          num_classes)\n",
    "        \n",
    "        self.optimizer = network.build_optimizer(self.loss, learning_rate, grad_clip)\n",
    "        \n",
    "        print(\"Built CharRNN model\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(data.chars)\n",
    "batch_size = 64\n",
    "num_steps = 50\n",
    "lstm_size = 128\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "grad_clip = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building CharRNN model ...\n",
      "\n",
      "Created placeholders\n",
      "\n",
      "Built LSTM cell\n",
      "Built LSTM cell\n",
      "Built LSTM layers\n",
      "\n",
      "Built output layer\n",
      "\n",
      "Added training loss computation\n",
      "\n",
      "Built optimizer\n",
      "\n",
      "Built CharRNN model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CharRNNModel(num_classes,\n",
    "                     batch_size,\n",
    "                     num_steps,\n",
    "                     lstm_size,\n",
    "                     num_layers,\n",
    "                     learning_rate,\n",
    "                     grad_clip,\n",
    "                     False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Character-wise RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataBatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBatchGenerator:\n",
    "    \n",
    "    def get_batches(self, arr, n_seqs, n_steps):\n",
    "        \"\"\"\n",
    "        Create a generator that returns batches of size n_seqs x n_steps from arr.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        : arr: Array you want to make batches from\n",
    "        : n_seqs: Number of sequences per batch\n",
    "        : n_steps: Number of sequence steps per batch\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the number of characters per batch and number of batches we can make\n",
    "        chars_per_batch = n_seqs * n_steps # batch size\n",
    "        n_batches = len(arr)//chars_per_batch\n",
    "        \n",
    "        # Keep only enough characters to make full batches\n",
    "        arr = arr[:n_batches * chars_per_batch]\n",
    "        \n",
    "        # Reshape into n_seqs rows\n",
    "        arr = arr.reshape((n_seqs, -1))\n",
    "        \n",
    "        # Generate each batch\n",
    "        for n in range(0, arr.shape[1], n_steps):\n",
    "            # features\n",
    "            x = arr[:, n:n+n_steps]\n",
    "            # targets\n",
    "            y = np.zeros_like(x)\n",
    "            \n",
    "            # Targets are inputs shifted by one character\n",
    "            # First input character is last target character\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "            \n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelTrainer:\n",
    "        \n",
    "    def train_model(self, \n",
    "                    model, \n",
    "                    data, \n",
    "                    epochs, \n",
    "                    keep_prob, \n",
    "                    save_every_n,\n",
    "                    max_to_keep):\n",
    "        \"\"\"\n",
    "        Train RNN model.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        : model: Model to train\n",
    "        : data: Data to train model on\n",
    "        : epochs: Number of epochs to train\n",
    "        : keep_prob: Keep proability to pass to model\n",
    "        : save_every_n: Interval to save session\n",
    "        : max_to_keep: Param to pass to session saver\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\nTraining CharRNN model ...\\n\")\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            saver = tf.train.Saver(max_to_keep=max_to_keep)\n",
    "            \n",
    "            # Load a checkpoint and resume training\n",
    "            #saver.restore(sess, 'checkpoints/_____.ckpt')\n",
    "            \n",
    "            counter = 0\n",
    "            \n",
    "            for e in range(epochs):\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                loss = 0\n",
    "                \n",
    "                dataBatchGenerator = DataBatchGenerator()\n",
    "                batches = dataBatchGenerator.get_batches(data.encoded, model.batch_size, model.num_steps)\n",
    "\n",
    "                for x, y in batches:\n",
    "                    counter += 1\n",
    "                    start = time.time()\n",
    "                    \n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: keep_prob,\n",
    "                            model.initial_state: new_state}\n",
    "                    \n",
    "                    batch_loss, new_state, _ = sess.run([model.loss,\n",
    "                                                         model.final_state,\n",
    "                                                         model.optimizer],\n",
    "                                                         feed_dict=feed)\n",
    "                    \n",
    "                    end = time.time()\n",
    "                    \n",
    "                    print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                          'Training Step: {}... '.format(counter),\n",
    "                          'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                          '{:.4f} sec/batch'.format((end-start)))\n",
    "                    \n",
    "                    if (counter % save_every_n == 0):\n",
    "                        saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, \n",
    "                                                                           model.lstm_size))\n",
    "        \n",
    "            saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, model.lstm_size))\n",
    "            \n",
    "        print(\"\\nTraining complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "keep_prob = 0.5\n",
    "save_every_n = 200\n",
    "max_to_keep = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir \"./checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CharRNN model ...\n",
      "\n",
      "Epoch: 1/20...  Training Step: 1...  Training loss: 4.4212...  0.2310 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 4.4082...  0.0523 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 4.3935...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 4.3727...  0.0578 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 4.3413...  0.0524 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 4.2838...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 4.1652...  0.0524 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 3.9284...  0.0524 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 3.7249...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 3.6379...  0.0577 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 3.6006...  0.0524 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 3.5656...  0.0591 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 3.5008...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 3.4517...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 3.4196...  0.0540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 3.4211...  0.0558 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 3.4172...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 3.3668...  0.0548 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 3.3630...  0.0519 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 3.3592...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 3.3359...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 3.3581...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 3.3326...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 3.3422...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 3.3115...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 3.3127...  0.0557 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 3.2887...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 3.3029...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 3.2601...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 3.2755...  0.0524 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 3.2566...  0.0542 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 3.2780...  0.0524 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 3.2626...  0.0602 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 3.2425...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 3.2362...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 3.2645...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 3.2170...  0.0517 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 3.2533...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 3.2664...  0.0523 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 3.2520...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 3.2223...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 3.2189...  0.0573 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 3.2049...  0.0573 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 3.2125...  0.0580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 3.2334...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 3.2094...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 3.1995...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 3.2389...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 3.1987...  0.0584 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1728...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 3.1737...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 3.1982...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 3.2090...  0.0568 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 3.2337...  0.0544 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 3.2375...  0.0523 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 3.1892...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 3.2095...  0.0572 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 3.1557...  0.0581 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 3.1675...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 3.1864...  0.0566 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 3.1705...  0.0569 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 3.1851...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 3.1987...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 3.1757...  0.0518 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 3.1492...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 3.1449...  0.0543 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 3.1485...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 3.1602...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 3.1867...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 3.1299...  0.0561 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 3.1518...  0.0586 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 3.1787...  0.0577 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 3.1900...  0.0596 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 3.1476...  0.0574 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 3.1716...  0.0558 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 3.1717...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 3.2210...  0.0566 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 3.1582...  0.0523 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 3.2160...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 3.1461...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 3.1908...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 3.1362...  0.0583 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 3.1545...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 3.1842...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 3.1563...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 3.1427...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 3.1839...  0.0572 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 3.2215...  0.0568 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 3.1723...  0.0522 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 3.1912...  0.0572 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 3.1654...  0.0567 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 3.1577...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 3.1812...  0.0537 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 3.1529...  0.0522 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 3.1843...  0.0582 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 3.1517...  0.0582 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 3.1908...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 3.1603...  0.0582 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 3.1657...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.1475...  0.0521 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 3.1457...  0.0583 sec/batch\n",
      "Epoch: 1/20...  Training Step: 102...  Training loss: 3.1534...  0.0580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 103...  Training loss: 3.1360...  0.0548 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 3.1376...  0.0551 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 105...  Training loss: 3.1503...  0.0544 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 3.1465...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 3.1825...  0.0581 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 3.1358...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 3.1054...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 3.1176...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 111...  Training loss: 3.1491...  0.0582 sec/batch\n",
      "Epoch: 1/20...  Training Step: 112...  Training loss: 3.1678...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 113...  Training loss: 3.1867...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 114...  Training loss: 3.1324...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 115...  Training loss: 3.1420...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 116...  Training loss: 3.1285...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 117...  Training loss: 3.1518...  0.0535 sec/batch\n",
      "Epoch: 1/20...  Training Step: 118...  Training loss: 3.1748...  0.0521 sec/batch\n",
      "Epoch: 1/20...  Training Step: 119...  Training loss: 3.1240...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 120...  Training loss: 3.1271...  0.0585 sec/batch\n",
      "Epoch: 1/20...  Training Step: 121...  Training loss: 3.1397...  0.0603 sec/batch\n",
      "Epoch: 1/20...  Training Step: 122...  Training loss: 3.1061...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 123...  Training loss: 3.1286...  0.0576 sec/batch\n",
      "Epoch: 1/20...  Training Step: 124...  Training loss: 3.1411...  0.0580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 125...  Training loss: 3.1616...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 126...  Training loss: 3.1057...  0.0545 sec/batch\n",
      "Epoch: 1/20...  Training Step: 127...  Training loss: 3.1571...  0.0548 sec/batch\n",
      "Epoch: 1/20...  Training Step: 128...  Training loss: 3.1361...  0.0521 sec/batch\n",
      "Epoch: 1/20...  Training Step: 129...  Training loss: 3.1172...  0.0578 sec/batch\n",
      "Epoch: 1/20...  Training Step: 130...  Training loss: 3.1227...  0.0573 sec/batch\n",
      "Epoch: 1/20...  Training Step: 131...  Training loss: 3.0938...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 132...  Training loss: 3.1504...  0.0557 sec/batch\n",
      "Epoch: 1/20...  Training Step: 133...  Training loss: 3.1596...  0.0562 sec/batch\n",
      "Epoch: 1/20...  Training Step: 134...  Training loss: 3.1246...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 135...  Training loss: 3.1197...  0.0539 sec/batch\n",
      "Epoch: 1/20...  Training Step: 136...  Training loss: 3.1010...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 137...  Training loss: 3.1494...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 138...  Training loss: 3.1360...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 139...  Training loss: 3.1441...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 140...  Training loss: 3.1170...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 141...  Training loss: 3.1295...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 142...  Training loss: 3.1236...  0.0610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 143...  Training loss: 3.1323...  0.0521 sec/batch\n",
      "Epoch: 1/20...  Training Step: 144...  Training loss: 3.1334...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 145...  Training loss: 3.1169...  0.0594 sec/batch\n",
      "Epoch: 1/20...  Training Step: 146...  Training loss: 3.1400...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 147...  Training loss: 3.0823...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 148...  Training loss: 3.1614...  0.0545 sec/batch\n",
      "Epoch: 1/20...  Training Step: 149...  Training loss: 3.1164...  0.0541 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 3.1166...  0.0523 sec/batch\n",
      "Epoch: 1/20...  Training Step: 151...  Training loss: 3.1495...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 152...  Training loss: 3.1324...  0.0523 sec/batch\n",
      "Epoch: 1/20...  Training Step: 153...  Training loss: 3.1050...  0.0577 sec/batch\n",
      "Epoch: 1/20...  Training Step: 154...  Training loss: 3.1611...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 155...  Training loss: 3.1304...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 156...  Training loss: 3.1199...  0.0548 sec/batch\n",
      "Epoch: 1/20...  Training Step: 157...  Training loss: 3.0671...  0.0625 sec/batch\n",
      "Epoch: 1/20...  Training Step: 158...  Training loss: 3.1164...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 159...  Training loss: 3.1328...  0.0545 sec/batch\n",
      "Epoch: 1/20...  Training Step: 160...  Training loss: 3.1163...  0.0569 sec/batch\n",
      "Epoch: 1/20...  Training Step: 161...  Training loss: 3.1103...  0.0571 sec/batch\n",
      "Epoch: 1/20...  Training Step: 162...  Training loss: 3.1061...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 163...  Training loss: 3.0941...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 164...  Training loss: 3.1123...  0.0557 sec/batch\n",
      "Epoch: 1/20...  Training Step: 165...  Training loss: 3.1195...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 166...  Training loss: 3.0721...  0.0523 sec/batch\n",
      "Epoch: 1/20...  Training Step: 167...  Training loss: 3.0928...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 168...  Training loss: 3.0719...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 169...  Training loss: 3.0969...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 170...  Training loss: 3.0950...  0.0524 sec/batch\n",
      "Epoch: 1/20...  Training Step: 171...  Training loss: 3.0719...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 172...  Training loss: 3.0625...  0.0567 sec/batch\n",
      "Epoch: 1/20...  Training Step: 173...  Training loss: 3.0868...  0.0559 sec/batch\n",
      "Epoch: 1/20...  Training Step: 174...  Training loss: 3.0957...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 175...  Training loss: 3.0525...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 176...  Training loss: 3.1128...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 177...  Training loss: 3.0895...  0.0564 sec/batch\n",
      "Epoch: 1/20...  Training Step: 178...  Training loss: 3.0828...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 179...  Training loss: 3.1055...  0.0534 sec/batch\n",
      "Epoch: 1/20...  Training Step: 180...  Training loss: 3.0548...  0.0559 sec/batch\n",
      "Epoch: 1/20...  Training Step: 181...  Training loss: 3.0825...  0.0523 sec/batch\n",
      "Epoch: 1/20...  Training Step: 182...  Training loss: 3.0965...  0.0548 sec/batch\n",
      "Epoch: 1/20...  Training Step: 183...  Training loss: 3.0955...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 184...  Training loss: 3.0157...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 185...  Training loss: 3.0731...  0.0586 sec/batch\n",
      "Epoch: 1/20...  Training Step: 186...  Training loss: 3.0851...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 187...  Training loss: 2.9994...  0.0559 sec/batch\n",
      "Epoch: 1/20...  Training Step: 188...  Training loss: 3.0382...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 189...  Training loss: 3.0539...  0.0560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 190...  Training loss: 3.0674...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 191...  Training loss: 3.0813...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 192...  Training loss: 3.0647...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 193...  Training loss: 3.0586...  0.0533 sec/batch\n",
      "Epoch: 1/20...  Training Step: 194...  Training loss: 3.0482...  0.0584 sec/batch\n",
      "Epoch: 1/20...  Training Step: 195...  Training loss: 3.0278...  0.0613 sec/batch\n",
      "Epoch: 1/20...  Training Step: 196...  Training loss: 3.0631...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 197...  Training loss: 3.0411...  0.0540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 198...  Training loss: 2.9948...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 199...  Training loss: 3.0283...  0.0584 sec/batch\n",
      "Epoch: 1/20...  Training Step: 200...  Training loss: 3.0089...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 201...  Training loss: 2.9868...  0.0563 sec/batch\n",
      "Epoch: 1/20...  Training Step: 202...  Training loss: 2.9883...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 203...  Training loss: 3.0046...  0.0533 sec/batch\n",
      "Epoch: 1/20...  Training Step: 204...  Training loss: 3.0023...  0.0584 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 205...  Training loss: 3.0024...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 206...  Training loss: 3.0014...  0.0533 sec/batch\n",
      "Epoch: 1/20...  Training Step: 207...  Training loss: 2.9861...  0.0524 sec/batch\n",
      "Epoch: 1/20...  Training Step: 208...  Training loss: 3.0394...  0.0604 sec/batch\n",
      "Epoch: 1/20...  Training Step: 209...  Training loss: 3.0042...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 210...  Training loss: 2.9627...  0.0522 sec/batch\n",
      "Epoch: 1/20...  Training Step: 211...  Training loss: 2.9870...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 212...  Training loss: 3.0226...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 213...  Training loss: 2.9869...  0.0561 sec/batch\n",
      "Epoch: 1/20...  Training Step: 214...  Training loss: 3.0003...  0.0557 sec/batch\n",
      "Epoch: 1/20...  Training Step: 215...  Training loss: 2.9986...  0.0561 sec/batch\n",
      "Epoch: 1/20...  Training Step: 216...  Training loss: 2.9795...  0.0594 sec/batch\n",
      "Epoch: 1/20...  Training Step: 217...  Training loss: 2.9478...  0.0542 sec/batch\n",
      "Epoch: 1/20...  Training Step: 218...  Training loss: 2.9706...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 219...  Training loss: 2.9926...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 220...  Training loss: 2.9850...  0.0542 sec/batch\n",
      "Epoch: 1/20...  Training Step: 221...  Training loss: 2.9564...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 222...  Training loss: 2.9373...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 223...  Training loss: 2.9736...  0.0572 sec/batch\n",
      "Epoch: 1/20...  Training Step: 224...  Training loss: 2.9391...  0.0548 sec/batch\n",
      "Epoch: 1/20...  Training Step: 225...  Training loss: 2.9500...  0.0558 sec/batch\n",
      "Epoch: 1/20...  Training Step: 226...  Training loss: 2.9890...  0.0585 sec/batch\n",
      "Epoch: 1/20...  Training Step: 227...  Training loss: 2.9516...  0.0587 sec/batch\n",
      "Epoch: 1/20...  Training Step: 228...  Training loss: 2.9218...  0.0522 sec/batch\n",
      "Epoch: 1/20...  Training Step: 229...  Training loss: 2.9446...  0.0596 sec/batch\n",
      "Epoch: 1/20...  Training Step: 230...  Training loss: 2.9382...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 231...  Training loss: 2.9256...  0.0584 sec/batch\n",
      "Epoch: 1/20...  Training Step: 232...  Training loss: 2.9754...  0.0588 sec/batch\n",
      "Epoch: 1/20...  Training Step: 233...  Training loss: 2.9170...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 234...  Training loss: 2.9450...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 235...  Training loss: 2.9080...  0.0543 sec/batch\n",
      "Epoch: 1/20...  Training Step: 236...  Training loss: 2.9045...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 237...  Training loss: 2.9054...  0.0575 sec/batch\n",
      "Epoch: 1/20...  Training Step: 238...  Training loss: 2.8519...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 239...  Training loss: 2.8926...  0.0558 sec/batch\n",
      "Epoch: 1/20...  Training Step: 240...  Training loss: 2.9354...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 241...  Training loss: 2.9194...  0.0556 sec/batch\n",
      "Epoch: 1/20...  Training Step: 242...  Training loss: 2.9009...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 243...  Training loss: 2.8706...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 244...  Training loss: 2.9032...  0.0626 sec/batch\n",
      "Epoch: 1/20...  Training Step: 245...  Training loss: 2.8704...  0.0533 sec/batch\n",
      "Epoch: 1/20...  Training Step: 246...  Training loss: 2.9088...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 247...  Training loss: 2.8594...  0.0525 sec/batch\n",
      "Epoch: 1/20...  Training Step: 248...  Training loss: 2.8620...  0.0563 sec/batch\n",
      "Epoch: 1/20...  Training Step: 249...  Training loss: 2.8604...  0.0557 sec/batch\n",
      "Epoch: 1/20...  Training Step: 250...  Training loss: 2.8735...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 251...  Training loss: 2.8410...  0.0522 sec/batch\n",
      "Epoch: 1/20...  Training Step: 252...  Training loss: 2.8468...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 253...  Training loss: 2.8555...  0.0568 sec/batch\n",
      "Epoch: 1/20...  Training Step: 254...  Training loss: 2.8698...  0.0571 sec/batch\n",
      "Epoch: 1/20...  Training Step: 255...  Training loss: 2.8364...  0.0524 sec/batch\n",
      "Epoch: 1/20...  Training Step: 256...  Training loss: 2.8745...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 257...  Training loss: 2.8750...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 258...  Training loss: 2.8736...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 259...  Training loss: 2.8366...  0.0562 sec/batch\n",
      "Epoch: 1/20...  Training Step: 260...  Training loss: 2.8086...  0.0583 sec/batch\n",
      "Epoch: 1/20...  Training Step: 261...  Training loss: 2.8288...  0.0578 sec/batch\n",
      "Epoch: 1/20...  Training Step: 262...  Training loss: 2.8448...  0.0600 sec/batch\n",
      "Epoch: 1/20...  Training Step: 263...  Training loss: 2.8432...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 264...  Training loss: 2.9045...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 265...  Training loss: 2.8633...  0.0560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 266...  Training loss: 2.7896...  0.0588 sec/batch\n",
      "Epoch: 1/20...  Training Step: 267...  Training loss: 2.8214...  0.0525 sec/batch\n",
      "Epoch: 1/20...  Training Step: 268...  Training loss: 2.8304...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 269...  Training loss: 2.8384...  0.0566 sec/batch\n",
      "Epoch: 1/20...  Training Step: 270...  Training loss: 2.8299...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 271...  Training loss: 2.8377...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 272...  Training loss: 2.7703...  0.0573 sec/batch\n",
      "Epoch: 1/20...  Training Step: 273...  Training loss: 2.7994...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 274...  Training loss: 2.7668...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 275...  Training loss: 2.8096...  0.0542 sec/batch\n",
      "Epoch: 1/20...  Training Step: 276...  Training loss: 2.8248...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 277...  Training loss: 2.7902...  0.0562 sec/batch\n",
      "Epoch: 1/20...  Training Step: 278...  Training loss: 2.7974...  0.0545 sec/batch\n",
      "Epoch: 1/20...  Training Step: 279...  Training loss: 2.8173...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 280...  Training loss: 2.8160...  0.0598 sec/batch\n",
      "Epoch: 1/20...  Training Step: 281...  Training loss: 2.7616...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 282...  Training loss: 2.7912...  0.0587 sec/batch\n",
      "Epoch: 1/20...  Training Step: 283...  Training loss: 2.7995...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 284...  Training loss: 2.7565...  0.0521 sec/batch\n",
      "Epoch: 1/20...  Training Step: 285...  Training loss: 2.7292...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 286...  Training loss: 2.7617...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 287...  Training loss: 2.7490...  0.0570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 288...  Training loss: 2.7552...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 289...  Training loss: 2.7749...  0.0560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 290...  Training loss: 2.7916...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 291...  Training loss: 2.7448...  0.0564 sec/batch\n",
      "Epoch: 1/20...  Training Step: 292...  Training loss: 2.7876...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 293...  Training loss: 2.7940...  0.0573 sec/batch\n",
      "Epoch: 1/20...  Training Step: 294...  Training loss: 2.7549...  0.0525 sec/batch\n",
      "Epoch: 1/20...  Training Step: 295...  Training loss: 2.7563...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 296...  Training loss: 2.7620...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 297...  Training loss: 2.7737...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 298...  Training loss: 2.7826...  0.0605 sec/batch\n",
      "Epoch: 1/20...  Training Step: 299...  Training loss: 2.7817...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 300...  Training loss: 2.7489...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 301...  Training loss: 2.7194...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 302...  Training loss: 2.7600...  0.0561 sec/batch\n",
      "Epoch: 1/20...  Training Step: 303...  Training loss: 2.7280...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 304...  Training loss: 2.7325...  0.0525 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 305...  Training loss: 2.7841...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 306...  Training loss: 2.7509...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 307...  Training loss: 2.7608...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 308...  Training loss: 2.7182...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 309...  Training loss: 2.7439...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 310...  Training loss: 2.6692...  0.0594 sec/batch\n",
      "Epoch: 1/20...  Training Step: 311...  Training loss: 2.7383...  0.0548 sec/batch\n",
      "Epoch: 1/20...  Training Step: 312...  Training loss: 2.7406...  0.0537 sec/batch\n",
      "Epoch: 1/20...  Training Step: 313...  Training loss: 2.7505...  0.0564 sec/batch\n",
      "Epoch: 1/20...  Training Step: 314...  Training loss: 2.7238...  0.0543 sec/batch\n",
      "Epoch: 1/20...  Training Step: 315...  Training loss: 2.7519...  0.0574 sec/batch\n",
      "Epoch: 1/20...  Training Step: 316...  Training loss: 2.7156...  0.0556 sec/batch\n",
      "Epoch: 1/20...  Training Step: 317...  Training loss: 2.6569...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 318...  Training loss: 2.6923...  0.0522 sec/batch\n",
      "Epoch: 1/20...  Training Step: 319...  Training loss: 2.7078...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 320...  Training loss: 2.7507...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 321...  Training loss: 2.7014...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 322...  Training loss: 2.6800...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 323...  Training loss: 2.6914...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 324...  Training loss: 2.6553...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 325...  Training loss: 2.7079...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 326...  Training loss: 2.6784...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 327...  Training loss: 2.7079...  0.0540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 328...  Training loss: 2.6610...  0.0536 sec/batch\n",
      "Epoch: 1/20...  Training Step: 329...  Training loss: 2.7235...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 330...  Training loss: 2.6970...  0.0559 sec/batch\n",
      "Epoch: 1/20...  Training Step: 331...  Training loss: 2.6704...  0.0556 sec/batch\n",
      "Epoch: 1/20...  Training Step: 332...  Training loss: 2.6955...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 333...  Training loss: 2.6448...  0.0563 sec/batch\n",
      "Epoch: 1/20...  Training Step: 334...  Training loss: 2.6869...  0.0617 sec/batch\n",
      "Epoch: 1/20...  Training Step: 335...  Training loss: 2.7094...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 336...  Training loss: 2.7134...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 337...  Training loss: 2.6783...  0.0593 sec/batch\n",
      "Epoch: 1/20...  Training Step: 338...  Training loss: 2.6831...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 339...  Training loss: 2.6509...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 340...  Training loss: 2.7038...  0.0583 sec/batch\n",
      "Epoch: 1/20...  Training Step: 341...  Training loss: 2.6978...  0.0542 sec/batch\n",
      "Epoch: 1/20...  Training Step: 342...  Training loss: 2.6730...  0.0523 sec/batch\n",
      "Epoch: 1/20...  Training Step: 343...  Training loss: 2.6553...  0.0543 sec/batch\n",
      "Epoch: 1/20...  Training Step: 344...  Training loss: 2.6671...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 345...  Training loss: 2.6604...  0.0573 sec/batch\n",
      "Epoch: 1/20...  Training Step: 346...  Training loss: 2.7112...  0.0578 sec/batch\n",
      "Epoch: 1/20...  Training Step: 347...  Training loss: 2.6671...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 348...  Training loss: 2.6251...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 349...  Training loss: 2.6529...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 350...  Training loss: 2.6208...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 351...  Training loss: 2.6389...  0.0594 sec/batch\n",
      "Epoch: 1/20...  Training Step: 352...  Training loss: 2.6644...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 353...  Training loss: 2.6842...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 354...  Training loss: 2.6523...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 355...  Training loss: 2.6588...  0.0540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 356...  Training loss: 2.7178...  0.0565 sec/batch\n",
      "Epoch: 1/20...  Training Step: 357...  Training loss: 2.7198...  0.0566 sec/batch\n",
      "Epoch: 1/20...  Training Step: 358...  Training loss: 2.6890...  0.0573 sec/batch\n",
      "Epoch: 1/20...  Training Step: 359...  Training loss: 2.6272...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 360...  Training loss: 2.6597...  0.0521 sec/batch\n",
      "Epoch: 1/20...  Training Step: 361...  Training loss: 2.6842...  0.0577 sec/batch\n",
      "Epoch: 1/20...  Training Step: 362...  Training loss: 2.6581...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 363...  Training loss: 2.6096...  0.0572 sec/batch\n",
      "Epoch: 1/20...  Training Step: 364...  Training loss: 2.6361...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 365...  Training loss: 2.6478...  0.0557 sec/batch\n",
      "Epoch: 1/20...  Training Step: 366...  Training loss: 2.6842...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 367...  Training loss: 2.6495...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 368...  Training loss: 2.6551...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 369...  Training loss: 2.6364...  0.0557 sec/batch\n",
      "Epoch: 1/20...  Training Step: 370...  Training loss: 2.6479...  0.0612 sec/batch\n",
      "Epoch: 1/20...  Training Step: 371...  Training loss: 2.6750...  0.0581 sec/batch\n",
      "Epoch: 1/20...  Training Step: 372...  Training loss: 2.6880...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 373...  Training loss: 2.6789...  0.0537 sec/batch\n",
      "Epoch: 1/20...  Training Step: 374...  Training loss: 2.6501...  0.0587 sec/batch\n",
      "Epoch: 1/20...  Training Step: 375...  Training loss: 2.6148...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 376...  Training loss: 2.6619...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 377...  Training loss: 2.6460...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 378...  Training loss: 2.6485...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 379...  Training loss: 2.6835...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 380...  Training loss: 2.6304...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 381...  Training loss: 2.5974...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 382...  Training loss: 2.6388...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 383...  Training loss: 2.6097...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 384...  Training loss: 2.6409...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 385...  Training loss: 2.6313...  0.0563 sec/batch\n",
      "Epoch: 1/20...  Training Step: 386...  Training loss: 2.5852...  0.0571 sec/batch\n",
      "Epoch: 1/20...  Training Step: 387...  Training loss: 2.5929...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 388...  Training loss: 2.6772...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 389...  Training loss: 2.5460...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 390...  Training loss: 2.5928...  0.0573 sec/batch\n",
      "Epoch: 1/20...  Training Step: 391...  Training loss: 2.6146...  0.0580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 392...  Training loss: 2.5698...  0.0580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 393...  Training loss: 2.6135...  0.0559 sec/batch\n",
      "Epoch: 1/20...  Training Step: 394...  Training loss: 2.5688...  0.0590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 395...  Training loss: 2.5927...  0.0541 sec/batch\n",
      "Epoch: 1/20...  Training Step: 396...  Training loss: 2.6041...  0.0566 sec/batch\n",
      "Epoch: 1/20...  Training Step: 397...  Training loss: 2.6009...  0.0545 sec/batch\n",
      "Epoch: 1/20...  Training Step: 398...  Training loss: 2.6162...  0.0563 sec/batch\n",
      "Epoch: 1/20...  Training Step: 399...  Training loss: 2.5889...  0.0525 sec/batch\n",
      "Epoch: 1/20...  Training Step: 400...  Training loss: 2.5892...  0.0561 sec/batch\n",
      "Epoch: 1/20...  Training Step: 401...  Training loss: 2.5948...  0.0543 sec/batch\n",
      "Epoch: 1/20...  Training Step: 402...  Training loss: 2.6151...  0.0556 sec/batch\n",
      "Epoch: 1/20...  Training Step: 403...  Training loss: 2.6173...  0.0593 sec/batch\n",
      "Epoch: 1/20...  Training Step: 404...  Training loss: 2.5908...  0.0562 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 405...  Training loss: 2.6703...  0.0566 sec/batch\n",
      "Epoch: 1/20...  Training Step: 406...  Training loss: 2.6186...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 407...  Training loss: 2.6175...  0.0557 sec/batch\n",
      "Epoch: 1/20...  Training Step: 408...  Training loss: 2.5954...  0.0520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 409...  Training loss: 2.6139...  0.0593 sec/batch\n",
      "Epoch: 1/20...  Training Step: 410...  Training loss: 2.5897...  0.0558 sec/batch\n",
      "Epoch: 1/20...  Training Step: 411...  Training loss: 2.5770...  0.0534 sec/batch\n",
      "Epoch: 1/20...  Training Step: 412...  Training loss: 2.6020...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 413...  Training loss: 2.5677...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 414...  Training loss: 2.5891...  0.0589 sec/batch\n",
      "Epoch: 1/20...  Training Step: 415...  Training loss: 2.5513...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 416...  Training loss: 2.5477...  0.0612 sec/batch\n",
      "Epoch: 1/20...  Training Step: 417...  Training loss: 2.5681...  0.0537 sec/batch\n",
      "Epoch: 1/20...  Training Step: 418...  Training loss: 2.6038...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 419...  Training loss: 2.5910...  0.0566 sec/batch\n",
      "Epoch: 1/20...  Training Step: 420...  Training loss: 2.5980...  0.0604 sec/batch\n",
      "Epoch: 1/20...  Training Step: 421...  Training loss: 2.5609...  0.0581 sec/batch\n",
      "Epoch: 1/20...  Training Step: 422...  Training loss: 2.5869...  0.0569 sec/batch\n",
      "Epoch: 1/20...  Training Step: 423...  Training loss: 2.5935...  0.0534 sec/batch\n",
      "Epoch: 1/20...  Training Step: 424...  Training loss: 2.6054...  0.0536 sec/batch\n",
      "Epoch: 1/20...  Training Step: 425...  Training loss: 2.6070...  0.0559 sec/batch\n",
      "Epoch: 1/20...  Training Step: 426...  Training loss: 2.6170...  0.0578 sec/batch\n",
      "Epoch: 1/20...  Training Step: 427...  Training loss: 2.5816...  0.0592 sec/batch\n",
      "Epoch: 1/20...  Training Step: 428...  Training loss: 2.5777...  0.0543 sec/batch\n",
      "Epoch: 1/20...  Training Step: 429...  Training loss: 2.5791...  0.0599 sec/batch\n",
      "Epoch: 1/20...  Training Step: 430...  Training loss: 2.5513...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 431...  Training loss: 2.5758...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 432...  Training loss: 2.5928...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 433...  Training loss: 2.5729...  0.0563 sec/batch\n",
      "Epoch: 1/20...  Training Step: 434...  Training loss: 2.5556...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 435...  Training loss: 2.5733...  0.0537 sec/batch\n",
      "Epoch: 1/20...  Training Step: 436...  Training loss: 2.5737...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 437...  Training loss: 2.5449...  0.0560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 438...  Training loss: 2.5911...  0.0582 sec/batch\n",
      "Epoch: 1/20...  Training Step: 439...  Training loss: 2.5721...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 440...  Training loss: 2.5622...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 441...  Training loss: 2.5647...  0.0585 sec/batch\n",
      "Epoch: 1/20...  Training Step: 442...  Training loss: 2.5416...  0.0582 sec/batch\n",
      "Epoch: 1/20...  Training Step: 443...  Training loss: 2.5706...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 444...  Training loss: 2.5661...  0.0562 sec/batch\n",
      "Epoch: 1/20...  Training Step: 445...  Training loss: 2.5903...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 446...  Training loss: 2.5434...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 447...  Training loss: 2.5762...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 448...  Training loss: 2.5744...  0.0557 sec/batch\n",
      "Epoch: 1/20...  Training Step: 449...  Training loss: 2.5599...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 450...  Training loss: 2.5698...  0.0538 sec/batch\n",
      "Epoch: 1/20...  Training Step: 451...  Training loss: 2.6686...  0.0558 sec/batch\n",
      "Epoch: 1/20...  Training Step: 452...  Training loss: 2.5258...  0.0594 sec/batch\n",
      "Epoch: 1/20...  Training Step: 453...  Training loss: 2.5529...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 454...  Training loss: 2.5515...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 455...  Training loss: 2.5342...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 456...  Training loss: 2.6155...  0.0556 sec/batch\n",
      "Epoch: 1/20...  Training Step: 457...  Training loss: 2.5549...  0.0585 sec/batch\n",
      "Epoch: 1/20...  Training Step: 458...  Training loss: 2.5871...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 459...  Training loss: 2.5657...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 460...  Training loss: 2.5660...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 461...  Training loss: 2.5498...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 462...  Training loss: 2.5695...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 463...  Training loss: 2.5327...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 464...  Training loss: 2.5542...  0.0593 sec/batch\n",
      "Epoch: 1/20...  Training Step: 465...  Training loss: 2.5800...  0.0579 sec/batch\n",
      "Epoch: 1/20...  Training Step: 466...  Training loss: 2.5619...  0.0588 sec/batch\n",
      "Epoch: 1/20...  Training Step: 467...  Training loss: 2.6110...  0.0580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 468...  Training loss: 2.5512...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 469...  Training loss: 2.5968...  0.0622 sec/batch\n",
      "Epoch: 1/20...  Training Step: 470...  Training loss: 2.5501...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 471...  Training loss: 2.5340...  0.0567 sec/batch\n",
      "Epoch: 1/20...  Training Step: 472...  Training loss: 2.5059...  0.0596 sec/batch\n",
      "Epoch: 1/20...  Training Step: 473...  Training loss: 2.5204...  0.0562 sec/batch\n",
      "Epoch: 1/20...  Training Step: 474...  Training loss: 2.5303...  0.0594 sec/batch\n",
      "Epoch: 1/20...  Training Step: 475...  Training loss: 2.5856...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 476...  Training loss: 2.5892...  0.0589 sec/batch\n",
      "Epoch: 1/20...  Training Step: 477...  Training loss: 2.5365...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 478...  Training loss: 2.6166...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 479...  Training loss: 2.5232...  0.0562 sec/batch\n",
      "Epoch: 1/20...  Training Step: 480...  Training loss: 2.5560...  0.0558 sec/batch\n",
      "Epoch: 1/20...  Training Step: 481...  Training loss: 2.5656...  0.0606 sec/batch\n",
      "Epoch: 1/20...  Training Step: 482...  Training loss: 2.5248...  0.0586 sec/batch\n",
      "Epoch: 1/20...  Training Step: 483...  Training loss: 2.5270...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 484...  Training loss: 2.5655...  0.0520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 485...  Training loss: 2.5729...  0.0585 sec/batch\n",
      "Epoch: 1/20...  Training Step: 486...  Training loss: 2.5488...  0.0561 sec/batch\n",
      "Epoch: 1/20...  Training Step: 487...  Training loss: 2.6217...  0.0541 sec/batch\n",
      "Epoch: 1/20...  Training Step: 488...  Training loss: 2.5896...  0.0567 sec/batch\n",
      "Epoch: 1/20...  Training Step: 489...  Training loss: 2.5927...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 490...  Training loss: 2.6012...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 491...  Training loss: 2.6139...  0.0561 sec/batch\n",
      "Epoch: 1/20...  Training Step: 492...  Training loss: 2.6245...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 493...  Training loss: 2.6046...  0.0558 sec/batch\n",
      "Epoch: 1/20...  Training Step: 494...  Training loss: 2.5970...  0.0556 sec/batch\n",
      "Epoch: 1/20...  Training Step: 495...  Training loss: 2.5272...  0.0556 sec/batch\n",
      "Epoch: 1/20...  Training Step: 496...  Training loss: 2.5531...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 497...  Training loss: 2.5418...  0.0534 sec/batch\n",
      "Epoch: 1/20...  Training Step: 498...  Training loss: 2.5436...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 499...  Training loss: 2.5325...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 500...  Training loss: 2.5363...  0.0575 sec/batch\n",
      "Epoch: 1/20...  Training Step: 501...  Training loss: 2.5306...  0.0576 sec/batch\n",
      "Epoch: 1/20...  Training Step: 502...  Training loss: 2.5896...  0.0544 sec/batch\n",
      "Epoch: 1/20...  Training Step: 503...  Training loss: 2.5970...  0.0575 sec/batch\n",
      "Epoch: 1/20...  Training Step: 504...  Training loss: 2.5490...  0.0530 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 505...  Training loss: 2.5432...  0.0577 sec/batch\n",
      "Epoch: 1/20...  Training Step: 506...  Training loss: 2.4832...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 507...  Training loss: 2.5578...  0.0574 sec/batch\n",
      "Epoch: 1/20...  Training Step: 508...  Training loss: 2.5307...  0.0602 sec/batch\n",
      "Epoch: 1/20...  Training Step: 509...  Training loss: 2.5446...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 510...  Training loss: 2.5604...  0.0583 sec/batch\n",
      "Epoch: 1/20...  Training Step: 511...  Training loss: 2.5563...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 512...  Training loss: 2.4947...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 513...  Training loss: 2.6403...  0.0587 sec/batch\n",
      "Epoch: 1/20...  Training Step: 514...  Training loss: 2.6023...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 515...  Training loss: 2.5628...  0.0587 sec/batch\n",
      "Epoch: 1/20...  Training Step: 516...  Training loss: 2.5322...  0.0593 sec/batch\n",
      "Epoch: 1/20...  Training Step: 517...  Training loss: 2.5253...  0.0594 sec/batch\n",
      "Epoch: 1/20...  Training Step: 518...  Training loss: 2.5427...  0.0592 sec/batch\n",
      "Epoch: 1/20...  Training Step: 519...  Training loss: 2.5231...  0.0566 sec/batch\n",
      "Epoch: 1/20...  Training Step: 520...  Training loss: 2.5220...  0.0540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 521...  Training loss: 2.5039...  0.0592 sec/batch\n",
      "Epoch: 1/20...  Training Step: 522...  Training loss: 2.5048...  0.0608 sec/batch\n",
      "Epoch: 1/20...  Training Step: 523...  Training loss: 2.5368...  0.0561 sec/batch\n",
      "Epoch: 1/20...  Training Step: 524...  Training loss: 2.4795...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 525...  Training loss: 2.5084...  0.0560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 526...  Training loss: 2.5440...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 527...  Training loss: 2.5278...  0.0588 sec/batch\n",
      "Epoch: 1/20...  Training Step: 528...  Training loss: 2.5618...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 529...  Training loss: 2.5112...  0.0588 sec/batch\n",
      "Epoch: 1/20...  Training Step: 530...  Training loss: 2.5009...  0.0527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 531...  Training loss: 2.5241...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 532...  Training loss: 2.4774...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 533...  Training loss: 2.5382...  0.0542 sec/batch\n",
      "Epoch: 1/20...  Training Step: 534...  Training loss: 2.4925...  0.0541 sec/batch\n",
      "Epoch: 1/20...  Training Step: 535...  Training loss: 2.4978...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 536...  Training loss: 2.5011...  0.0537 sec/batch\n",
      "Epoch: 1/20...  Training Step: 537...  Training loss: 2.4977...  0.0595 sec/batch\n",
      "Epoch: 1/20...  Training Step: 538...  Training loss: 2.5623...  0.0585 sec/batch\n",
      "Epoch: 1/20...  Training Step: 539...  Training loss: 2.4895...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 540...  Training loss: 2.5197...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 541...  Training loss: 2.5266...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 542...  Training loss: 2.5188...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 543...  Training loss: 2.4836...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 544...  Training loss: 2.5031...  0.0596 sec/batch\n",
      "Epoch: 1/20...  Training Step: 545...  Training loss: 2.5236...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 546...  Training loss: 2.5251...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 547...  Training loss: 2.5005...  0.0558 sec/batch\n",
      "Epoch: 1/20...  Training Step: 548...  Training loss: 2.5459...  0.0524 sec/batch\n",
      "Epoch: 1/20...  Training Step: 549...  Training loss: 2.5398...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 550...  Training loss: 2.5240...  0.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 551...  Training loss: 2.4839...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 552...  Training loss: 2.5208...  0.0573 sec/batch\n",
      "Epoch: 1/20...  Training Step: 553...  Training loss: 2.4980...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 554...  Training loss: 2.5211...  0.0522 sec/batch\n",
      "Epoch: 1/20...  Training Step: 555...  Training loss: 2.5022...  0.0531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 556...  Training loss: 2.4592...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 557...  Training loss: 2.4806...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 558...  Training loss: 2.5146...  0.0578 sec/batch\n",
      "Epoch: 1/20...  Training Step: 559...  Training loss: 2.4979...  0.0559 sec/batch\n",
      "Epoch: 1/20...  Training Step: 560...  Training loss: 2.5107...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 561...  Training loss: 2.4668...  0.0587 sec/batch\n",
      "Epoch: 1/20...  Training Step: 562...  Training loss: 2.5123...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 563...  Training loss: 2.4974...  0.0565 sec/batch\n",
      "Epoch: 1/20...  Training Step: 564...  Training loss: 2.5079...  0.0556 sec/batch\n",
      "Epoch: 1/20...  Training Step: 565...  Training loss: 2.5454...  0.0559 sec/batch\n",
      "Epoch: 1/20...  Training Step: 566...  Training loss: 2.5048...  0.0525 sec/batch\n",
      "Epoch: 1/20...  Training Step: 567...  Training loss: 2.5453...  0.0581 sec/batch\n",
      "Epoch: 1/20...  Training Step: 568...  Training loss: 2.5598...  0.0557 sec/batch\n",
      "Epoch: 1/20...  Training Step: 569...  Training loss: 2.5426...  0.0533 sec/batch\n",
      "Epoch: 1/20...  Training Step: 570...  Training loss: 2.5049...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 571...  Training loss: 2.4709...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 572...  Training loss: 2.5464...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 573...  Training loss: 2.4865...  0.0533 sec/batch\n",
      "Epoch: 1/20...  Training Step: 574...  Training loss: 2.4907...  0.0575 sec/batch\n",
      "Epoch: 1/20...  Training Step: 575...  Training loss: 2.5025...  0.0544 sec/batch\n",
      "Epoch: 1/20...  Training Step: 576...  Training loss: 2.5584...  0.0559 sec/batch\n",
      "Epoch: 1/20...  Training Step: 577...  Training loss: 2.5109...  0.0524 sec/batch\n",
      "Epoch: 1/20...  Training Step: 578...  Training loss: 2.4988...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 579...  Training loss: 2.5517...  0.0524 sec/batch\n",
      "Epoch: 1/20...  Training Step: 580...  Training loss: 2.5387...  0.0557 sec/batch\n",
      "Epoch: 1/20...  Training Step: 581...  Training loss: 2.5005...  0.0577 sec/batch\n",
      "Epoch: 1/20...  Training Step: 582...  Training loss: 2.4925...  0.0526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 583...  Training loss: 2.4887...  0.0585 sec/batch\n",
      "Epoch: 1/20...  Training Step: 584...  Training loss: 2.5285...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 585...  Training loss: 2.5082...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 586...  Training loss: 2.4660...  0.0523 sec/batch\n",
      "Epoch: 1/20...  Training Step: 587...  Training loss: 2.4890...  0.0523 sec/batch\n",
      "Epoch: 1/20...  Training Step: 588...  Training loss: 2.5125...  0.0543 sec/batch\n",
      "Epoch: 1/20...  Training Step: 589...  Training loss: 2.4472...  0.0549 sec/batch\n",
      "Epoch: 1/20...  Training Step: 590...  Training loss: 2.5471...  0.0559 sec/batch\n",
      "Epoch: 1/20...  Training Step: 591...  Training loss: 2.4506...  0.0574 sec/batch\n",
      "Epoch: 1/20...  Training Step: 592...  Training loss: 2.4955...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 593...  Training loss: 2.4829...  0.0538 sec/batch\n",
      "Epoch: 1/20...  Training Step: 594...  Training loss: 2.4597...  0.0595 sec/batch\n",
      "Epoch: 1/20...  Training Step: 595...  Training loss: 2.4578...  0.0525 sec/batch\n",
      "Epoch: 1/20...  Training Step: 596...  Training loss: 2.4756...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 597...  Training loss: 2.4560...  0.0563 sec/batch\n",
      "Epoch: 1/20...  Training Step: 598...  Training loss: 2.4607...  0.0535 sec/batch\n",
      "Epoch: 1/20...  Training Step: 599...  Training loss: 2.4873...  0.0528 sec/batch\n",
      "Epoch: 1/20...  Training Step: 600...  Training loss: 2.5227...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 601...  Training loss: 2.4849...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 602...  Training loss: 2.5049...  0.0584 sec/batch\n",
      "Epoch: 1/20...  Training Step: 603...  Training loss: 2.5313...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 604...  Training loss: 2.4529...  0.0604 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 605...  Training loss: 2.4762...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 606...  Training loss: 2.4682...  0.0529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 607...  Training loss: 2.4414...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 608...  Training loss: 2.4718...  0.0548 sec/batch\n",
      "Epoch: 1/20...  Training Step: 609...  Training loss: 2.4387...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 610...  Training loss: 2.4874...  0.0555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 611...  Training loss: 2.4799...  0.0534 sec/batch\n",
      "Epoch: 1/20...  Training Step: 612...  Training loss: 2.4928...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 613...  Training loss: 2.4592...  0.0596 sec/batch\n",
      "Epoch: 1/20...  Training Step: 614...  Training loss: 2.4616...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 615...  Training loss: 2.4409...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 616...  Training loss: 2.4979...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 617...  Training loss: 2.5088...  0.0553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 618...  Training loss: 2.4500...  0.0591 sec/batch\n",
      "Epoch: 1/20...  Training Step: 619...  Training loss: 2.4533...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 620...  Training loss: 2.4528...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 621...  Training loss: 2.6068...  0.0548 sec/batch\n",
      "Epoch: 2/20...  Training Step: 622...  Training loss: 2.5003...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 623...  Training loss: 2.4887...  0.0562 sec/batch\n",
      "Epoch: 2/20...  Training Step: 624...  Training loss: 2.4515...  0.0536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 625...  Training loss: 2.4628...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 626...  Training loss: 2.4944...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 627...  Training loss: 2.4526...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 628...  Training loss: 2.4134...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 629...  Training loss: 2.4245...  0.0563 sec/batch\n",
      "Epoch: 2/20...  Training Step: 630...  Training loss: 2.4795...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 631...  Training loss: 2.4526...  0.0546 sec/batch\n",
      "Epoch: 2/20...  Training Step: 632...  Training loss: 2.4488...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 633...  Training loss: 2.5133...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 634...  Training loss: 2.4158...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 635...  Training loss: 2.4525...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 636...  Training loss: 2.4708...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 637...  Training loss: 2.4957...  0.0583 sec/batch\n",
      "Epoch: 2/20...  Training Step: 638...  Training loss: 2.4799...  0.0549 sec/batch\n",
      "Epoch: 2/20...  Training Step: 639...  Training loss: 2.4305...  0.0673 sec/batch\n",
      "Epoch: 2/20...  Training Step: 640...  Training loss: 2.4616...  0.0607 sec/batch\n",
      "Epoch: 2/20...  Training Step: 641...  Training loss: 2.4869...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 642...  Training loss: 2.4288...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 643...  Training loss: 2.4539...  0.0572 sec/batch\n",
      "Epoch: 2/20...  Training Step: 644...  Training loss: 2.4581...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 645...  Training loss: 2.4799...  0.0541 sec/batch\n",
      "Epoch: 2/20...  Training Step: 646...  Training loss: 2.4670...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 647...  Training loss: 2.4716...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 648...  Training loss: 2.4684...  0.0598 sec/batch\n",
      "Epoch: 2/20...  Training Step: 649...  Training loss: 2.5176...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 650...  Training loss: 2.4326...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 651...  Training loss: 2.4265...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 652...  Training loss: 2.4763...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 653...  Training loss: 2.4425...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 654...  Training loss: 2.4561...  0.0584 sec/batch\n",
      "Epoch: 2/20...  Training Step: 655...  Training loss: 2.4514...  0.0549 sec/batch\n",
      "Epoch: 2/20...  Training Step: 656...  Training loss: 2.4770...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 657...  Training loss: 2.4453...  0.0602 sec/batch\n",
      "Epoch: 2/20...  Training Step: 658...  Training loss: 2.4570...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 659...  Training loss: 2.4444...  0.0579 sec/batch\n",
      "Epoch: 2/20...  Training Step: 660...  Training loss: 2.4789...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 661...  Training loss: 2.4311...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 662...  Training loss: 2.4479...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 663...  Training loss: 2.4224...  0.0545 sec/batch\n",
      "Epoch: 2/20...  Training Step: 664...  Training loss: 2.4492...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 665...  Training loss: 2.4488...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 666...  Training loss: 2.4384...  0.0548 sec/batch\n",
      "Epoch: 2/20...  Training Step: 667...  Training loss: 2.4135...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 668...  Training loss: 2.4735...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 669...  Training loss: 2.4833...  0.0525 sec/batch\n",
      "Epoch: 2/20...  Training Step: 670...  Training loss: 2.4484...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 671...  Training loss: 2.3935...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 672...  Training loss: 2.4383...  0.0547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 673...  Training loss: 2.4586...  0.0593 sec/batch\n",
      "Epoch: 2/20...  Training Step: 674...  Training loss: 2.4958...  0.0546 sec/batch\n",
      "Epoch: 2/20...  Training Step: 675...  Training loss: 2.4959...  0.0583 sec/batch\n",
      "Epoch: 2/20...  Training Step: 676...  Training loss: 2.4326...  0.0571 sec/batch\n",
      "Epoch: 2/20...  Training Step: 677...  Training loss: 2.4276...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 678...  Training loss: 2.4711...  0.0548 sec/batch\n",
      "Epoch: 2/20...  Training Step: 679...  Training loss: 2.4228...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 680...  Training loss: 2.4789...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 681...  Training loss: 2.4611...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 682...  Training loss: 2.4351...  0.0569 sec/batch\n",
      "Epoch: 2/20...  Training Step: 683...  Training loss: 2.4627...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 684...  Training loss: 2.4305...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 685...  Training loss: 2.4075...  0.0589 sec/batch\n",
      "Epoch: 2/20...  Training Step: 686...  Training loss: 2.4119...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 687...  Training loss: 2.3929...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 688...  Training loss: 2.4030...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 689...  Training loss: 2.4525...  0.0525 sec/batch\n",
      "Epoch: 2/20...  Training Step: 690...  Training loss: 2.4176...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 691...  Training loss: 2.4236...  0.0523 sec/batch\n",
      "Epoch: 2/20...  Training Step: 692...  Training loss: 2.4460...  0.0549 sec/batch\n",
      "Epoch: 2/20...  Training Step: 693...  Training loss: 2.4427...  0.0582 sec/batch\n",
      "Epoch: 2/20...  Training Step: 694...  Training loss: 2.4264...  0.0573 sec/batch\n",
      "Epoch: 2/20...  Training Step: 695...  Training loss: 2.4649...  0.0570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 696...  Training loss: 2.4511...  0.0575 sec/batch\n",
      "Epoch: 2/20...  Training Step: 697...  Training loss: 2.4916...  0.0549 sec/batch\n",
      "Epoch: 2/20...  Training Step: 698...  Training loss: 2.4242...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 699...  Training loss: 2.4627...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 700...  Training loss: 2.4688...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 701...  Training loss: 2.4601...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 702...  Training loss: 2.4072...  0.0527 sec/batch\n",
      "Epoch: 2/20...  Training Step: 703...  Training loss: 2.4271...  0.0523 sec/batch\n",
      "Epoch: 2/20...  Training Step: 704...  Training loss: 2.4624...  0.0561 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 705...  Training loss: 2.4474...  0.0587 sec/batch\n",
      "Epoch: 2/20...  Training Step: 706...  Training loss: 2.4332...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 707...  Training loss: 2.4310...  0.0522 sec/batch\n",
      "Epoch: 2/20...  Training Step: 708...  Training loss: 2.5048...  0.0581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 709...  Training loss: 2.4331...  0.0576 sec/batch\n",
      "Epoch: 2/20...  Training Step: 710...  Training loss: 2.4370...  0.0590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 711...  Training loss: 2.4099...  0.0540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 712...  Training loss: 2.4816...  0.0612 sec/batch\n",
      "Epoch: 2/20...  Training Step: 713...  Training loss: 2.4454...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 714...  Training loss: 2.4161...  0.0523 sec/batch\n",
      "Epoch: 2/20...  Training Step: 715...  Training loss: 2.4583...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 716...  Training loss: 2.4312...  0.0522 sec/batch\n",
      "Epoch: 2/20...  Training Step: 717...  Training loss: 2.4608...  0.0552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 718...  Training loss: 2.4233...  0.0591 sec/batch\n",
      "Epoch: 2/20...  Training Step: 719...  Training loss: 2.4855...  0.0588 sec/batch\n",
      "Epoch: 2/20...  Training Step: 720...  Training loss: 2.4321...  0.0548 sec/batch\n",
      "Epoch: 2/20...  Training Step: 721...  Training loss: 2.4276...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 722...  Training loss: 2.4484...  0.0583 sec/batch\n",
      "Epoch: 2/20...  Training Step: 723...  Training loss: 2.4311...  0.0589 sec/batch\n",
      "Epoch: 2/20...  Training Step: 724...  Training loss: 2.3922...  0.0564 sec/batch\n",
      "Epoch: 2/20...  Training Step: 725...  Training loss: 2.4438...  0.0572 sec/batch\n",
      "Epoch: 2/20...  Training Step: 726...  Training loss: 2.3969...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 727...  Training loss: 2.4620...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 728...  Training loss: 2.4257...  0.0577 sec/batch\n",
      "Epoch: 2/20...  Training Step: 729...  Training loss: 2.3981...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 730...  Training loss: 2.4054...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 731...  Training loss: 2.3947...  0.0588 sec/batch\n",
      "Epoch: 2/20...  Training Step: 732...  Training loss: 2.4604...  0.0548 sec/batch\n",
      "Epoch: 2/20...  Training Step: 733...  Training loss: 2.4293...  0.0565 sec/batch\n",
      "Epoch: 2/20...  Training Step: 734...  Training loss: 2.4285...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 735...  Training loss: 2.4562...  0.0581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 736...  Training loss: 2.4438...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 737...  Training loss: 2.4218...  0.0580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 738...  Training loss: 2.4471...  0.0527 sec/batch\n",
      "Epoch: 2/20...  Training Step: 739...  Training loss: 2.4072...  0.0598 sec/batch\n",
      "Epoch: 2/20...  Training Step: 740...  Training loss: 2.4257...  0.0548 sec/batch\n",
      "Epoch: 2/20...  Training Step: 741...  Training loss: 2.3930...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 742...  Training loss: 2.3653...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 743...  Training loss: 2.4196...  0.0552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 744...  Training loss: 2.4212...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 745...  Training loss: 2.4303...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 746...  Training loss: 2.4759...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 747...  Training loss: 2.4543...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 748...  Training loss: 2.4146...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 749...  Training loss: 2.3890...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 750...  Training loss: 2.4353...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 751...  Training loss: 2.3827...  0.0522 sec/batch\n",
      "Epoch: 2/20...  Training Step: 752...  Training loss: 2.4481...  0.0536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 753...  Training loss: 2.4597...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 754...  Training loss: 2.4285...  0.0561 sec/batch\n",
      "Epoch: 2/20...  Training Step: 755...  Training loss: 2.3895...  0.0563 sec/batch\n",
      "Epoch: 2/20...  Training Step: 756...  Training loss: 2.4161...  0.0581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 757...  Training loss: 2.4146...  0.0538 sec/batch\n",
      "Epoch: 2/20...  Training Step: 758...  Training loss: 2.4292...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 759...  Training loss: 2.4386...  0.0545 sec/batch\n",
      "Epoch: 2/20...  Training Step: 760...  Training loss: 2.3922...  0.0527 sec/batch\n",
      "Epoch: 2/20...  Training Step: 761...  Training loss: 2.4018...  0.0536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 762...  Training loss: 2.4245...  0.0527 sec/batch\n",
      "Epoch: 2/20...  Training Step: 763...  Training loss: 2.4121...  0.0547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 764...  Training loss: 2.4197...  0.0525 sec/batch\n",
      "Epoch: 2/20...  Training Step: 765...  Training loss: 2.3923...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 766...  Training loss: 2.4349...  0.0569 sec/batch\n",
      "Epoch: 2/20...  Training Step: 767...  Training loss: 2.3695...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 768...  Training loss: 2.4527...  0.0525 sec/batch\n",
      "Epoch: 2/20...  Training Step: 769...  Training loss: 2.3951...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 770...  Training loss: 2.4309...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 771...  Training loss: 2.4311...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 772...  Training loss: 2.4382...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 773...  Training loss: 2.3830...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 774...  Training loss: 2.4420...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 775...  Training loss: 2.4009...  0.0552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 776...  Training loss: 2.4182...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 777...  Training loss: 2.3731...  0.0562 sec/batch\n",
      "Epoch: 2/20...  Training Step: 778...  Training loss: 2.4403...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 779...  Training loss: 2.4581...  0.0595 sec/batch\n",
      "Epoch: 2/20...  Training Step: 780...  Training loss: 2.3708...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 781...  Training loss: 2.4064...  0.0572 sec/batch\n",
      "Epoch: 2/20...  Training Step: 782...  Training loss: 2.3976...  0.0571 sec/batch\n",
      "Epoch: 2/20...  Training Step: 783...  Training loss: 2.4106...  0.0585 sec/batch\n",
      "Epoch: 2/20...  Training Step: 784...  Training loss: 2.3877...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 785...  Training loss: 2.4377...  0.0539 sec/batch\n",
      "Epoch: 2/20...  Training Step: 786...  Training loss: 2.3589...  0.0534 sec/batch\n",
      "Epoch: 2/20...  Training Step: 787...  Training loss: 2.4501...  0.0586 sec/batch\n",
      "Epoch: 2/20...  Training Step: 788...  Training loss: 2.3855...  0.0603 sec/batch\n",
      "Epoch: 2/20...  Training Step: 789...  Training loss: 2.3941...  0.0594 sec/batch\n",
      "Epoch: 2/20...  Training Step: 790...  Training loss: 2.3936...  0.0593 sec/batch\n",
      "Epoch: 2/20...  Training Step: 791...  Training loss: 2.3842...  0.0580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 792...  Training loss: 2.4013...  0.0535 sec/batch\n",
      "Epoch: 2/20...  Training Step: 793...  Training loss: 2.4369...  0.0563 sec/batch\n",
      "Epoch: 2/20...  Training Step: 794...  Training loss: 2.4184...  0.0575 sec/batch\n",
      "Epoch: 2/20...  Training Step: 795...  Training loss: 2.3985...  0.0596 sec/batch\n",
      "Epoch: 2/20...  Training Step: 796...  Training loss: 2.4336...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 797...  Training loss: 2.4119...  0.0540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 798...  Training loss: 2.4479...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 799...  Training loss: 2.3817...  0.0597 sec/batch\n",
      "Epoch: 2/20...  Training Step: 800...  Training loss: 2.3866...  0.0578 sec/batch\n",
      "Epoch: 2/20...  Training Step: 801...  Training loss: 2.4047...  0.0582 sec/batch\n",
      "Epoch: 2/20...  Training Step: 802...  Training loss: 2.4132...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 803...  Training loss: 2.4334...  0.0537 sec/batch\n",
      "Epoch: 2/20...  Training Step: 804...  Training loss: 2.3557...  0.0527 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 805...  Training loss: 2.3811...  0.0578 sec/batch\n",
      "Epoch: 2/20...  Training Step: 806...  Training loss: 2.4054...  0.0536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 807...  Training loss: 2.3657...  0.0595 sec/batch\n",
      "Epoch: 2/20...  Training Step: 808...  Training loss: 2.3687...  0.0536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 809...  Training loss: 2.3852...  0.0566 sec/batch\n",
      "Epoch: 2/20...  Training Step: 810...  Training loss: 2.4511...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 811...  Training loss: 2.4869...  0.0537 sec/batch\n",
      "Epoch: 2/20...  Training Step: 812...  Training loss: 2.4617...  0.0563 sec/batch\n",
      "Epoch: 2/20...  Training Step: 813...  Training loss: 2.4216...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 814...  Training loss: 2.3869...  0.0585 sec/batch\n",
      "Epoch: 2/20...  Training Step: 815...  Training loss: 2.4299...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 816...  Training loss: 2.4498...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 817...  Training loss: 2.4271...  0.0575 sec/batch\n",
      "Epoch: 2/20...  Training Step: 818...  Training loss: 2.4167...  0.0562 sec/batch\n",
      "Epoch: 2/20...  Training Step: 819...  Training loss: 2.3889...  0.0599 sec/batch\n",
      "Epoch: 2/20...  Training Step: 820...  Training loss: 2.3830...  0.0525 sec/batch\n",
      "Epoch: 2/20...  Training Step: 821...  Training loss: 2.3930...  0.0563 sec/batch\n",
      "Epoch: 2/20...  Training Step: 822...  Training loss: 2.3864...  0.0569 sec/batch\n",
      "Epoch: 2/20...  Training Step: 823...  Training loss: 2.3611...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 824...  Training loss: 2.3806...  0.0547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 825...  Training loss: 2.4007...  0.0541 sec/batch\n",
      "Epoch: 2/20...  Training Step: 826...  Training loss: 2.3747...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 827...  Training loss: 2.3925...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 828...  Training loss: 2.4027...  0.0597 sec/batch\n",
      "Epoch: 2/20...  Training Step: 829...  Training loss: 2.4179...  0.0536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 830...  Training loss: 2.3841...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 831...  Training loss: 2.3879...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 832...  Training loss: 2.4233...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 833...  Training loss: 2.4110...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 834...  Training loss: 2.4163...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 835...  Training loss: 2.4089...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 836...  Training loss: 2.4144...  0.0589 sec/batch\n",
      "Epoch: 2/20...  Training Step: 837...  Training loss: 2.3666...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 838...  Training loss: 2.3700...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 839...  Training loss: 2.4512...  0.0521 sec/batch\n",
      "Epoch: 2/20...  Training Step: 840...  Training loss: 2.4229...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 841...  Training loss: 2.3664...  0.0561 sec/batch\n",
      "Epoch: 2/20...  Training Step: 842...  Training loss: 2.3726...  0.0536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 843...  Training loss: 2.4294...  0.0597 sec/batch\n",
      "Epoch: 2/20...  Training Step: 844...  Training loss: 2.3747...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 845...  Training loss: 2.3930...  0.0563 sec/batch\n",
      "Epoch: 2/20...  Training Step: 846...  Training loss: 2.4365...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 847...  Training loss: 2.4215...  0.0546 sec/batch\n",
      "Epoch: 2/20...  Training Step: 848...  Training loss: 2.3717...  0.0587 sec/batch\n",
      "Epoch: 2/20...  Training Step: 849...  Training loss: 2.4393...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 850...  Training loss: 2.3872...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 851...  Training loss: 2.4072...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 852...  Training loss: 2.4203...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 853...  Training loss: 2.3924...  0.0552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 854...  Training loss: 2.3797...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 855...  Training loss: 2.3945...  0.0590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 856...  Training loss: 2.4140...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 857...  Training loss: 2.3859...  0.0563 sec/batch\n",
      "Epoch: 2/20...  Training Step: 858...  Training loss: 2.3381...  0.0600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 859...  Training loss: 2.3664...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 860...  Training loss: 2.4124...  0.0561 sec/batch\n",
      "Epoch: 2/20...  Training Step: 861...  Training loss: 2.4060...  0.0547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 862...  Training loss: 2.3748...  0.0535 sec/batch\n",
      "Epoch: 2/20...  Training Step: 863...  Training loss: 2.3827...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 864...  Training loss: 2.3937...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 865...  Training loss: 2.3532...  0.0564 sec/batch\n",
      "Epoch: 2/20...  Training Step: 866...  Training loss: 2.4003...  0.0522 sec/batch\n",
      "Epoch: 2/20...  Training Step: 867...  Training loss: 2.3915...  0.0670 sec/batch\n",
      "Epoch: 2/20...  Training Step: 868...  Training loss: 2.3956...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 869...  Training loss: 2.3591...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 870...  Training loss: 2.3853...  0.0549 sec/batch\n",
      "Epoch: 2/20...  Training Step: 871...  Training loss: 2.3731...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 872...  Training loss: 2.3391...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 873...  Training loss: 2.3971...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 874...  Training loss: 2.3969...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 875...  Training loss: 2.4099...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 876...  Training loss: 2.3974...  0.0544 sec/batch\n",
      "Epoch: 2/20...  Training Step: 877...  Training loss: 2.3958...  0.0572 sec/batch\n",
      "Epoch: 2/20...  Training Step: 878...  Training loss: 2.4074...  0.0534 sec/batch\n",
      "Epoch: 2/20...  Training Step: 879...  Training loss: 2.3932...  0.0590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 880...  Training loss: 2.3417...  0.0534 sec/batch\n",
      "Epoch: 2/20...  Training Step: 881...  Training loss: 2.3736...  0.0525 sec/batch\n",
      "Epoch: 2/20...  Training Step: 882...  Training loss: 2.3706...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 883...  Training loss: 2.3829...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 884...  Training loss: 2.4202...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 885...  Training loss: 2.4176...  0.0582 sec/batch\n",
      "Epoch: 2/20...  Training Step: 886...  Training loss: 2.3347...  0.0610 sec/batch\n",
      "Epoch: 2/20...  Training Step: 887...  Training loss: 2.4060...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 888...  Training loss: 2.3783...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 889...  Training loss: 2.3894...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 890...  Training loss: 2.3824...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 891...  Training loss: 2.3920...  0.0552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 892...  Training loss: 2.3634...  0.0546 sec/batch\n",
      "Epoch: 2/20...  Training Step: 893...  Training loss: 2.3746...  0.0542 sec/batch\n",
      "Epoch: 2/20...  Training Step: 894...  Training loss: 2.3522...  0.0593 sec/batch\n",
      "Epoch: 2/20...  Training Step: 895...  Training loss: 2.3741...  0.0539 sec/batch\n",
      "Epoch: 2/20...  Training Step: 896...  Training loss: 2.3972...  0.0562 sec/batch\n",
      "Epoch: 2/20...  Training Step: 897...  Training loss: 2.3944...  0.0541 sec/batch\n",
      "Epoch: 2/20...  Training Step: 898...  Training loss: 2.3543...  0.0545 sec/batch\n",
      "Epoch: 2/20...  Training Step: 899...  Training loss: 2.4024...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 900...  Training loss: 2.4325...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 901...  Training loss: 2.3618...  0.0538 sec/batch\n",
      "Epoch: 2/20...  Training Step: 902...  Training loss: 2.3317...  0.0523 sec/batch\n",
      "Epoch: 2/20...  Training Step: 903...  Training loss: 2.3852...  0.0573 sec/batch\n",
      "Epoch: 2/20...  Training Step: 904...  Training loss: 2.3466...  0.0529 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 905...  Training loss: 2.3243...  0.0534 sec/batch\n",
      "Epoch: 2/20...  Training Step: 906...  Training loss: 2.3438...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 907...  Training loss: 2.3414...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 908...  Training loss: 2.3627...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 909...  Training loss: 2.3803...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 910...  Training loss: 2.3880...  0.0575 sec/batch\n",
      "Epoch: 2/20...  Training Step: 911...  Training loss: 2.3792...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 912...  Training loss: 2.3861...  0.0565 sec/batch\n",
      "Epoch: 2/20...  Training Step: 913...  Training loss: 2.4007...  0.0547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 914...  Training loss: 2.3498...  0.0602 sec/batch\n",
      "Epoch: 2/20...  Training Step: 915...  Training loss: 2.3576...  0.0538 sec/batch\n",
      "Epoch: 2/20...  Training Step: 916...  Training loss: 2.3551...  0.0561 sec/batch\n",
      "Epoch: 2/20...  Training Step: 917...  Training loss: 2.3872...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 918...  Training loss: 2.3784...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 919...  Training loss: 2.4209...  0.0574 sec/batch\n",
      "Epoch: 2/20...  Training Step: 920...  Training loss: 2.3690...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 921...  Training loss: 2.3560...  0.0570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 922...  Training loss: 2.4055...  0.0562 sec/batch\n",
      "Epoch: 2/20...  Training Step: 923...  Training loss: 2.3477...  0.0601 sec/batch\n",
      "Epoch: 2/20...  Training Step: 924...  Training loss: 2.3789...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 925...  Training loss: 2.3760...  0.0544 sec/batch\n",
      "Epoch: 2/20...  Training Step: 926...  Training loss: 2.4077...  0.0552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 927...  Training loss: 2.3697...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 928...  Training loss: 2.3299...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 929...  Training loss: 2.3827...  0.0602 sec/batch\n",
      "Epoch: 2/20...  Training Step: 930...  Training loss: 2.3157...  0.0575 sec/batch\n",
      "Epoch: 2/20...  Training Step: 931...  Training loss: 2.3555...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 932...  Training loss: 2.3753...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 933...  Training loss: 2.3578...  0.0582 sec/batch\n",
      "Epoch: 2/20...  Training Step: 934...  Training loss: 2.3463...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 935...  Training loss: 2.3766...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 936...  Training loss: 2.3676...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 937...  Training loss: 2.3204...  0.0534 sec/batch\n",
      "Epoch: 2/20...  Training Step: 938...  Training loss: 2.3447...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 939...  Training loss: 2.3715...  0.0652 sec/batch\n",
      "Epoch: 2/20...  Training Step: 940...  Training loss: 2.4027...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 941...  Training loss: 2.3460...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 942...  Training loss: 2.3313...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 943...  Training loss: 2.3537...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 944...  Training loss: 2.3409...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 945...  Training loss: 2.3338...  0.0548 sec/batch\n",
      "Epoch: 2/20...  Training Step: 946...  Training loss: 2.3546...  0.0592 sec/batch\n",
      "Epoch: 2/20...  Training Step: 947...  Training loss: 2.3239...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 948...  Training loss: 2.3084...  0.0577 sec/batch\n",
      "Epoch: 2/20...  Training Step: 949...  Training loss: 2.3666...  0.0542 sec/batch\n",
      "Epoch: 2/20...  Training Step: 950...  Training loss: 2.3100...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 951...  Training loss: 2.3655...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 952...  Training loss: 2.3440...  0.0594 sec/batch\n",
      "Epoch: 2/20...  Training Step: 953...  Training loss: 2.3112...  0.0536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 954...  Training loss: 2.3187...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 955...  Training loss: 2.3394...  0.0576 sec/batch\n",
      "Epoch: 2/20...  Training Step: 956...  Training loss: 2.3571...  0.0549 sec/batch\n",
      "Epoch: 2/20...  Training Step: 957...  Training loss: 2.3314...  0.0567 sec/batch\n",
      "Epoch: 2/20...  Training Step: 958...  Training loss: 2.3490...  0.0606 sec/batch\n",
      "Epoch: 2/20...  Training Step: 959...  Training loss: 2.3383...  0.0583 sec/batch\n",
      "Epoch: 2/20...  Training Step: 960...  Training loss: 2.3695...  0.0569 sec/batch\n",
      "Epoch: 2/20...  Training Step: 961...  Training loss: 2.3666...  0.0540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 962...  Training loss: 2.3534...  0.0541 sec/batch\n",
      "Epoch: 2/20...  Training Step: 963...  Training loss: 2.3583...  0.0547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 964...  Training loss: 2.3389...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 965...  Training loss: 2.3263...  0.0609 sec/batch\n",
      "Epoch: 2/20...  Training Step: 966...  Training loss: 2.4011...  0.0596 sec/batch\n",
      "Epoch: 2/20...  Training Step: 967...  Training loss: 2.3486...  0.0594 sec/batch\n",
      "Epoch: 2/20...  Training Step: 968...  Training loss: 2.3260...  0.0566 sec/batch\n",
      "Epoch: 2/20...  Training Step: 969...  Training loss: 2.3256...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 970...  Training loss: 2.2957...  0.0535 sec/batch\n",
      "Epoch: 2/20...  Training Step: 971...  Training loss: 2.3452...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 972...  Training loss: 2.3271...  0.0562 sec/batch\n",
      "Epoch: 2/20...  Training Step: 973...  Training loss: 2.3710...  0.0601 sec/batch\n",
      "Epoch: 2/20...  Training Step: 974...  Training loss: 2.3215...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 975...  Training loss: 2.3339...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 976...  Training loss: 2.3971...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 977...  Training loss: 2.4300...  0.0592 sec/batch\n",
      "Epoch: 2/20...  Training Step: 978...  Training loss: 2.3855...  0.0566 sec/batch\n",
      "Epoch: 2/20...  Training Step: 979...  Training loss: 2.3316...  0.0593 sec/batch\n",
      "Epoch: 2/20...  Training Step: 980...  Training loss: 2.3566...  0.0583 sec/batch\n",
      "Epoch: 2/20...  Training Step: 981...  Training loss: 2.3962...  0.0573 sec/batch\n",
      "Epoch: 2/20...  Training Step: 982...  Training loss: 2.3594...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 983...  Training loss: 2.2905...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 984...  Training loss: 2.3046...  0.0572 sec/batch\n",
      "Epoch: 2/20...  Training Step: 985...  Training loss: 2.3524...  0.0549 sec/batch\n",
      "Epoch: 2/20...  Training Step: 986...  Training loss: 2.3681...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 987...  Training loss: 2.3497...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 988...  Training loss: 2.3855...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 989...  Training loss: 2.3622...  0.0562 sec/batch\n",
      "Epoch: 2/20...  Training Step: 990...  Training loss: 2.3500...  0.0534 sec/batch\n",
      "Epoch: 2/20...  Training Step: 991...  Training loss: 2.3866...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 992...  Training loss: 2.4063...  0.0539 sec/batch\n",
      "Epoch: 2/20...  Training Step: 993...  Training loss: 2.3663...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 994...  Training loss: 2.3523...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 995...  Training loss: 2.3280...  0.0536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 996...  Training loss: 2.3735...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 997...  Training loss: 2.3421...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 998...  Training loss: 2.3806...  0.0540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 999...  Training loss: 2.3831...  0.0567 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1000...  Training loss: 2.3150...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1001...  Training loss: 2.2941...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1002...  Training loss: 2.3782...  0.0580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1003...  Training loss: 2.3148...  0.0572 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1004...  Training loss: 2.3541...  0.0605 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 1005...  Training loss: 2.3561...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1006...  Training loss: 2.3090...  0.0527 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1007...  Training loss: 2.3014...  0.0596 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1008...  Training loss: 2.3603...  0.0552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1009...  Training loss: 2.2715...  0.0569 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1010...  Training loss: 2.3030...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1011...  Training loss: 2.3737...  0.0525 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1012...  Training loss: 2.2827...  0.0583 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1013...  Training loss: 2.3358...  0.0592 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1014...  Training loss: 2.3317...  0.0583 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1015...  Training loss: 2.3329...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1016...  Training loss: 2.3214...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1017...  Training loss: 2.3281...  0.0570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1018...  Training loss: 2.3502...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1019...  Training loss: 2.3193...  0.0594 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1020...  Training loss: 2.3484...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1021...  Training loss: 2.3493...  0.0587 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1022...  Training loss: 2.3361...  0.0567 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1023...  Training loss: 2.3564...  0.0635 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1024...  Training loss: 2.3467...  0.0565 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1025...  Training loss: 2.3691...  0.0537 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1026...  Training loss: 2.3728...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1027...  Training loss: 2.3840...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1028...  Training loss: 2.3152...  0.0598 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1029...  Training loss: 2.3728...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1030...  Training loss: 2.3317...  0.0521 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1031...  Training loss: 2.3255...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1032...  Training loss: 2.3682...  0.0620 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1033...  Training loss: 2.3256...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1034...  Training loss: 2.3464...  0.0595 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1035...  Training loss: 2.2893...  0.0547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1036...  Training loss: 2.2992...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1037...  Training loss: 2.3172...  0.0540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1038...  Training loss: 2.3559...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1039...  Training loss: 2.3564...  0.0564 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1040...  Training loss: 2.3555...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1041...  Training loss: 2.2957...  0.0547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1042...  Training loss: 2.3184...  0.0592 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1043...  Training loss: 2.3541...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1044...  Training loss: 2.3510...  0.0565 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1045...  Training loss: 2.3631...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1046...  Training loss: 2.3567...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1047...  Training loss: 2.3299...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1048...  Training loss: 2.2919...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1049...  Training loss: 2.3320...  0.0562 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1050...  Training loss: 2.3057...  0.0574 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1051...  Training loss: 2.3230...  0.0535 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1052...  Training loss: 2.3705...  0.0535 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1053...  Training loss: 2.3502...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1054...  Training loss: 2.3126...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1055...  Training loss: 2.3185...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1056...  Training loss: 2.3329...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1057...  Training loss: 2.2864...  0.0635 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1058...  Training loss: 2.3509...  0.0543 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1059...  Training loss: 2.3277...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1060...  Training loss: 2.2924...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1061...  Training loss: 2.3216...  0.0535 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1062...  Training loss: 2.2943...  0.0522 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1063...  Training loss: 2.3663...  0.0593 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1064...  Training loss: 2.3180...  0.0579 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1065...  Training loss: 2.3282...  0.0563 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1066...  Training loss: 2.3094...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1067...  Training loss: 2.3329...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1068...  Training loss: 2.3092...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1069...  Training loss: 2.3295...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1070...  Training loss: 2.3514...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1071...  Training loss: 2.3979...  0.0568 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1072...  Training loss: 2.3222...  0.0535 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1073...  Training loss: 2.2986...  0.0568 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1074...  Training loss: 2.3147...  0.0581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1075...  Training loss: 2.2875...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1076...  Training loss: 2.3544...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1077...  Training loss: 2.3124...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1078...  Training loss: 2.3673...  0.0522 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1079...  Training loss: 2.3308...  0.0525 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1080...  Training loss: 2.3312...  0.0545 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1081...  Training loss: 2.2996...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1082...  Training loss: 2.3429...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1083...  Training loss: 2.3099...  0.0552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1084...  Training loss: 2.3242...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1085...  Training loss: 2.3521...  0.0576 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1086...  Training loss: 2.3321...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1087...  Training loss: 2.3187...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1088...  Training loss: 2.3752...  0.0573 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1089...  Training loss: 2.4386...  0.0525 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1090...  Training loss: 2.4109...  0.0547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1091...  Training loss: 2.3991...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1092...  Training loss: 2.3545...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1093...  Training loss: 2.3541...  0.0591 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1094...  Training loss: 2.3397...  0.0527 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1095...  Training loss: 2.4067...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1096...  Training loss: 2.4074...  0.0576 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1097...  Training loss: 2.3644...  0.0536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1098...  Training loss: 2.4216...  0.0520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1099...  Training loss: 2.3200...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1100...  Training loss: 2.3851...  0.0590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1101...  Training loss: 2.3663...  0.0590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1102...  Training loss: 2.3381...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1103...  Training loss: 2.3548...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1104...  Training loss: 2.3704...  0.0609 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 1105...  Training loss: 2.3946...  0.0562 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1106...  Training loss: 2.3364...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1107...  Training loss: 2.3851...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1108...  Training loss: 2.3523...  0.0562 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1109...  Training loss: 2.3721...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1110...  Training loss: 2.3509...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1111...  Training loss: 2.3638...  0.0576 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1112...  Training loss: 2.3992...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1113...  Training loss: 2.3431...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1114...  Training loss: 2.3694...  0.0547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1115...  Training loss: 2.3637...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1116...  Training loss: 2.3633...  0.0552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1117...  Training loss: 2.3230...  0.0540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1118...  Training loss: 2.3479...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1119...  Training loss: 2.3183...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1120...  Training loss: 2.3394...  0.0546 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1121...  Training loss: 2.3342...  0.0588 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1122...  Training loss: 2.3795...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1123...  Training loss: 2.4095...  0.0552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1124...  Training loss: 2.3402...  0.0572 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1125...  Training loss: 2.3415...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1126...  Training loss: 2.3050...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1127...  Training loss: 2.3524...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1128...  Training loss: 2.3259...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1129...  Training loss: 2.3413...  0.0581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1130...  Training loss: 2.3297...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1131...  Training loss: 2.3391...  0.0594 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1132...  Training loss: 2.2946...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1133...  Training loss: 2.4094...  0.0582 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1134...  Training loss: 2.3695...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1135...  Training loss: 2.3565...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1136...  Training loss: 2.3463...  0.0589 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1137...  Training loss: 2.3092...  0.0592 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1138...  Training loss: 2.3321...  0.0546 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1139...  Training loss: 2.3155...  0.0593 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1140...  Training loss: 2.3176...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1141...  Training loss: 2.3159...  0.0536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1142...  Training loss: 2.2937...  0.0548 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1143...  Training loss: 2.3518...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1144...  Training loss: 2.2346...  0.0543 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1145...  Training loss: 2.3066...  0.0585 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1146...  Training loss: 2.3190...  0.0583 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1147...  Training loss: 2.3295...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1148...  Training loss: 2.3375...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1149...  Training loss: 2.3026...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1150...  Training loss: 2.2956...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1151...  Training loss: 2.3350...  0.0568 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1152...  Training loss: 2.2933...  0.0628 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1153...  Training loss: 2.3200...  0.0611 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1154...  Training loss: 2.2943...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1155...  Training loss: 2.3196...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1156...  Training loss: 2.3243...  0.0552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1157...  Training loss: 2.2752...  0.0561 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1158...  Training loss: 2.3725...  0.0527 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1159...  Training loss: 2.2747...  0.0522 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1160...  Training loss: 2.3624...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1161...  Training loss: 2.3375...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1162...  Training loss: 2.3315...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1163...  Training loss: 2.2990...  0.0534 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1164...  Training loss: 2.2868...  0.0535 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1165...  Training loss: 2.3241...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1166...  Training loss: 2.3230...  0.0570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1167...  Training loss: 2.3150...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1168...  Training loss: 2.3660...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1169...  Training loss: 2.3624...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1170...  Training loss: 2.3457...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1171...  Training loss: 2.2904...  0.0552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1172...  Training loss: 2.3119...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1173...  Training loss: 2.3158...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1174...  Training loss: 2.3438...  0.0527 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1175...  Training loss: 2.3188...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1176...  Training loss: 2.2842...  0.0600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1177...  Training loss: 2.3102...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1178...  Training loss: 2.3142...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1179...  Training loss: 2.3219...  0.0527 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1180...  Training loss: 2.3119...  0.0527 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1181...  Training loss: 2.3118...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1182...  Training loss: 2.3294...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1183...  Training loss: 2.3240...  0.0600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1184...  Training loss: 2.3491...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1185...  Training loss: 2.3932...  0.0543 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1186...  Training loss: 2.3264...  0.0595 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1187...  Training loss: 2.3432...  0.0551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1188...  Training loss: 2.3658...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1189...  Training loss: 2.3453...  0.0566 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1190...  Training loss: 2.3235...  0.0534 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1191...  Training loss: 2.3099...  0.0524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1192...  Training loss: 2.3619...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1193...  Training loss: 2.3101...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1194...  Training loss: 2.3003...  0.0535 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1195...  Training loss: 2.3084...  0.0539 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1196...  Training loss: 2.3847...  0.0529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1197...  Training loss: 2.3116...  0.0592 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1198...  Training loss: 2.3297...  0.0523 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1199...  Training loss: 2.3681...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1200...  Training loss: 2.3573...  0.0605 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1201...  Training loss: 2.3372...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1202...  Training loss: 2.3089...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1203...  Training loss: 2.3087...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1204...  Training loss: 2.3519...  0.0526 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 1205...  Training loss: 2.3165...  0.0585 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1206...  Training loss: 2.3093...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1207...  Training loss: 2.2724...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1208...  Training loss: 2.3202...  0.0577 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1209...  Training loss: 2.2639...  0.0555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1210...  Training loss: 2.3686...  0.0567 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1211...  Training loss: 2.2592...  0.0578 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1212...  Training loss: 2.3463...  0.0527 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1213...  Training loss: 2.2960...  0.0602 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1214...  Training loss: 2.2749...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1215...  Training loss: 2.2871...  0.0547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1216...  Training loss: 2.2868...  0.0574 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1217...  Training loss: 2.2741...  0.0564 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1218...  Training loss: 2.3019...  0.0588 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1219...  Training loss: 2.3214...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1220...  Training loss: 2.3424...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1221...  Training loss: 2.3057...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1222...  Training loss: 2.3221...  0.0546 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1223...  Training loss: 2.3384...  0.0531 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1224...  Training loss: 2.2764...  0.0584 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1225...  Training loss: 2.3014...  0.0542 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1226...  Training loss: 2.2745...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1227...  Training loss: 2.2734...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1228...  Training loss: 2.2975...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1229...  Training loss: 2.2791...  0.0548 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1230...  Training loss: 2.3138...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1231...  Training loss: 2.3050...  0.0566 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1232...  Training loss: 2.3315...  0.0574 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1233...  Training loss: 2.2871...  0.0567 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1234...  Training loss: 2.3074...  0.0548 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1235...  Training loss: 2.2786...  0.0567 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1236...  Training loss: 2.3353...  0.0528 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1237...  Training loss: 2.3355...  0.0523 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1238...  Training loss: 2.2729...  0.0556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1239...  Training loss: 2.2705...  0.0568 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1240...  Training loss: 2.2856...  0.0526 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1241...  Training loss: 2.4156...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1242...  Training loss: 2.3488...  0.0532 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1243...  Training loss: 2.3251...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1244...  Training loss: 2.2695...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1245...  Training loss: 2.3100...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1246...  Training loss: 2.2951...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1247...  Training loss: 2.2684...  0.0567 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1248...  Training loss: 2.2439...  0.0581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1249...  Training loss: 2.2317...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1250...  Training loss: 2.2688...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1251...  Training loss: 2.2693...  0.0521 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1252...  Training loss: 2.2501...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1253...  Training loss: 2.3325...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1254...  Training loss: 2.2616...  0.0548 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1255...  Training loss: 2.2842...  0.0588 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1256...  Training loss: 2.3128...  0.0526 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1257...  Training loss: 2.3173...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1258...  Training loss: 2.3029...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1259...  Training loss: 2.2473...  0.0552 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1260...  Training loss: 2.2986...  0.0525 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1261...  Training loss: 2.3168...  0.0550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1262...  Training loss: 2.2645...  0.0547 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1263...  Training loss: 2.2631...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1264...  Training loss: 2.2956...  0.0549 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1265...  Training loss: 2.2724...  0.0532 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1266...  Training loss: 2.2889...  0.0517 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1267...  Training loss: 2.2860...  0.0587 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1268...  Training loss: 2.2779...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1269...  Training loss: 2.3146...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1270...  Training loss: 2.2422...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1271...  Training loss: 2.2442...  0.0526 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1272...  Training loss: 2.2843...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1273...  Training loss: 2.2600...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1274...  Training loss: 2.2710...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1275...  Training loss: 2.2571...  0.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1276...  Training loss: 2.2722...  0.0550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1277...  Training loss: 2.2561...  0.0595 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1278...  Training loss: 2.2694...  0.0524 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1279...  Training loss: 2.2683...  0.0552 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1280...  Training loss: 2.2607...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1281...  Training loss: 2.2523...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1282...  Training loss: 2.2762...  0.0559 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1283...  Training loss: 2.2254...  0.0544 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1284...  Training loss: 2.2918...  0.0550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1285...  Training loss: 2.2650...  0.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1286...  Training loss: 2.2498...  0.0577 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1287...  Training loss: 2.1994...  0.0525 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1288...  Training loss: 2.2869...  0.0552 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1289...  Training loss: 2.2922...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1290...  Training loss: 2.2742...  0.0525 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1291...  Training loss: 2.2172...  0.0584 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1292...  Training loss: 2.2728...  0.0594 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1293...  Training loss: 2.2778...  0.0578 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1294...  Training loss: 2.3240...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1295...  Training loss: 2.3079...  0.0586 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1296...  Training loss: 2.2520...  0.0525 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1297...  Training loss: 2.2583...  0.0563 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1298...  Training loss: 2.2869...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1299...  Training loss: 2.2542...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1300...  Training loss: 2.2960...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1301...  Training loss: 2.2876...  0.0536 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1302...  Training loss: 2.2527...  0.0525 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1303...  Training loss: 2.2966...  0.0589 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1304...  Training loss: 2.2577...  0.0531 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 1305...  Training loss: 2.2351...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1306...  Training loss: 2.2244...  0.0526 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1307...  Training loss: 2.2176...  0.0573 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1308...  Training loss: 2.2162...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1309...  Training loss: 2.2811...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1310...  Training loss: 2.2348...  0.0548 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1311...  Training loss: 2.2503...  0.0558 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1312...  Training loss: 2.2646...  0.0527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1313...  Training loss: 2.2595...  0.0524 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1314...  Training loss: 2.2503...  0.0543 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1315...  Training loss: 2.2836...  0.0555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1316...  Training loss: 2.2602...  0.0555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1317...  Training loss: 2.3237...  0.0571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1318...  Training loss: 2.2421...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1319...  Training loss: 2.2867...  0.0589 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1320...  Training loss: 2.2860...  0.0555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1321...  Training loss: 2.2547...  0.0603 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1322...  Training loss: 2.2258...  0.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1323...  Training loss: 2.2375...  0.0549 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1324...  Training loss: 2.2733...  0.0525 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1325...  Training loss: 2.2687...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1326...  Training loss: 2.2654...  0.0527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1327...  Training loss: 2.2558...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1328...  Training loss: 2.3205...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1329...  Training loss: 2.2709...  0.0585 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1330...  Training loss: 2.2919...  0.0527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1331...  Training loss: 2.2386...  0.0581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1332...  Training loss: 2.3087...  0.0524 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1333...  Training loss: 2.2740...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1334...  Training loss: 2.2553...  0.0549 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1335...  Training loss: 2.2816...  0.0551 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1336...  Training loss: 2.2360...  0.0593 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1337...  Training loss: 2.2796...  0.0576 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1338...  Training loss: 2.2220...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1339...  Training loss: 2.3161...  0.0559 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1340...  Training loss: 2.2385...  0.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1341...  Training loss: 2.2502...  0.0536 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1342...  Training loss: 2.2688...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1343...  Training loss: 2.2827...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1344...  Training loss: 2.2693...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1345...  Training loss: 2.2577...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1346...  Training loss: 2.2419...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1347...  Training loss: 2.2678...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1348...  Training loss: 2.2691...  0.0542 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1349...  Training loss: 2.2180...  0.0558 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1350...  Training loss: 2.2473...  0.0548 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1351...  Training loss: 2.2277...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1352...  Training loss: 2.2668...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1353...  Training loss: 2.2436...  0.0540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1354...  Training loss: 2.2562...  0.0577 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1355...  Training loss: 2.2755...  0.0595 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1356...  Training loss: 2.2665...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1357...  Training loss: 2.2390...  0.0613 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1358...  Training loss: 2.2891...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1359...  Training loss: 2.2340...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1360...  Training loss: 2.2256...  0.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1361...  Training loss: 2.2395...  0.0575 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1362...  Training loss: 2.2058...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1363...  Training loss: 2.2371...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1364...  Training loss: 2.2546...  0.0579 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1365...  Training loss: 2.2692...  0.0589 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1366...  Training loss: 2.3087...  0.0527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1367...  Training loss: 2.2756...  0.0538 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1368...  Training loss: 2.2204...  0.0581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1369...  Training loss: 2.1978...  0.0609 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1370...  Training loss: 2.2910...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1371...  Training loss: 2.2052...  0.0550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1372...  Training loss: 2.2905...  0.0590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1373...  Training loss: 2.2903...  0.0571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1374...  Training loss: 2.2450...  0.0613 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1375...  Training loss: 2.1952...  0.0577 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1376...  Training loss: 2.2498...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1377...  Training loss: 2.2326...  0.0566 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1378...  Training loss: 2.2524...  0.0581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1379...  Training loss: 2.2963...  0.0539 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1380...  Training loss: 2.2273...  0.0525 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1381...  Training loss: 2.2590...  0.0580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1382...  Training loss: 2.2105...  0.0599 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1383...  Training loss: 2.2589...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1384...  Training loss: 2.2371...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1385...  Training loss: 2.2179...  0.0563 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1386...  Training loss: 2.2718...  0.0540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1387...  Training loss: 2.2383...  0.0527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1388...  Training loss: 2.2908...  0.0555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1389...  Training loss: 2.2184...  0.0547 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1390...  Training loss: 2.2519...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1391...  Training loss: 2.2546...  0.0525 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1392...  Training loss: 2.2538...  0.0611 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1393...  Training loss: 2.2286...  0.0576 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1394...  Training loss: 2.2791...  0.0549 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1395...  Training loss: 2.2206...  0.0563 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1396...  Training loss: 2.2450...  0.0594 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1397...  Training loss: 2.2084...  0.0573 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1398...  Training loss: 2.2831...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1399...  Training loss: 2.2914...  0.0547 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1400...  Training loss: 2.2123...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1401...  Training loss: 2.2218...  0.0605 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1402...  Training loss: 2.2368...  0.0576 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1403...  Training loss: 2.2252...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1404...  Training loss: 2.2347...  0.0556 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 1405...  Training loss: 2.2481...  0.0589 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1406...  Training loss: 2.1857...  0.0591 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1407...  Training loss: 2.2710...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1408...  Training loss: 2.2268...  0.0521 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1409...  Training loss: 2.2238...  0.0541 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1410...  Training loss: 2.2366...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1411...  Training loss: 2.2212...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1412...  Training loss: 2.2367...  0.0568 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1413...  Training loss: 2.2591...  0.0569 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1414...  Training loss: 2.2565...  0.0568 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1415...  Training loss: 2.2317...  0.0580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1416...  Training loss: 2.2431...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1417...  Training loss: 2.2535...  0.0577 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1418...  Training loss: 2.2764...  0.0600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1419...  Training loss: 2.2140...  0.0607 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1420...  Training loss: 2.2345...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1421...  Training loss: 2.2496...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1422...  Training loss: 2.2449...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1423...  Training loss: 2.2549...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1424...  Training loss: 2.1777...  0.0583 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1425...  Training loss: 2.2205...  0.0579 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1426...  Training loss: 2.2443...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1427...  Training loss: 2.2014...  0.0559 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1428...  Training loss: 2.2137...  0.0532 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1429...  Training loss: 2.2462...  0.0581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1430...  Training loss: 2.2918...  0.0538 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1431...  Training loss: 2.3061...  0.0558 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1432...  Training loss: 2.3087...  0.0549 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1433...  Training loss: 2.2760...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1434...  Training loss: 2.2072...  0.0586 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1435...  Training loss: 2.2451...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1436...  Training loss: 2.3069...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1437...  Training loss: 2.2612...  0.0608 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1438...  Training loss: 2.2709...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1439...  Training loss: 2.2151...  0.0597 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1440...  Training loss: 2.2332...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1441...  Training loss: 2.2448...  0.0592 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1442...  Training loss: 2.2300...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1443...  Training loss: 2.2095...  0.0549 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1444...  Training loss: 2.2310...  0.0539 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1445...  Training loss: 2.2340...  0.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1446...  Training loss: 2.2019...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1447...  Training loss: 2.2537...  0.0539 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1448...  Training loss: 2.2236...  0.0559 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1449...  Training loss: 2.2349...  0.0547 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1450...  Training loss: 2.2300...  0.0593 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1451...  Training loss: 2.2347...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1452...  Training loss: 2.2441...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1453...  Training loss: 2.2585...  0.0618 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1454...  Training loss: 2.2788...  0.0596 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1455...  Training loss: 2.2483...  0.0573 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1456...  Training loss: 2.2307...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1457...  Training loss: 2.2234...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1458...  Training loss: 2.2054...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1459...  Training loss: 2.2990...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1460...  Training loss: 2.2486...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1461...  Training loss: 2.2215...  0.0544 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1462...  Training loss: 2.2396...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1463...  Training loss: 2.2787...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1464...  Training loss: 2.2056...  0.0582 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1465...  Training loss: 2.2149...  0.0567 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1466...  Training loss: 2.2663...  0.0551 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1467...  Training loss: 2.2709...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1468...  Training loss: 2.1901...  0.0552 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1469...  Training loss: 2.2623...  0.0540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1470...  Training loss: 2.2343...  0.0558 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1471...  Training loss: 2.2710...  0.0580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1472...  Training loss: 2.2553...  0.0567 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1473...  Training loss: 2.2154...  0.0586 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1474...  Training loss: 2.2178...  0.0586 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1475...  Training loss: 2.2248...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1476...  Training loss: 2.2589...  0.0552 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1477...  Training loss: 2.2462...  0.0563 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1478...  Training loss: 2.1814...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1479...  Training loss: 2.1995...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1480...  Training loss: 2.2606...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1481...  Training loss: 2.2486...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1482...  Training loss: 2.2334...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1483...  Training loss: 2.2140...  0.0552 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1484...  Training loss: 2.2200...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1485...  Training loss: 2.2101...  0.0544 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1486...  Training loss: 2.2270...  0.0550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1487...  Training loss: 2.2427...  0.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1488...  Training loss: 2.2196...  0.0558 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1489...  Training loss: 2.1902...  0.0580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1490...  Training loss: 2.2236...  0.0591 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1491...  Training loss: 2.2116...  0.0582 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1492...  Training loss: 2.1713...  0.0527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1493...  Training loss: 2.2247...  0.0573 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1494...  Training loss: 2.2370...  0.0555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1495...  Training loss: 2.2583...  0.0545 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1496...  Training loss: 2.2285...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1497...  Training loss: 2.2202...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1498...  Training loss: 2.2304...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1499...  Training loss: 2.2393...  0.0524 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1500...  Training loss: 2.1985...  0.0589 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1501...  Training loss: 2.2166...  0.0563 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1502...  Training loss: 2.2175...  0.0587 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1503...  Training loss: 2.2475...  0.0583 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1504...  Training loss: 2.2481...  0.0553 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 1505...  Training loss: 2.2477...  0.0585 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1506...  Training loss: 2.1696...  0.0538 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1507...  Training loss: 2.2490...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1508...  Training loss: 2.2323...  0.0591 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1509...  Training loss: 2.2201...  0.0619 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1510...  Training loss: 2.2051...  0.0536 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1511...  Training loss: 2.2115...  0.0551 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1512...  Training loss: 2.2274...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1513...  Training loss: 2.2287...  0.0610 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1514...  Training loss: 2.1949...  0.0576 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1515...  Training loss: 2.2181...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1516...  Training loss: 2.2936...  0.0532 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1517...  Training loss: 2.2750...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1518...  Training loss: 2.2138...  0.0550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1519...  Training loss: 2.2441...  0.0548 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1520...  Training loss: 2.2830...  0.0571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1521...  Training loss: 2.2276...  0.0547 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1522...  Training loss: 2.1777...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1523...  Training loss: 2.2126...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1524...  Training loss: 2.2264...  0.0536 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1525...  Training loss: 2.1760...  0.0626 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1526...  Training loss: 2.2166...  0.0600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1527...  Training loss: 2.1940...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1528...  Training loss: 2.2080...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1529...  Training loss: 2.2348...  0.0542 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1530...  Training loss: 2.2381...  0.0532 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1531...  Training loss: 2.2152...  0.0593 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1532...  Training loss: 2.2220...  0.0559 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1533...  Training loss: 2.2322...  0.0563 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1534...  Training loss: 2.2158...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1535...  Training loss: 2.1903...  0.0542 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1536...  Training loss: 2.2036...  0.0615 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1537...  Training loss: 2.2409...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1538...  Training loss: 2.2306...  0.0574 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1539...  Training loss: 2.2617...  0.0541 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1540...  Training loss: 2.2125...  0.0559 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1541...  Training loss: 2.1955...  0.0590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1542...  Training loss: 2.2650...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1543...  Training loss: 2.1885...  0.0639 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1544...  Training loss: 2.2109...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1545...  Training loss: 2.2187...  0.0608 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1546...  Training loss: 2.2302...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1547...  Training loss: 2.2060...  0.0576 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1548...  Training loss: 2.1731...  0.0579 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1549...  Training loss: 2.2383...  0.0539 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1550...  Training loss: 2.1788...  0.0536 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1551...  Training loss: 2.1921...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1552...  Training loss: 2.2309...  0.0573 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1553...  Training loss: 2.2161...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1554...  Training loss: 2.2006...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1555...  Training loss: 2.2164...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1556...  Training loss: 2.2257...  0.0555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1557...  Training loss: 2.1743...  0.0594 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1558...  Training loss: 2.1847...  0.0532 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1559...  Training loss: 2.2180...  0.0527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1560...  Training loss: 2.2660...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1561...  Training loss: 2.2163...  0.0618 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1562...  Training loss: 2.1672...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1563...  Training loss: 2.2095...  0.0571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1564...  Training loss: 2.1977...  0.0565 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1565...  Training loss: 2.1662...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1566...  Training loss: 2.1894...  0.0568 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1567...  Training loss: 2.1613...  0.0558 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1568...  Training loss: 2.1581...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1569...  Training loss: 2.2261...  0.0592 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1570...  Training loss: 2.1717...  0.0555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1571...  Training loss: 2.2230...  0.0634 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1572...  Training loss: 2.1703...  0.0550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1573...  Training loss: 2.1652...  0.0546 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1574...  Training loss: 2.1866...  0.0549 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1575...  Training loss: 2.2103...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1576...  Training loss: 2.2105...  0.0559 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1577...  Training loss: 2.1961...  0.0566 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1578...  Training loss: 2.1797...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1579...  Training loss: 2.1784...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1580...  Training loss: 2.2180...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1581...  Training loss: 2.1991...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1582...  Training loss: 2.2245...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1583...  Training loss: 2.1961...  0.0558 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1584...  Training loss: 2.1903...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1585...  Training loss: 2.1950...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1586...  Training loss: 2.2268...  0.0532 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1587...  Training loss: 2.2167...  0.0524 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1588...  Training loss: 2.1801...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1589...  Training loss: 2.1750...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1590...  Training loss: 2.1508...  0.0588 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1591...  Training loss: 2.1979...  0.0538 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1592...  Training loss: 2.1967...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1593...  Training loss: 2.2220...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1594...  Training loss: 2.1694...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1595...  Training loss: 2.2104...  0.0570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1596...  Training loss: 2.2566...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1597...  Training loss: 2.2950...  0.0622 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1598...  Training loss: 2.2520...  0.0589 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1599...  Training loss: 2.1977...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1600...  Training loss: 2.2200...  0.0566 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1601...  Training loss: 2.2243...  0.0569 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1602...  Training loss: 2.2040...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1603...  Training loss: 2.1484...  0.0538 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1604...  Training loss: 2.1464...  0.0550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 1605...  Training loss: 2.1865...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1606...  Training loss: 2.2115...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1607...  Training loss: 2.1901...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1608...  Training loss: 2.2573...  0.0539 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1609...  Training loss: 2.2002...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1610...  Training loss: 2.1800...  0.0594 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1611...  Training loss: 2.2399...  0.0539 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1612...  Training loss: 2.2596...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1613...  Training loss: 2.2314...  0.0597 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1614...  Training loss: 2.2189...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1615...  Training loss: 2.1705...  0.0540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1616...  Training loss: 2.2313...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1617...  Training loss: 2.1914...  0.0577 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1618...  Training loss: 2.2663...  0.0542 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1619...  Training loss: 2.2201...  0.0589 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1620...  Training loss: 2.2018...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1621...  Training loss: 2.1545...  0.0581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1622...  Training loss: 2.2533...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1623...  Training loss: 2.1591...  0.0558 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1624...  Training loss: 2.2193...  0.0532 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1625...  Training loss: 2.2032...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1626...  Training loss: 2.1614...  0.0571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1627...  Training loss: 2.1713...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1628...  Training loss: 2.2561...  0.0602 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1629...  Training loss: 2.1482...  0.0565 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1630...  Training loss: 2.1705...  0.0582 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1631...  Training loss: 2.2188...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1632...  Training loss: 2.1403...  0.0573 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1633...  Training loss: 2.1861...  0.0538 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1634...  Training loss: 2.1739...  0.0552 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1635...  Training loss: 2.1687...  0.0569 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1636...  Training loss: 2.1846...  0.0571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1637...  Training loss: 2.1940...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1638...  Training loss: 2.1981...  0.0526 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1639...  Training loss: 2.1811...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1640...  Training loss: 2.2058...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1641...  Training loss: 2.2010...  0.0539 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1642...  Training loss: 2.2128...  0.0587 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1643...  Training loss: 2.1889...  0.0525 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1644...  Training loss: 2.2305...  0.0539 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1645...  Training loss: 2.2358...  0.0591 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1646...  Training loss: 2.2349...  0.0565 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1647...  Training loss: 2.2322...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1648...  Training loss: 2.1849...  0.0559 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1649...  Training loss: 2.2129...  0.0538 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1650...  Training loss: 2.2122...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1651...  Training loss: 2.1790...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1652...  Training loss: 2.2186...  0.0532 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1653...  Training loss: 2.2014...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1654...  Training loss: 2.2083...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1655...  Training loss: 2.1382...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1656...  Training loss: 2.1505...  0.0565 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1657...  Training loss: 2.1728...  0.0628 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1658...  Training loss: 2.2027...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1659...  Training loss: 2.2123...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1660...  Training loss: 2.2025...  0.0584 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1661...  Training loss: 2.1624...  0.0536 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1662...  Training loss: 2.1887...  0.0594 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1663...  Training loss: 2.2370...  0.0602 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1664...  Training loss: 2.1973...  0.0624 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1665...  Training loss: 2.2326...  0.0580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1666...  Training loss: 2.2109...  0.0593 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1667...  Training loss: 2.1762...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1668...  Training loss: 2.2049...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1669...  Training loss: 2.1946...  0.0555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1670...  Training loss: 2.1474...  0.0563 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1671...  Training loss: 2.1668...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1672...  Training loss: 2.2135...  0.0522 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1673...  Training loss: 2.1929...  0.0599 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1674...  Training loss: 2.1625...  0.0526 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1675...  Training loss: 2.1882...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1676...  Training loss: 2.1966...  0.0549 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1677...  Training loss: 2.1467...  0.0552 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1678...  Training loss: 2.1968...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1679...  Training loss: 2.2082...  0.0536 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1680...  Training loss: 2.1831...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1681...  Training loss: 2.1758...  0.0544 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1682...  Training loss: 2.1700...  0.0538 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1683...  Training loss: 2.2204...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1684...  Training loss: 2.1827...  0.0532 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1685...  Training loss: 2.1761...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1686...  Training loss: 2.1614...  0.0588 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1687...  Training loss: 2.1704...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1688...  Training loss: 2.1611...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1689...  Training loss: 2.2012...  0.0558 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1690...  Training loss: 2.2354...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1691...  Training loss: 2.2541...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1692...  Training loss: 2.1997...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1693...  Training loss: 2.1742...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1694...  Training loss: 2.1905...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1695...  Training loss: 2.1418...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1696...  Training loss: 2.2032...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1697...  Training loss: 2.1755...  0.0580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1698...  Training loss: 2.2228...  0.0538 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1699...  Training loss: 2.1652...  0.0568 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1700...  Training loss: 2.2016...  0.0611 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1701...  Training loss: 2.1607...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1702...  Training loss: 2.1920...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1703...  Training loss: 2.1713...  0.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1704...  Training loss: 2.1717...  0.0531 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 1705...  Training loss: 2.2131...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1706...  Training loss: 2.1807...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1707...  Training loss: 2.1761...  0.0555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1708...  Training loss: 2.1527...  0.0552 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1709...  Training loss: 2.2178...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1710...  Training loss: 2.1851...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1711...  Training loss: 2.1915...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1712...  Training loss: 2.1530...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1713...  Training loss: 2.1556...  0.0597 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1714...  Training loss: 2.1219...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1715...  Training loss: 2.2280...  0.0579 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1716...  Training loss: 2.2135...  0.0546 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1717...  Training loss: 2.1831...  0.0577 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1718...  Training loss: 2.2422...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1719...  Training loss: 2.1601...  0.0575 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1720...  Training loss: 2.2199...  0.0581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1721...  Training loss: 2.2136...  0.0527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1722...  Training loss: 2.1699...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1723...  Training loss: 2.1864...  0.0586 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1724...  Training loss: 2.1809...  0.0551 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1725...  Training loss: 2.2316...  0.0582 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1726...  Training loss: 2.1713...  0.0573 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1727...  Training loss: 2.2245...  0.0555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1728...  Training loss: 2.1901...  0.0550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1729...  Training loss: 2.1894...  0.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1730...  Training loss: 2.1884...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1731...  Training loss: 2.1800...  0.0587 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1732...  Training loss: 2.2293...  0.0525 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1733...  Training loss: 2.1907...  0.0532 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1734...  Training loss: 2.1914...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1735...  Training loss: 2.1742...  0.0591 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1736...  Training loss: 2.2029...  0.0590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1737...  Training loss: 2.1662...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1738...  Training loss: 2.1879...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1739...  Training loss: 2.1551...  0.0582 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1740...  Training loss: 2.1875...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1741...  Training loss: 2.1885...  0.0582 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1742...  Training loss: 2.2123...  0.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1743...  Training loss: 2.2862...  0.0524 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1744...  Training loss: 2.1881...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1745...  Training loss: 2.1821...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1746...  Training loss: 2.1314...  0.0528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1747...  Training loss: 2.2235...  0.0527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1748...  Training loss: 2.1409...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1749...  Training loss: 2.2085...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1750...  Training loss: 2.1936...  0.0559 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1751...  Training loss: 2.1747...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1752...  Training loss: 2.1658...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1753...  Training loss: 2.2715...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1754...  Training loss: 2.2412...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1755...  Training loss: 2.1936...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1756...  Training loss: 2.1767...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1757...  Training loss: 2.1769...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1758...  Training loss: 2.1607...  0.0546 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1759...  Training loss: 2.1611...  0.0524 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1760...  Training loss: 2.1653...  0.0569 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1761...  Training loss: 2.1764...  0.0548 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1762...  Training loss: 2.1548...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1763...  Training loss: 2.1801...  0.0524 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1764...  Training loss: 2.1068...  0.0550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1765...  Training loss: 2.1551...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1766...  Training loss: 2.2023...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1767...  Training loss: 2.2055...  0.0543 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1768...  Training loss: 2.1868...  0.0548 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1769...  Training loss: 2.1727...  0.0523 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1770...  Training loss: 2.1403...  0.0590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1771...  Training loss: 2.1764...  0.0527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1772...  Training loss: 2.1404...  0.0596 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1773...  Training loss: 2.1827...  0.0567 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1774...  Training loss: 2.1570...  0.0549 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1775...  Training loss: 2.1745...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1776...  Training loss: 2.1707...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1777...  Training loss: 2.1333...  0.0581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1778...  Training loss: 2.2137...  0.0526 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1779...  Training loss: 2.1145...  0.0582 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1780...  Training loss: 2.2018...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1781...  Training loss: 2.1901...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1782...  Training loss: 2.1608...  0.0542 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1783...  Training loss: 2.1390...  0.0536 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1784...  Training loss: 2.1413...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1785...  Training loss: 2.1925...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1786...  Training loss: 2.1582...  0.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1787...  Training loss: 2.1949...  0.0603 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1788...  Training loss: 2.2256...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1789...  Training loss: 2.2297...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1790...  Training loss: 2.1754...  0.0592 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1791...  Training loss: 2.1805...  0.0569 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1792...  Training loss: 2.1661...  0.0597 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1793...  Training loss: 2.1668...  0.0551 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1794...  Training loss: 2.1938...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1795...  Training loss: 2.1823...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1796...  Training loss: 2.1520...  0.0611 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1797...  Training loss: 2.1746...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1798...  Training loss: 2.1624...  0.0533 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1799...  Training loss: 2.1872...  0.0551 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1800...  Training loss: 2.1819...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1801...  Training loss: 2.1912...  0.0619 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1802...  Training loss: 2.2025...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1803...  Training loss: 2.1780...  0.0587 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1804...  Training loss: 2.2129...  0.0572 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 1805...  Training loss: 2.2626...  0.0551 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1806...  Training loss: 2.1983...  0.0536 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1807...  Training loss: 2.1795...  0.0558 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1808...  Training loss: 2.2219...  0.0571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1809...  Training loss: 2.1824...  0.0577 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1810...  Training loss: 2.1767...  0.0575 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1811...  Training loss: 2.1707...  0.0616 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1812...  Training loss: 2.2412...  0.0596 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1813...  Training loss: 2.1619...  0.0577 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1814...  Training loss: 2.1717...  0.0586 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1815...  Training loss: 2.1734...  0.0568 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1816...  Training loss: 2.2447...  0.0577 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1817...  Training loss: 2.1319...  0.0573 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1818...  Training loss: 2.1774...  0.0634 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1819...  Training loss: 2.2320...  0.0568 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1820...  Training loss: 2.2124...  0.0552 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1821...  Training loss: 2.1882...  0.0632 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1822...  Training loss: 2.1684...  0.0568 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1823...  Training loss: 2.1670...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1824...  Training loss: 2.2163...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1825...  Training loss: 2.1698...  0.0604 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1826...  Training loss: 2.1684...  0.0591 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1827...  Training loss: 2.1359...  0.0541 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1828...  Training loss: 2.1818...  0.0601 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1829...  Training loss: 2.1179...  0.0571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1830...  Training loss: 2.2134...  0.0568 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1831...  Training loss: 2.1341...  0.0591 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1832...  Training loss: 2.2072...  0.0567 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1833...  Training loss: 2.1604...  0.0536 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1834...  Training loss: 2.1319...  0.0588 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1835...  Training loss: 2.1551...  0.0620 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1836...  Training loss: 2.1446...  0.0566 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1837...  Training loss: 2.1244...  0.0572 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1838...  Training loss: 2.1468...  0.0573 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1839...  Training loss: 2.1881...  0.0557 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1840...  Training loss: 2.1904...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1841...  Training loss: 2.1571...  0.0624 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1842...  Training loss: 2.1888...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1843...  Training loss: 2.1900...  0.0561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1844...  Training loss: 2.1273...  0.0579 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1845...  Training loss: 2.1486...  0.0553 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1846...  Training loss: 2.1553...  0.0551 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1847...  Training loss: 2.1043...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1848...  Training loss: 2.1331...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1849...  Training loss: 2.1265...  0.0539 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1850...  Training loss: 2.1919...  0.0564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1851...  Training loss: 2.1928...  0.0591 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1852...  Training loss: 2.1933...  0.0604 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1853...  Training loss: 2.1303...  0.0578 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1854...  Training loss: 2.1655...  0.0606 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1855...  Training loss: 2.1212...  0.0579 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1856...  Training loss: 2.1847...  0.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1857...  Training loss: 2.2049...  0.0562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1858...  Training loss: 2.1468...  0.0583 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1859...  Training loss: 2.1227...  0.0571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 1860...  Training loss: 2.1390...  0.0537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1861...  Training loss: 2.2332...  0.0575 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1862...  Training loss: 2.2261...  0.0564 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1863...  Training loss: 2.1967...  0.0565 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1864...  Training loss: 2.1299...  0.0600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1865...  Training loss: 2.1605...  0.0572 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1866...  Training loss: 2.1574...  0.0536 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1867...  Training loss: 2.1523...  0.0540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1868...  Training loss: 2.1222...  0.0586 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1869...  Training loss: 2.1086...  0.0580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1870...  Training loss: 2.1450...  0.0571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1871...  Training loss: 2.1536...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1872...  Training loss: 2.1280...  0.0546 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1873...  Training loss: 2.1794...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1874...  Training loss: 2.1242...  0.0571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1875...  Training loss: 2.1673...  0.0567 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1876...  Training loss: 2.2010...  0.0601 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1877...  Training loss: 2.1754...  0.0568 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1878...  Training loss: 2.1843...  0.0564 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1879...  Training loss: 2.1267...  0.0545 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1880...  Training loss: 2.1560...  0.0572 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1881...  Training loss: 2.1960...  0.0564 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1882...  Training loss: 2.1417...  0.0573 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1883...  Training loss: 2.1217...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1884...  Training loss: 2.1599...  0.0539 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1885...  Training loss: 2.1603...  0.0616 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1886...  Training loss: 2.1389...  0.0557 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1887...  Training loss: 2.1555...  0.0604 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1888...  Training loss: 2.1438...  0.0587 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1889...  Training loss: 2.1800...  0.0565 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1890...  Training loss: 2.0974...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1891...  Training loss: 2.1371...  0.0550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1892...  Training loss: 2.1687...  0.0550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1893...  Training loss: 2.1398...  0.0571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1894...  Training loss: 2.1341...  0.0585 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1895...  Training loss: 2.1562...  0.0566 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1896...  Training loss: 2.1553...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1897...  Training loss: 2.1491...  0.0603 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1898...  Training loss: 2.1276...  0.0575 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1899...  Training loss: 2.1537...  0.0591 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1900...  Training loss: 2.1317...  0.0570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1901...  Training loss: 2.1309...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1902...  Training loss: 2.1476...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1903...  Training loss: 2.1387...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1904...  Training loss: 2.1715...  0.0542 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 1905...  Training loss: 2.1487...  0.0600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1906...  Training loss: 2.1255...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1907...  Training loss: 2.0526...  0.0535 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1908...  Training loss: 2.1674...  0.0598 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1909...  Training loss: 2.1525...  0.0569 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1910...  Training loss: 2.1548...  0.0562 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1911...  Training loss: 2.1114...  0.0578 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1912...  Training loss: 2.1231...  0.0547 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1913...  Training loss: 2.1397...  0.0568 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1914...  Training loss: 2.2108...  0.0572 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1915...  Training loss: 2.1870...  0.0569 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1916...  Training loss: 2.1301...  0.0561 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1917...  Training loss: 2.1113...  0.0533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1918...  Training loss: 2.1553...  0.0614 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1919...  Training loss: 2.1353...  0.0538 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1920...  Training loss: 2.1877...  0.0601 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1921...  Training loss: 2.1528...  0.0552 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1922...  Training loss: 2.1492...  0.0566 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1923...  Training loss: 2.1717...  0.0589 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1924...  Training loss: 2.1222...  0.0545 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1925...  Training loss: 2.1123...  0.0575 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1926...  Training loss: 2.0876...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1927...  Training loss: 2.1051...  0.0536 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1928...  Training loss: 2.0963...  0.0535 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1929...  Training loss: 2.1640...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1930...  Training loss: 2.1492...  0.0578 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1931...  Training loss: 2.1583...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1932...  Training loss: 2.1572...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1933...  Training loss: 2.1155...  0.0575 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1934...  Training loss: 2.1492...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1935...  Training loss: 2.1862...  0.0524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1936...  Training loss: 2.1495...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1937...  Training loss: 2.1717...  0.0524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1938...  Training loss: 2.1449...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1939...  Training loss: 2.1461...  0.0571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1940...  Training loss: 2.1650...  0.0614 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1941...  Training loss: 2.1293...  0.0607 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1942...  Training loss: 2.1191...  0.0566 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1943...  Training loss: 2.1079...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1944...  Training loss: 2.1213...  0.0602 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1945...  Training loss: 2.1404...  0.0563 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1946...  Training loss: 2.1610...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1947...  Training loss: 2.1098...  0.0557 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1948...  Training loss: 2.1858...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1949...  Training loss: 2.1357...  0.0543 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1950...  Training loss: 2.1367...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1951...  Training loss: 2.1245...  0.0522 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1952...  Training loss: 2.1921...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1953...  Training loss: 2.1444...  0.0525 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1954...  Training loss: 2.1344...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1955...  Training loss: 2.1631...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1956...  Training loss: 2.1365...  0.0532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1957...  Training loss: 2.1682...  0.0561 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1958...  Training loss: 2.0951...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1959...  Training loss: 2.1779...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1960...  Training loss: 2.1378...  0.0561 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1961...  Training loss: 2.1426...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1962...  Training loss: 2.1267...  0.0524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1963...  Training loss: 2.1661...  0.0590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1964...  Training loss: 2.1692...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1965...  Training loss: 2.1363...  0.0542 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1966...  Training loss: 2.1119...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1967...  Training loss: 2.1534...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1968...  Training loss: 2.1522...  0.0524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1969...  Training loss: 2.1141...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1970...  Training loss: 2.1138...  0.0586 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1971...  Training loss: 2.1029...  0.0576 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1972...  Training loss: 2.1352...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1973...  Training loss: 2.1374...  0.0559 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1974...  Training loss: 2.1460...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1975...  Training loss: 2.1603...  0.0594 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1976...  Training loss: 2.1651...  0.0585 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1977...  Training loss: 2.1059...  0.0561 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1978...  Training loss: 2.1485...  0.0559 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1979...  Training loss: 2.1119...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1980...  Training loss: 2.1218...  0.0571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1981...  Training loss: 2.1130...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1982...  Training loss: 2.0822...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1983...  Training loss: 2.1322...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1984...  Training loss: 2.1460...  0.0548 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1985...  Training loss: 2.1416...  0.0566 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1986...  Training loss: 2.1680...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1987...  Training loss: 2.1593...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1988...  Training loss: 2.0875...  0.0590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1989...  Training loss: 2.1007...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1990...  Training loss: 2.1753...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1991...  Training loss: 2.1082...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1992...  Training loss: 2.1990...  0.0525 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1993...  Training loss: 2.1776...  0.0589 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1994...  Training loss: 2.1500...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1995...  Training loss: 2.1064...  0.0547 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1996...  Training loss: 2.1306...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1997...  Training loss: 2.1276...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1998...  Training loss: 2.1209...  0.0550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 1999...  Training loss: 2.1772...  0.0533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2000...  Training loss: 2.1166...  0.0574 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2001...  Training loss: 2.1475...  0.0576 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2002...  Training loss: 2.0796...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2003...  Training loss: 2.1342...  0.0562 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2004...  Training loss: 2.1046...  0.0584 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 2005...  Training loss: 2.0927...  0.0559 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2006...  Training loss: 2.1324...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2007...  Training loss: 2.1199...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2008...  Training loss: 2.1535...  0.0526 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2009...  Training loss: 2.1072...  0.0574 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2010...  Training loss: 2.1246...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2011...  Training loss: 2.1437...  0.0568 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2012...  Training loss: 2.1317...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2013...  Training loss: 2.1252...  0.0549 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2014...  Training loss: 2.1655...  0.0523 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2015...  Training loss: 2.1009...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2016...  Training loss: 2.1371...  0.0525 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2017...  Training loss: 2.1057...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2018...  Training loss: 2.1517...  0.0576 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2019...  Training loss: 2.1617...  0.0544 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2020...  Training loss: 2.1085...  0.0562 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2021...  Training loss: 2.0885...  0.0572 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2022...  Training loss: 2.1011...  0.0542 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2023...  Training loss: 2.1123...  0.0550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2024...  Training loss: 2.1260...  0.0525 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2025...  Training loss: 2.1454...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2026...  Training loss: 2.1025...  0.0573 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2027...  Training loss: 2.1681...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2028...  Training loss: 2.1194...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2029...  Training loss: 2.1148...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2030...  Training loss: 2.1154...  0.0590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2031...  Training loss: 2.1145...  0.0552 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2032...  Training loss: 2.1161...  0.0525 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2033...  Training loss: 2.1166...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2034...  Training loss: 2.1406...  0.0533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2035...  Training loss: 2.1309...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2036...  Training loss: 2.1325...  0.0552 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2037...  Training loss: 2.1427...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2038...  Training loss: 2.1465...  0.0580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2039...  Training loss: 2.0980...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2040...  Training loss: 2.1083...  0.0547 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2041...  Training loss: 2.1139...  0.0606 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2042...  Training loss: 2.1242...  0.0568 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2043...  Training loss: 2.1469...  0.0526 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2044...  Training loss: 2.0572...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2045...  Training loss: 2.0860...  0.0543 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2046...  Training loss: 2.1255...  0.0595 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2047...  Training loss: 2.0998...  0.0539 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2048...  Training loss: 2.1011...  0.0562 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2049...  Training loss: 2.1049...  0.0541 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2050...  Training loss: 2.1817...  0.0563 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2051...  Training loss: 2.1763...  0.0585 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2052...  Training loss: 2.1704...  0.0524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2053...  Training loss: 2.1516...  0.0605 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2054...  Training loss: 2.0921...  0.0535 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2055...  Training loss: 2.1381...  0.0537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2056...  Training loss: 2.2018...  0.0525 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2057...  Training loss: 2.1441...  0.0583 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2058...  Training loss: 2.1744...  0.0610 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2059...  Training loss: 2.0958...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2060...  Training loss: 2.1045...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2061...  Training loss: 2.1123...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2062...  Training loss: 2.1244...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2063...  Training loss: 2.0920...  0.0570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2064...  Training loss: 2.1002...  0.0591 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2065...  Training loss: 2.1282...  0.0594 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2066...  Training loss: 2.0784...  0.0548 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2067...  Training loss: 2.1373...  0.0537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2068...  Training loss: 2.1384...  0.0562 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2069...  Training loss: 2.1178...  0.0533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2070...  Training loss: 2.0999...  0.0573 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2071...  Training loss: 2.1342...  0.0552 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2072...  Training loss: 2.1444...  0.0576 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2073...  Training loss: 2.1502...  0.0557 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2074...  Training loss: 2.1553...  0.0583 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2075...  Training loss: 2.1269...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2076...  Training loss: 2.1381...  0.0600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2077...  Training loss: 2.1248...  0.0548 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2078...  Training loss: 2.0952...  0.0548 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2079...  Training loss: 2.2111...  0.0570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2080...  Training loss: 2.1359...  0.0587 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2081...  Training loss: 2.1180...  0.0578 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2082...  Training loss: 2.1401...  0.0573 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2083...  Training loss: 2.1615...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2084...  Training loss: 2.1131...  0.0526 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2085...  Training loss: 2.1089...  0.0532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2086...  Training loss: 2.1651...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2087...  Training loss: 2.1763...  0.0577 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2088...  Training loss: 2.0753...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2089...  Training loss: 2.1482...  0.0550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2090...  Training loss: 2.1143...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2091...  Training loss: 2.1510...  0.0550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2092...  Training loss: 2.1206...  0.0590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2093...  Training loss: 2.1046...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2094...  Training loss: 2.0952...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2095...  Training loss: 2.1068...  0.0573 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2096...  Training loss: 2.1369...  0.0538 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2097...  Training loss: 2.1220...  0.0526 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2098...  Training loss: 2.0748...  0.0550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2099...  Training loss: 2.0925...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2100...  Training loss: 2.1461...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2101...  Training loss: 2.1244...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2102...  Training loss: 2.1016...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2103...  Training loss: 2.0969...  0.0580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2104...  Training loss: 2.1257...  0.0555 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 2105...  Training loss: 2.0944...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2106...  Training loss: 2.1404...  0.0589 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2107...  Training loss: 2.0982...  0.0550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2108...  Training loss: 2.1115...  0.0533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2109...  Training loss: 2.0715...  0.0524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2110...  Training loss: 2.1187...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2111...  Training loss: 2.1075...  0.0540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2112...  Training loss: 2.0683...  0.0541 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2113...  Training loss: 2.1309...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2114...  Training loss: 2.1293...  0.0586 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2115...  Training loss: 2.1459...  0.0526 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2116...  Training loss: 2.1131...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2117...  Training loss: 2.1215...  0.0544 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2118...  Training loss: 2.1279...  0.0608 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2119...  Training loss: 2.1164...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2120...  Training loss: 2.1032...  0.0549 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2121...  Training loss: 2.1281...  0.0546 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2122...  Training loss: 2.0966...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2123...  Training loss: 2.1368...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2124...  Training loss: 2.1226...  0.0548 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2125...  Training loss: 2.1424...  0.0571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2126...  Training loss: 2.0627...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2127...  Training loss: 2.1261...  0.0592 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2128...  Training loss: 2.1228...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2129...  Training loss: 2.0940...  0.0573 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2130...  Training loss: 2.0797...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2131...  Training loss: 2.0915...  0.0581 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2132...  Training loss: 2.1081...  0.0557 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2133...  Training loss: 2.1294...  0.0536 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2134...  Training loss: 2.0955...  0.0526 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2135...  Training loss: 2.1269...  0.0597 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2136...  Training loss: 2.1460...  0.0535 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2137...  Training loss: 2.1669...  0.0581 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2138...  Training loss: 2.1109...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2139...  Training loss: 2.1473...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2140...  Training loss: 2.1633...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2141...  Training loss: 2.1116...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2142...  Training loss: 2.0661...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2143...  Training loss: 2.0873...  0.0545 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2144...  Training loss: 2.1147...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2145...  Training loss: 2.0816...  0.0548 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2146...  Training loss: 2.1070...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2147...  Training loss: 2.0899...  0.0629 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2148...  Training loss: 2.1072...  0.0549 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2149...  Training loss: 2.1414...  0.0565 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2150...  Training loss: 2.1577...  0.0522 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2151...  Training loss: 2.1091...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2152...  Training loss: 2.1185...  0.0526 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2153...  Training loss: 2.1196...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2154...  Training loss: 2.1129...  0.0576 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2155...  Training loss: 2.0782...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2156...  Training loss: 2.0861...  0.0540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2157...  Training loss: 2.1488...  0.0564 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2158...  Training loss: 2.1265...  0.0588 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2159...  Training loss: 2.1668...  0.0548 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2160...  Training loss: 2.1191...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2161...  Training loss: 2.1172...  0.0559 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2162...  Training loss: 2.1612...  0.0569 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2163...  Training loss: 2.0493...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2164...  Training loss: 2.1239...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2165...  Training loss: 2.1097...  0.0582 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2166...  Training loss: 2.1197...  0.0533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2167...  Training loss: 2.0853...  0.0536 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2168...  Training loss: 2.0829...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2169...  Training loss: 2.1277...  0.0582 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2170...  Training loss: 2.0662...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2171...  Training loss: 2.0910...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2172...  Training loss: 2.1012...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2173...  Training loss: 2.0985...  0.0587 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2174...  Training loss: 2.0701...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2175...  Training loss: 2.1198...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2176...  Training loss: 2.1376...  0.0568 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2177...  Training loss: 2.0660...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2178...  Training loss: 2.0760...  0.0559 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2179...  Training loss: 2.1085...  0.0568 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2180...  Training loss: 2.1499...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2181...  Training loss: 2.0995...  0.0545 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2182...  Training loss: 2.0574...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2183...  Training loss: 2.1178...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2184...  Training loss: 2.0965...  0.0532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2185...  Training loss: 2.0738...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2186...  Training loss: 2.0781...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2187...  Training loss: 2.0556...  0.0548 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2188...  Training loss: 2.0666...  0.0546 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2189...  Training loss: 2.1061...  0.0567 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2190...  Training loss: 2.0787...  0.0547 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2191...  Training loss: 2.1231...  0.0549 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2192...  Training loss: 2.0791...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2193...  Training loss: 2.0609...  0.0569 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2194...  Training loss: 2.0838...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2195...  Training loss: 2.1156...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2196...  Training loss: 2.0956...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2197...  Training loss: 2.0821...  0.0564 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2198...  Training loss: 2.0758...  0.0542 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2199...  Training loss: 2.0700...  0.0576 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2200...  Training loss: 2.1013...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2201...  Training loss: 2.0965...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2202...  Training loss: 2.0916...  0.0591 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2203...  Training loss: 2.0999...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2204...  Training loss: 2.0804...  0.0570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 2205...  Training loss: 2.0872...  0.0559 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2206...  Training loss: 2.1185...  0.0604 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2207...  Training loss: 2.1117...  0.0561 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2208...  Training loss: 2.0837...  0.0572 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2209...  Training loss: 2.0823...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2210...  Training loss: 2.0826...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2211...  Training loss: 2.1009...  0.0533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2212...  Training loss: 2.0947...  0.0549 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2213...  Training loss: 2.1154...  0.0610 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2214...  Training loss: 2.0836...  0.0640 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2215...  Training loss: 2.0956...  0.0525 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2216...  Training loss: 2.1478...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2217...  Training loss: 2.2096...  0.0576 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2218...  Training loss: 2.1305...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2219...  Training loss: 2.0857...  0.0523 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2220...  Training loss: 2.1119...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2221...  Training loss: 2.1216...  0.0533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2222...  Training loss: 2.0834...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2223...  Training loss: 2.0486...  0.0561 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2224...  Training loss: 2.0343...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2225...  Training loss: 2.0603...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2226...  Training loss: 2.1257...  0.0526 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2227...  Training loss: 2.0914...  0.0582 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2228...  Training loss: 2.1469...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2229...  Training loss: 2.1020...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2230...  Training loss: 2.0800...  0.0568 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2231...  Training loss: 2.1297...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2232...  Training loss: 2.1720...  0.0588 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2233...  Training loss: 2.1298...  0.0543 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2234...  Training loss: 2.1207...  0.0537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2235...  Training loss: 2.0885...  0.0552 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2236...  Training loss: 2.1168...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2237...  Training loss: 2.0898...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2238...  Training loss: 2.1395...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2239...  Training loss: 2.1214...  0.0593 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2240...  Training loss: 2.1010...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2241...  Training loss: 2.0334...  0.0533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2242...  Training loss: 2.1439...  0.0590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2243...  Training loss: 2.0499...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2244...  Training loss: 2.1029...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2245...  Training loss: 2.1229...  0.0569 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2246...  Training loss: 2.0432...  0.0563 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2247...  Training loss: 2.0466...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2248...  Training loss: 2.1229...  0.0585 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2249...  Training loss: 2.0489...  0.0620 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2250...  Training loss: 2.0637...  0.0577 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2251...  Training loss: 2.1190...  0.0523 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2252...  Training loss: 2.0351...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2253...  Training loss: 2.1017...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2254...  Training loss: 2.0824...  0.0535 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2255...  Training loss: 2.0646...  0.0522 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2256...  Training loss: 2.0874...  0.0580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2257...  Training loss: 2.0861...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2258...  Training loss: 2.1087...  0.0537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2259...  Training loss: 2.0804...  0.0522 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2260...  Training loss: 2.0978...  0.0547 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2261...  Training loss: 2.1271...  0.0550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2262...  Training loss: 2.1019...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2263...  Training loss: 2.0851...  0.0532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2264...  Training loss: 2.1021...  0.0541 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2265...  Training loss: 2.1441...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2266...  Training loss: 2.1175...  0.0588 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2267...  Training loss: 2.1487...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2268...  Training loss: 2.0926...  0.0539 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2269...  Training loss: 2.1447...  0.0582 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2270...  Training loss: 2.1077...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2271...  Training loss: 2.0832...  0.0546 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2272...  Training loss: 2.1231...  0.0562 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2273...  Training loss: 2.0903...  0.0590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2274...  Training loss: 2.1003...  0.0595 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2275...  Training loss: 2.0497...  0.0526 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2276...  Training loss: 2.0321...  0.0562 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2277...  Training loss: 2.0880...  0.0580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2278...  Training loss: 2.1067...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2279...  Training loss: 2.1498...  0.0552 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2280...  Training loss: 2.0946...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2281...  Training loss: 2.0548...  0.0591 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2282...  Training loss: 2.0554...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2283...  Training loss: 2.1261...  0.0590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2284...  Training loss: 2.0868...  0.0525 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2285...  Training loss: 2.1188...  0.0539 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2286...  Training loss: 2.1274...  0.0539 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2287...  Training loss: 2.0675...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2288...  Training loss: 2.1037...  0.0593 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2289...  Training loss: 2.0928...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2290...  Training loss: 2.0589...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2291...  Training loss: 2.0785...  0.0587 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2292...  Training loss: 2.1243...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2293...  Training loss: 2.1062...  0.0538 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2294...  Training loss: 2.0617...  0.0595 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2295...  Training loss: 2.0741...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2296...  Training loss: 2.0998...  0.0523 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2297...  Training loss: 2.0566...  0.0532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2298...  Training loss: 2.0919...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2299...  Training loss: 2.0754...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2300...  Training loss: 2.0776...  0.0552 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2301...  Training loss: 2.0793...  0.0563 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2302...  Training loss: 2.0909...  0.0563 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2303...  Training loss: 2.1191...  0.0572 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2304...  Training loss: 2.0878...  0.0558 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 2305...  Training loss: 2.0447...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2306...  Training loss: 2.0592...  0.0572 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2307...  Training loss: 2.0633...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2308...  Training loss: 2.0486...  0.0525 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2309...  Training loss: 2.1130...  0.0537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2310...  Training loss: 2.1351...  0.0536 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2311...  Training loss: 2.1446...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2312...  Training loss: 2.0973...  0.0548 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2313...  Training loss: 2.0543...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2314...  Training loss: 2.0569...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2315...  Training loss: 2.0697...  0.0561 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2316...  Training loss: 2.0987...  0.0585 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2317...  Training loss: 2.0592...  0.0536 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2318...  Training loss: 2.0982...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2319...  Training loss: 2.0683...  0.0547 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2320...  Training loss: 2.0798...  0.0568 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2321...  Training loss: 2.0332...  0.0601 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2322...  Training loss: 2.1070...  0.0585 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2323...  Training loss: 2.0838...  0.0524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2324...  Training loss: 2.0777...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2325...  Training loss: 2.1034...  0.0538 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2326...  Training loss: 2.0939...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2327...  Training loss: 2.0645...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2328...  Training loss: 2.0645...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2329...  Training loss: 2.1154...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2330...  Training loss: 2.0874...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2331...  Training loss: 2.0802...  0.0582 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2332...  Training loss: 2.0763...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2333...  Training loss: 2.0565...  0.0557 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2334...  Training loss: 2.0341...  0.0532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2335...  Training loss: 2.1341...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2336...  Training loss: 2.1299...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2337...  Training loss: 2.0943...  0.0569 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2338...  Training loss: 2.1577...  0.0567 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2339...  Training loss: 2.0467...  0.0576 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2340...  Training loss: 2.1298...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2341...  Training loss: 2.0987...  0.0552 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2342...  Training loss: 2.0513...  0.0568 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2343...  Training loss: 2.0907...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2344...  Training loss: 2.0972...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2345...  Training loss: 2.1324...  0.0567 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2346...  Training loss: 2.0819...  0.0537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2347...  Training loss: 2.1096...  0.0557 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2348...  Training loss: 2.0914...  0.0549 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2349...  Training loss: 2.0999...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2350...  Training loss: 2.0926...  0.0573 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2351...  Training loss: 2.0770...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2352...  Training loss: 2.1252...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2353...  Training loss: 2.0767...  0.0563 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2354...  Training loss: 2.0793...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2355...  Training loss: 2.0814...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2356...  Training loss: 2.0885...  0.0521 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2357...  Training loss: 2.0402...  0.0543 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2358...  Training loss: 2.1124...  0.0563 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2359...  Training loss: 2.0396...  0.0537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2360...  Training loss: 2.0966...  0.0571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2361...  Training loss: 2.0955...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2362...  Training loss: 2.1374...  0.0557 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2363...  Training loss: 2.1429...  0.0555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2364...  Training loss: 2.1180...  0.0570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2365...  Training loss: 2.0927...  0.0564 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2366...  Training loss: 2.0451...  0.0552 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2367...  Training loss: 2.1000...  0.0584 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2368...  Training loss: 2.0591...  0.0561 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2369...  Training loss: 2.1208...  0.0546 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2370...  Training loss: 2.1245...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2371...  Training loss: 2.1214...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2372...  Training loss: 2.0533...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2373...  Training loss: 2.1530...  0.0535 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2374...  Training loss: 2.1152...  0.0543 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2375...  Training loss: 2.0924...  0.0559 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2376...  Training loss: 2.0754...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2377...  Training loss: 2.0690...  0.0532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2378...  Training loss: 2.0968...  0.0569 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2379...  Training loss: 2.0720...  0.0523 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2380...  Training loss: 2.0696...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2381...  Training loss: 2.0707...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2382...  Training loss: 2.0416...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2383...  Training loss: 2.0940...  0.0537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2384...  Training loss: 1.9849...  0.0586 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2385...  Training loss: 2.0701...  0.0549 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2386...  Training loss: 2.1236...  0.0546 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2387...  Training loss: 2.1195...  0.0559 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2388...  Training loss: 2.0894...  0.0595 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2389...  Training loss: 2.0733...  0.0544 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2390...  Training loss: 2.0296...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2391...  Training loss: 2.0866...  0.0532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2392...  Training loss: 2.0424...  0.0532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2393...  Training loss: 2.0730...  0.0573 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2394...  Training loss: 2.0541...  0.0524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2395...  Training loss: 2.0868...  0.0548 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2396...  Training loss: 2.0700...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2397...  Training loss: 2.0389...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2398...  Training loss: 2.1116...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2399...  Training loss: 2.0233...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2400...  Training loss: 2.1173...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2401...  Training loss: 2.0927...  0.0570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2402...  Training loss: 2.0611...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2403...  Training loss: 2.0530...  0.0544 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2404...  Training loss: 2.0550...  0.0527 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 2405...  Training loss: 2.1047...  0.0604 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2406...  Training loss: 2.0704...  0.0599 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2407...  Training loss: 2.0929...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2408...  Training loss: 2.1169...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2409...  Training loss: 2.1337...  0.0564 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2410...  Training loss: 2.0911...  0.0533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2411...  Training loss: 2.0647...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2412...  Training loss: 2.0451...  0.0594 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2413...  Training loss: 2.0760...  0.0528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2414...  Training loss: 2.0993...  0.0551 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2415...  Training loss: 2.0612...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2416...  Training loss: 2.0457...  0.0522 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2417...  Training loss: 2.0715...  0.0532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2418...  Training loss: 2.0520...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2419...  Training loss: 2.0850...  0.0522 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2420...  Training loss: 2.0843...  0.0526 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2421...  Training loss: 2.0810...  0.0550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2422...  Training loss: 2.1229...  0.0532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2423...  Training loss: 2.0782...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2424...  Training loss: 2.1314...  0.0597 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2425...  Training loss: 2.2001...  0.0584 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2426...  Training loss: 2.1006...  0.0583 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2427...  Training loss: 2.1055...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2428...  Training loss: 2.1415...  0.0524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2429...  Training loss: 2.0907...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2430...  Training loss: 2.0759...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2431...  Training loss: 2.0932...  0.0578 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2432...  Training loss: 2.1264...  0.0586 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2433...  Training loss: 2.0667...  0.0541 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2434...  Training loss: 2.0824...  0.0577 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2435...  Training loss: 2.0663...  0.0564 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2436...  Training loss: 2.1356...  0.0573 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2437...  Training loss: 2.0179...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2438...  Training loss: 2.0910...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2439...  Training loss: 2.1413...  0.0571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2440...  Training loss: 2.1175...  0.0536 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2441...  Training loss: 2.1028...  0.0542 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2442...  Training loss: 2.0686...  0.0568 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2443...  Training loss: 2.0780...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2444...  Training loss: 2.1411...  0.0567 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2445...  Training loss: 2.0957...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2446...  Training loss: 2.0660...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2447...  Training loss: 2.0495...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2448...  Training loss: 2.1037...  0.0587 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2449...  Training loss: 2.0396...  0.0582 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2450...  Training loss: 2.1238...  0.0552 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2451...  Training loss: 2.0256...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2452...  Training loss: 2.1302...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2453...  Training loss: 2.0398...  0.0571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2454...  Training loss: 2.0517...  0.0534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2455...  Training loss: 2.0283...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2456...  Training loss: 2.0408...  0.0581 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2457...  Training loss: 2.0324...  0.0537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2458...  Training loss: 2.0588...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2459...  Training loss: 2.0915...  0.0554 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2460...  Training loss: 2.1080...  0.0589 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2461...  Training loss: 2.0902...  0.0552 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2462...  Training loss: 2.1005...  0.0594 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2463...  Training loss: 2.0930...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2464...  Training loss: 2.0301...  0.0584 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2465...  Training loss: 2.0672...  0.0588 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2466...  Training loss: 2.0739...  0.0538 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2467...  Training loss: 2.0118...  0.0577 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2468...  Training loss: 2.0529...  0.0595 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2469...  Training loss: 2.0335...  0.0569 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2470...  Training loss: 2.0956...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2471...  Training loss: 2.1091...  0.0569 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2472...  Training loss: 2.0966...  0.0535 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2473...  Training loss: 2.0335...  0.0598 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2474...  Training loss: 2.0769...  0.0533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2475...  Training loss: 2.0284...  0.0563 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2476...  Training loss: 2.0957...  0.0524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2477...  Training loss: 2.1009...  0.0564 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2478...  Training loss: 2.0453...  0.0565 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2479...  Training loss: 2.0056...  0.0588 sec/batch\n",
      "Epoch: 4/20...  Training Step: 2480...  Training loss: 2.0581...  0.0526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2481...  Training loss: 2.1695...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2482...  Training loss: 2.1368...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2483...  Training loss: 2.0952...  0.0559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2484...  Training loss: 2.0274...  0.0549 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2485...  Training loss: 2.0771...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2486...  Training loss: 2.0799...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2487...  Training loss: 2.0520...  0.0534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2488...  Training loss: 2.0292...  0.0534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2489...  Training loss: 2.0161...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2490...  Training loss: 2.0599...  0.0538 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2491...  Training loss: 2.0649...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2492...  Training loss: 2.0426...  0.0524 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2493...  Training loss: 2.0823...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2494...  Training loss: 2.0432...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2495...  Training loss: 2.0832...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2496...  Training loss: 2.1089...  0.0545 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2497...  Training loss: 2.0919...  0.0571 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2498...  Training loss: 2.0934...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2499...  Training loss: 2.0531...  0.0537 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2500...  Training loss: 2.0909...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2501...  Training loss: 2.1269...  0.0560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2502...  Training loss: 2.0660...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2503...  Training loss: 2.0257...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2504...  Training loss: 2.0747...  0.0591 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 2505...  Training loss: 2.0591...  0.0545 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2506...  Training loss: 2.0381...  0.0534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2507...  Training loss: 2.0644...  0.0573 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2508...  Training loss: 2.0459...  0.0574 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2509...  Training loss: 2.0927...  0.0573 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2510...  Training loss: 2.0235...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2511...  Training loss: 2.0283...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2512...  Training loss: 2.0799...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2513...  Training loss: 2.0468...  0.0611 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2514...  Training loss: 2.0300...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2515...  Training loss: 2.0495...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2516...  Training loss: 2.0521...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2517...  Training loss: 2.0602...  0.0573 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2518...  Training loss: 2.0676...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2519...  Training loss: 2.0714...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2520...  Training loss: 2.0358...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2521...  Training loss: 2.0546...  0.0520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2522...  Training loss: 2.0821...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2523...  Training loss: 2.0345...  0.0572 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2524...  Training loss: 2.0787...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2525...  Training loss: 2.0473...  0.0592 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2526...  Training loss: 2.0351...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2527...  Training loss: 1.9660...  0.0534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2528...  Training loss: 2.0652...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2529...  Training loss: 2.0631...  0.0562 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2530...  Training loss: 2.0633...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2531...  Training loss: 2.0224...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2532...  Training loss: 2.0486...  0.0575 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2533...  Training loss: 2.0792...  0.0608 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2534...  Training loss: 2.0835...  0.0564 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2535...  Training loss: 2.1138...  0.0618 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2536...  Training loss: 2.0316...  0.0562 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2537...  Training loss: 2.0153...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2538...  Training loss: 2.0538...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2539...  Training loss: 2.0293...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2540...  Training loss: 2.1005...  0.0559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2541...  Training loss: 2.0785...  0.0544 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2542...  Training loss: 2.0576...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2543...  Training loss: 2.0645...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2544...  Training loss: 2.0333...  0.0523 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2545...  Training loss: 2.0199...  0.0603 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2546...  Training loss: 1.9847...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2547...  Training loss: 2.0156...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2548...  Training loss: 1.9962...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2549...  Training loss: 2.0897...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2550...  Training loss: 2.0674...  0.0584 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2551...  Training loss: 2.0641...  0.0546 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2552...  Training loss: 2.0678...  0.0542 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2553...  Training loss: 2.0421...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2554...  Training loss: 2.0383...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2555...  Training loss: 2.0946...  0.0546 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2556...  Training loss: 2.0668...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2557...  Training loss: 2.0813...  0.0526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2558...  Training loss: 2.0572...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2559...  Training loss: 2.0619...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2560...  Training loss: 2.0881...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2561...  Training loss: 2.0021...  0.0589 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2562...  Training loss: 2.0419...  0.0546 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2563...  Training loss: 2.0108...  0.0562 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2564...  Training loss: 2.0307...  0.0566 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2565...  Training loss: 2.0551...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2566...  Training loss: 2.0632...  0.0550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2567...  Training loss: 2.0360...  0.0584 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2568...  Training loss: 2.0963...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2569...  Training loss: 2.0476...  0.0578 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2570...  Training loss: 2.0765...  0.0540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2571...  Training loss: 2.0228...  0.0589 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2572...  Training loss: 2.1057...  0.0559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2573...  Training loss: 2.0608...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2574...  Training loss: 2.0558...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2575...  Training loss: 2.0658...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2576...  Training loss: 2.0725...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2577...  Training loss: 2.0765...  0.0525 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2578...  Training loss: 2.0188...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2579...  Training loss: 2.1145...  0.0541 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2580...  Training loss: 2.0504...  0.0547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2581...  Training loss: 2.0294...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2582...  Training loss: 2.0386...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2583...  Training loss: 2.0882...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2584...  Training loss: 2.0749...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2585...  Training loss: 2.0310...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2586...  Training loss: 2.0114...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2587...  Training loss: 2.0526...  0.0549 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2588...  Training loss: 2.0226...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2589...  Training loss: 2.0272...  0.0523 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2590...  Training loss: 2.0328...  0.0546 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2591...  Training loss: 2.0096...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2592...  Training loss: 2.0401...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2593...  Training loss: 2.0402...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2594...  Training loss: 2.0372...  0.0526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2595...  Training loss: 2.0659...  0.0524 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2596...  Training loss: 2.0791...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2597...  Training loss: 2.0048...  0.0550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2598...  Training loss: 2.0782...  0.0584 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2599...  Training loss: 2.0269...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2600...  Training loss: 2.0025...  0.0523 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2601...  Training loss: 2.0236...  0.0592 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2602...  Training loss: 2.0176...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2603...  Training loss: 2.0219...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2604...  Training loss: 2.0423...  0.0589 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 2605...  Training loss: 2.0768...  0.0587 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2606...  Training loss: 2.0953...  0.0547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2607...  Training loss: 2.0763...  0.0526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2608...  Training loss: 2.0058...  0.0526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2609...  Training loss: 2.0252...  0.0597 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2610...  Training loss: 2.0813...  0.0582 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2611...  Training loss: 2.0306...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2612...  Training loss: 2.0973...  0.0521 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2613...  Training loss: 2.0862...  0.0543 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2614...  Training loss: 2.0393...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2615...  Training loss: 1.9946...  0.0525 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2616...  Training loss: 2.0381...  0.0601 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2617...  Training loss: 2.0320...  0.0590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2618...  Training loss: 2.0325...  0.0584 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2619...  Training loss: 2.0991...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2620...  Training loss: 2.0244...  0.0546 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2621...  Training loss: 2.0549...  0.0541 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2622...  Training loss: 2.0005...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2623...  Training loss: 2.0448...  0.0588 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2624...  Training loss: 2.0142...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2625...  Training loss: 2.0178...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2626...  Training loss: 2.0722...  0.0525 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2627...  Training loss: 2.0530...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2628...  Training loss: 2.0820...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2629...  Training loss: 2.0231...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2630...  Training loss: 2.0656...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2631...  Training loss: 2.0759...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2632...  Training loss: 2.0425...  0.0568 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2633...  Training loss: 2.0147...  0.0562 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2634...  Training loss: 2.0654...  0.0576 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2635...  Training loss: 2.0340...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2636...  Training loss: 2.0695...  0.0548 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2637...  Training loss: 2.0404...  0.0587 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2638...  Training loss: 2.0826...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2639...  Training loss: 2.0748...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2640...  Training loss: 2.0199...  0.0522 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2641...  Training loss: 2.0002...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2642...  Training loss: 2.0269...  0.0559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2643...  Training loss: 2.0236...  0.0578 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2644...  Training loss: 2.0376...  0.0587 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2645...  Training loss: 2.0352...  0.0586 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2646...  Training loss: 2.0341...  0.0584 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2647...  Training loss: 2.0648...  0.0541 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2648...  Training loss: 2.0491...  0.0559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2649...  Training loss: 2.0091...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2650...  Training loss: 2.0052...  0.0573 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2651...  Training loss: 2.0124...  0.0605 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2652...  Training loss: 2.0501...  0.0559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2653...  Training loss: 2.0437...  0.0565 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2654...  Training loss: 2.0304...  0.0591 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2655...  Training loss: 2.0294...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2656...  Training loss: 2.0532...  0.0590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2657...  Training loss: 2.0438...  0.0559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2658...  Training loss: 2.0342...  0.0545 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2659...  Training loss: 2.0268...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2660...  Training loss: 2.0179...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2661...  Training loss: 2.0241...  0.0575 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2662...  Training loss: 2.0546...  0.0590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2663...  Training loss: 2.0365...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2664...  Training loss: 1.9967...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2665...  Training loss: 1.9880...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2666...  Training loss: 2.0417...  0.0574 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2667...  Training loss: 2.0050...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2668...  Training loss: 2.0289...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2669...  Training loss: 2.0474...  0.0578 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2670...  Training loss: 2.1160...  0.0587 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2671...  Training loss: 2.0822...  0.0576 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2672...  Training loss: 2.1106...  0.0537 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2673...  Training loss: 2.0615...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2674...  Training loss: 2.0105...  0.0525 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2675...  Training loss: 2.0464...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2676...  Training loss: 2.1049...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2677...  Training loss: 2.0500...  0.0572 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2678...  Training loss: 2.0972...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2679...  Training loss: 2.0190...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2680...  Training loss: 2.0417...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2681...  Training loss: 2.0539...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2682...  Training loss: 2.0358...  0.0562 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2683...  Training loss: 2.0189...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2684...  Training loss: 2.0207...  0.0547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2685...  Training loss: 2.0449...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2686...  Training loss: 1.9940...  0.0564 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2687...  Training loss: 2.0620...  0.0617 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2688...  Training loss: 2.0389...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2689...  Training loss: 2.0429...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2690...  Training loss: 2.0268...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2691...  Training loss: 2.0359...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2692...  Training loss: 2.0545...  0.0588 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2693...  Training loss: 2.0662...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2694...  Training loss: 2.0793...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2695...  Training loss: 2.0576...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2696...  Training loss: 2.0644...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2697...  Training loss: 2.0497...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2698...  Training loss: 2.0056...  0.0566 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2699...  Training loss: 2.1337...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2700...  Training loss: 2.0838...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2701...  Training loss: 2.0359...  0.0549 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2702...  Training loss: 2.0524...  0.0538 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2703...  Training loss: 2.0992...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2704...  Training loss: 2.0059...  0.0561 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 2705...  Training loss: 2.0306...  0.0606 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2706...  Training loss: 2.0722...  0.0548 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2707...  Training loss: 2.0847...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2708...  Training loss: 2.0117...  0.0545 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2709...  Training loss: 2.0587...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2710...  Training loss: 2.0375...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2711...  Training loss: 2.0865...  0.0580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2712...  Training loss: 2.0198...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2713...  Training loss: 2.0220...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2714...  Training loss: 2.0048...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2715...  Training loss: 2.0250...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2716...  Training loss: 2.0606...  0.0562 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2717...  Training loss: 2.0448...  0.0581 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2718...  Training loss: 1.9851...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2719...  Training loss: 1.9956...  0.0577 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2720...  Training loss: 2.0772...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2721...  Training loss: 2.0420...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2722...  Training loss: 2.0283...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2723...  Training loss: 2.0113...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2724...  Training loss: 2.0281...  0.0524 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2725...  Training loss: 2.0188...  0.0573 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2726...  Training loss: 2.0250...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2727...  Training loss: 2.0329...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2728...  Training loss: 2.0554...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2729...  Training loss: 2.0050...  0.0534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2730...  Training loss: 2.0411...  0.0591 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2731...  Training loss: 2.0173...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2732...  Training loss: 1.9794...  0.0524 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2733...  Training loss: 2.0478...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2734...  Training loss: 2.0476...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2735...  Training loss: 2.0735...  0.0574 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2736...  Training loss: 2.0335...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2737...  Training loss: 2.0339...  0.0566 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2738...  Training loss: 2.0512...  0.0537 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2739...  Training loss: 2.0481...  0.0564 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2740...  Training loss: 2.0348...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2741...  Training loss: 2.0545...  0.0564 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2742...  Training loss: 2.0302...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2743...  Training loss: 2.0557...  0.0678 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2744...  Training loss: 2.0408...  0.0720 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2745...  Training loss: 2.0645...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2746...  Training loss: 1.9728...  0.0525 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2747...  Training loss: 2.0571...  0.0524 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2748...  Training loss: 2.0332...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2749...  Training loss: 2.0294...  0.0546 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2750...  Training loss: 2.0210...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2751...  Training loss: 1.9981...  0.0559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2752...  Training loss: 2.0397...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2753...  Training loss: 2.0199...  0.0585 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2754...  Training loss: 2.0196...  0.0584 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2755...  Training loss: 2.0538...  0.0560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2756...  Training loss: 2.0636...  0.0583 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2757...  Training loss: 2.0935...  0.0592 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2758...  Training loss: 2.0429...  0.0606 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2759...  Training loss: 2.0672...  0.0534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2760...  Training loss: 2.0783...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2761...  Training loss: 2.0411...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2762...  Training loss: 1.9783...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2763...  Training loss: 2.0165...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2764...  Training loss: 2.0361...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2765...  Training loss: 2.0019...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2766...  Training loss: 2.0521...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2767...  Training loss: 2.0179...  0.0581 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2768...  Training loss: 2.0181...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2769...  Training loss: 2.0380...  0.0541 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2770...  Training loss: 2.0877...  0.0560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2771...  Training loss: 2.0156...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2772...  Training loss: 2.0437...  0.0560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2773...  Training loss: 2.0274...  0.0547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2774...  Training loss: 2.0329...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2775...  Training loss: 2.0044...  0.0593 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2776...  Training loss: 2.0183...  0.0624 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2777...  Training loss: 2.0547...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2778...  Training loss: 2.0542...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2779...  Training loss: 2.0924...  0.0526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2780...  Training loss: 2.0230...  0.0567 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2781...  Training loss: 2.0437...  0.0567 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2782...  Training loss: 2.0898...  0.0539 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2783...  Training loss: 1.9815...  0.0602 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2784...  Training loss: 2.0566...  0.0540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2785...  Training loss: 2.0260...  0.0539 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2786...  Training loss: 2.0391...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2787...  Training loss: 2.0314...  0.0584 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2788...  Training loss: 2.0097...  0.0547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2789...  Training loss: 2.0492...  0.0580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2790...  Training loss: 1.9834...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2791...  Training loss: 2.0053...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2792...  Training loss: 2.0204...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2793...  Training loss: 2.0061...  0.0590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2794...  Training loss: 1.9878...  0.0565 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2795...  Training loss: 2.0171...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2796...  Training loss: 2.0400...  0.0548 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2797...  Training loss: 1.9941...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2798...  Training loss: 1.9882...  0.0541 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2799...  Training loss: 2.0400...  0.0576 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2800...  Training loss: 2.0835...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2801...  Training loss: 2.0298...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2802...  Training loss: 1.9802...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2803...  Training loss: 2.0307...  0.0545 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2804...  Training loss: 2.0187...  0.0571 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 2805...  Training loss: 1.9803...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2806...  Training loss: 2.0047...  0.0579 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2807...  Training loss: 1.9958...  0.0585 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2808...  Training loss: 1.9718...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2809...  Training loss: 2.0269...  0.0574 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2810...  Training loss: 2.0017...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2811...  Training loss: 2.0310...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2812...  Training loss: 2.0110...  0.0589 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2813...  Training loss: 1.9760...  0.0562 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2814...  Training loss: 2.0153...  0.0549 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2815...  Training loss: 2.0290...  0.0523 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2816...  Training loss: 2.0425...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2817...  Training loss: 1.9883...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2818...  Training loss: 2.0125...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2819...  Training loss: 2.0169...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2820...  Training loss: 2.0481...  0.0566 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2821...  Training loss: 2.0253...  0.0594 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2822...  Training loss: 2.0009...  0.0574 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2823...  Training loss: 2.0011...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2824...  Training loss: 2.0091...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2825...  Training loss: 2.0217...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2826...  Training loss: 2.0438...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2827...  Training loss: 2.0332...  0.0534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2828...  Training loss: 2.0136...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2829...  Training loss: 2.0166...  0.0560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2830...  Training loss: 1.9992...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2831...  Training loss: 2.0447...  0.0526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2832...  Training loss: 2.0050...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2833...  Training loss: 2.0514...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2834...  Training loss: 2.0066...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2835...  Training loss: 1.9978...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2836...  Training loss: 2.0675...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2837...  Training loss: 2.1150...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2838...  Training loss: 2.0404...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2839...  Training loss: 2.0439...  0.0564 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2840...  Training loss: 2.0549...  0.0564 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2841...  Training loss: 2.0501...  0.0585 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2842...  Training loss: 2.0001...  0.0572 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2843...  Training loss: 1.9734...  0.0588 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2844...  Training loss: 1.9425...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2845...  Training loss: 1.9971...  0.0590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2846...  Training loss: 2.0387...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2847...  Training loss: 2.0071...  0.0550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2848...  Training loss: 2.0884...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2849...  Training loss: 2.0456...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2850...  Training loss: 2.0165...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2851...  Training loss: 2.0505...  0.0559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2852...  Training loss: 2.0928...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2853...  Training loss: 2.0461...  0.0538 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2854...  Training loss: 2.0325...  0.0567 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2855...  Training loss: 2.0098...  0.0544 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2856...  Training loss: 2.0461...  0.0526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2857...  Training loss: 2.0115...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2858...  Training loss: 2.0843...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2859...  Training loss: 2.0436...  0.0594 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2860...  Training loss: 2.0056...  0.0525 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2861...  Training loss: 1.9680...  0.0538 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2862...  Training loss: 2.0832...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2863...  Training loss: 1.9719...  0.0588 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2864...  Training loss: 2.0441...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2865...  Training loss: 2.0304...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2866...  Training loss: 1.9607...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2867...  Training loss: 1.9801...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2868...  Training loss: 2.0216...  0.0547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2869...  Training loss: 1.9573...  0.0597 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2870...  Training loss: 1.9790...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2871...  Training loss: 2.0520...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2872...  Training loss: 1.9529...  0.0547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2873...  Training loss: 2.0118...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2874...  Training loss: 2.0204...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2875...  Training loss: 1.9901...  0.0539 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2876...  Training loss: 2.0252...  0.0595 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2877...  Training loss: 2.0149...  0.0599 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2878...  Training loss: 2.0333...  0.0580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2879...  Training loss: 2.0159...  0.0616 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2880...  Training loss: 2.0236...  0.0586 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2881...  Training loss: 2.0522...  0.0560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2882...  Training loss: 2.0340...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2883...  Training loss: 2.0111...  0.0564 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2884...  Training loss: 2.0527...  0.0562 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2885...  Training loss: 2.0663...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2886...  Training loss: 2.0355...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2887...  Training loss: 2.0900...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2888...  Training loss: 2.0102...  0.0578 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2889...  Training loss: 2.0661...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2890...  Training loss: 2.0349...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2891...  Training loss: 2.0117...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2892...  Training loss: 2.0587...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2893...  Training loss: 2.0372...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2894...  Training loss: 2.0161...  0.0548 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2895...  Training loss: 1.9809...  0.0572 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2896...  Training loss: 1.9773...  0.0570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2897...  Training loss: 2.0051...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2898...  Training loss: 2.0301...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2899...  Training loss: 2.0568...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2900...  Training loss: 2.0186...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2901...  Training loss: 1.9740...  0.0559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2902...  Training loss: 2.0090...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2903...  Training loss: 2.0436...  0.0598 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2904...  Training loss: 1.9999...  0.0526 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 2905...  Training loss: 2.0568...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2906...  Training loss: 2.0454...  0.0523 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2907...  Training loss: 1.9957...  0.0592 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2908...  Training loss: 2.0217...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2909...  Training loss: 2.0291...  0.0538 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2910...  Training loss: 1.9733...  0.0534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2911...  Training loss: 2.0149...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2912...  Training loss: 2.0706...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2913...  Training loss: 2.0253...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2914...  Training loss: 1.9967...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2915...  Training loss: 1.9910...  0.0546 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2916...  Training loss: 2.0317...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2917...  Training loss: 1.9757...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2918...  Training loss: 2.0365...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2919...  Training loss: 2.0341...  0.0589 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2920...  Training loss: 1.9875...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2921...  Training loss: 2.0072...  0.0546 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2922...  Training loss: 2.0038...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2923...  Training loss: 2.0579...  0.0526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2924...  Training loss: 1.9951...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2925...  Training loss: 1.9519...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2926...  Training loss: 2.0071...  0.0525 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2927...  Training loss: 1.9909...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2928...  Training loss: 1.9739...  0.0549 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2929...  Training loss: 2.0298...  0.0572 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2930...  Training loss: 2.0755...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2931...  Training loss: 2.0937...  0.0564 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2932...  Training loss: 2.0357...  0.0560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2933...  Training loss: 1.9836...  0.0571 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2934...  Training loss: 1.9997...  0.0523 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2935...  Training loss: 1.9992...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2936...  Training loss: 2.0233...  0.0562 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2937...  Training loss: 2.0087...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2938...  Training loss: 2.0352...  0.0547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2939...  Training loss: 2.0027...  0.0559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2940...  Training loss: 2.0237...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2941...  Training loss: 1.9788...  0.0539 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2942...  Training loss: 2.0147...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2943...  Training loss: 1.9916...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2944...  Training loss: 1.9953...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2945...  Training loss: 2.0516...  0.0582 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2946...  Training loss: 2.0189...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2947...  Training loss: 2.0038...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2948...  Training loss: 1.9963...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2949...  Training loss: 2.0266...  0.0594 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2950...  Training loss: 2.0140...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2951...  Training loss: 2.0087...  0.0567 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2952...  Training loss: 2.0202...  0.0564 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2953...  Training loss: 1.9766...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2954...  Training loss: 1.9499...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2955...  Training loss: 2.0609...  0.0522 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2956...  Training loss: 2.0646...  0.0564 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2957...  Training loss: 2.0174...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2958...  Training loss: 2.0667...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2959...  Training loss: 1.9907...  0.0577 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2960...  Training loss: 2.0555...  0.0590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2961...  Training loss: 2.0302...  0.0549 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2962...  Training loss: 1.9714...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2963...  Training loss: 2.0476...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2964...  Training loss: 2.0349...  0.0577 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2965...  Training loss: 2.0680...  0.0538 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2966...  Training loss: 1.9889...  0.0593 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2967...  Training loss: 2.0524...  0.0570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2968...  Training loss: 2.0230...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2969...  Training loss: 2.0279...  0.0591 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2970...  Training loss: 2.0110...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2971...  Training loss: 2.0151...  0.0541 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2972...  Training loss: 2.0656...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2973...  Training loss: 2.0034...  0.0526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2974...  Training loss: 2.0077...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2975...  Training loss: 1.9894...  0.0585 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2976...  Training loss: 2.0278...  0.0584 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2977...  Training loss: 2.0106...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2978...  Training loss: 2.0454...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2979...  Training loss: 1.9611...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2980...  Training loss: 2.0319...  0.0587 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2981...  Training loss: 2.0237...  0.0560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2982...  Training loss: 2.0472...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2983...  Training loss: 2.0783...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2984...  Training loss: 2.0296...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2985...  Training loss: 2.0080...  0.0565 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2986...  Training loss: 1.9580...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2987...  Training loss: 2.0340...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2988...  Training loss: 2.0015...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2989...  Training loss: 2.0221...  0.0534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2990...  Training loss: 2.0219...  0.0565 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2991...  Training loss: 2.0324...  0.0571 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2992...  Training loss: 2.0154...  0.0582 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2993...  Training loss: 2.0846...  0.0579 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2994...  Training loss: 2.0392...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2995...  Training loss: 2.0283...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2996...  Training loss: 2.0108...  0.0549 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2997...  Training loss: 1.9970...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2998...  Training loss: 1.9970...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 2999...  Training loss: 1.9892...  0.0548 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3000...  Training loss: 1.9872...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3001...  Training loss: 1.9982...  0.0537 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3002...  Training loss: 1.9784...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3003...  Training loss: 1.9874...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3004...  Training loss: 1.9104...  0.0550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 3005...  Training loss: 2.0033...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3006...  Training loss: 2.0532...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3007...  Training loss: 2.0406...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3008...  Training loss: 2.0278...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3009...  Training loss: 2.0059...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3010...  Training loss: 1.9803...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3011...  Training loss: 2.0164...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3012...  Training loss: 1.9805...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3013...  Training loss: 1.9973...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3014...  Training loss: 1.9794...  0.0534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3015...  Training loss: 2.0066...  0.0537 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3016...  Training loss: 2.0143...  0.0581 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3017...  Training loss: 1.9668...  0.0547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3018...  Training loss: 2.0254...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3019...  Training loss: 1.9461...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3020...  Training loss: 2.0366...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3021...  Training loss: 2.0261...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3022...  Training loss: 2.0041...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3023...  Training loss: 1.9887...  0.0594 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3024...  Training loss: 1.9671...  0.0550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3025...  Training loss: 2.0332...  0.0540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3026...  Training loss: 2.0078...  0.0525 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3027...  Training loss: 2.0489...  0.0526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3028...  Training loss: 2.0743...  0.0590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3029...  Training loss: 2.0585...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3030...  Training loss: 2.0186...  0.0556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3031...  Training loss: 2.0220...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3032...  Training loss: 1.9702...  0.0564 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3033...  Training loss: 2.0178...  0.0526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3034...  Training loss: 2.0344...  0.0573 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3035...  Training loss: 1.9863...  0.0568 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3036...  Training loss: 1.9964...  0.0570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3037...  Training loss: 1.9911...  0.0593 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3038...  Training loss: 1.9719...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3039...  Training loss: 2.0371...  0.0560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3040...  Training loss: 2.0236...  0.0543 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3041...  Training loss: 2.0381...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3042...  Training loss: 2.0460...  0.0524 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3043...  Training loss: 2.0358...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3044...  Training loss: 2.0713...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3045...  Training loss: 2.1089...  0.0547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3046...  Training loss: 2.0470...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3047...  Training loss: 2.0257...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3048...  Training loss: 2.0750...  0.0595 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3049...  Training loss: 1.9868...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3050...  Training loss: 1.9976...  0.0547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3051...  Training loss: 2.0155...  0.0524 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3052...  Training loss: 2.0667...  0.0593 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3053...  Training loss: 1.9999...  0.0597 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3054...  Training loss: 1.9758...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3055...  Training loss: 1.9919...  0.0527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3056...  Training loss: 2.0697...  0.0528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3057...  Training loss: 1.9663...  0.0585 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3058...  Training loss: 2.0139...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3059...  Training loss: 2.0951...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3060...  Training loss: 2.0440...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3061...  Training loss: 2.0292...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3062...  Training loss: 2.0033...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3063...  Training loss: 2.0261...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3064...  Training loss: 2.0645...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3065...  Training loss: 2.0366...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3066...  Training loss: 2.0059...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3067...  Training loss: 1.9704...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3068...  Training loss: 2.0160...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3069...  Training loss: 1.9745...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3070...  Training loss: 2.0422...  0.0621 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3071...  Training loss: 1.9534...  0.0547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3072...  Training loss: 2.0408...  0.0548 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3073...  Training loss: 1.9854...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3074...  Training loss: 1.9734...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3075...  Training loss: 1.9790...  0.0585 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3076...  Training loss: 1.9788...  0.0550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3077...  Training loss: 1.9689...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3078...  Training loss: 2.0103...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3079...  Training loss: 2.0095...  0.0531 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3080...  Training loss: 2.0487...  0.0559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3081...  Training loss: 2.0149...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3082...  Training loss: 2.0178...  0.0523 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3083...  Training loss: 2.0249...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3084...  Training loss: 1.9823...  0.0555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3085...  Training loss: 2.0215...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3086...  Training loss: 2.0048...  0.0581 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3087...  Training loss: 1.9684...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3088...  Training loss: 1.9909...  0.0565 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3089...  Training loss: 1.9808...  0.0560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3090...  Training loss: 2.0614...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3091...  Training loss: 2.0326...  0.0588 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3092...  Training loss: 2.0448...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3093...  Training loss: 1.9692...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3094...  Training loss: 2.0110...  0.0581 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3095...  Training loss: 1.9543...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3096...  Training loss: 2.0275...  0.0597 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3097...  Training loss: 2.0350...  0.0542 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3098...  Training loss: 1.9795...  0.0534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3099...  Training loss: 1.9358...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 3100...  Training loss: 2.0082...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3101...  Training loss: 2.0826...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3102...  Training loss: 2.0875...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3103...  Training loss: 2.0583...  0.0559 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3104...  Training loss: 1.9699...  0.0528 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 3105...  Training loss: 2.0061...  0.0549 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3106...  Training loss: 2.0171...  0.0602 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3107...  Training loss: 1.9765...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3108...  Training loss: 1.9670...  0.0585 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3109...  Training loss: 1.9571...  0.0633 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3110...  Training loss: 1.9838...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3111...  Training loss: 2.0116...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3112...  Training loss: 1.9510...  0.0579 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3113...  Training loss: 2.0327...  0.0566 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3114...  Training loss: 1.9830...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3115...  Training loss: 2.0377...  0.0589 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3116...  Training loss: 2.0528...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3117...  Training loss: 2.0313...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3118...  Training loss: 2.0124...  0.0575 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3119...  Training loss: 1.9684...  0.0535 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3120...  Training loss: 2.0067...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3121...  Training loss: 2.0591...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3122...  Training loss: 1.9890...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3123...  Training loss: 1.9755...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3124...  Training loss: 1.9963...  0.0572 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3125...  Training loss: 1.9853...  0.0590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3126...  Training loss: 1.9805...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3127...  Training loss: 2.0032...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3128...  Training loss: 1.9778...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3129...  Training loss: 2.0264...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3130...  Training loss: 1.9688...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3131...  Training loss: 1.9580...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3132...  Training loss: 2.0095...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3133...  Training loss: 1.9856...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3134...  Training loss: 2.0042...  0.0525 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3135...  Training loss: 2.0127...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3136...  Training loss: 1.9920...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3137...  Training loss: 1.9720...  0.0525 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3138...  Training loss: 1.9948...  0.0549 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3139...  Training loss: 2.0076...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3140...  Training loss: 1.9505...  0.0569 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3141...  Training loss: 1.9926...  0.0596 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3142...  Training loss: 2.0170...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3143...  Training loss: 1.9846...  0.0524 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3144...  Training loss: 2.0295...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3145...  Training loss: 1.9998...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3146...  Training loss: 1.9727...  0.0544 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3147...  Training loss: 1.8922...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3148...  Training loss: 2.0085...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3149...  Training loss: 2.0028...  0.0535 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3150...  Training loss: 1.9855...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3151...  Training loss: 1.9703...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3152...  Training loss: 1.9769...  0.0575 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3153...  Training loss: 1.9982...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3154...  Training loss: 2.0398...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3155...  Training loss: 2.0354...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3156...  Training loss: 1.9916...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3157...  Training loss: 1.9643...  0.0607 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3158...  Training loss: 2.0131...  0.0585 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3159...  Training loss: 1.9787...  0.0576 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3160...  Training loss: 2.0474...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3161...  Training loss: 1.9854...  0.0540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3162...  Training loss: 1.9723...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3163...  Training loss: 2.0182...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3164...  Training loss: 1.9942...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3165...  Training loss: 1.9644...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3166...  Training loss: 1.9323...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3167...  Training loss: 1.9477...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3168...  Training loss: 1.9597...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3169...  Training loss: 2.0166...  0.0561 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3170...  Training loss: 1.9813...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3171...  Training loss: 2.0081...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3172...  Training loss: 2.0145...  0.0539 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3173...  Training loss: 1.9518...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3174...  Training loss: 1.9935...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3175...  Training loss: 2.0445...  0.0562 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3176...  Training loss: 1.9915...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3177...  Training loss: 2.0236...  0.0574 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3178...  Training loss: 1.9771...  0.0601 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3179...  Training loss: 1.9931...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3180...  Training loss: 2.0064...  0.0586 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3181...  Training loss: 1.9469...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3182...  Training loss: 1.9864...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3183...  Training loss: 1.9433...  0.0524 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3184...  Training loss: 1.9689...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3185...  Training loss: 1.9816...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3186...  Training loss: 2.0314...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3187...  Training loss: 1.9680...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3188...  Training loss: 2.0343...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3189...  Training loss: 1.9781...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3190...  Training loss: 2.0219...  0.0591 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3191...  Training loss: 1.9737...  0.0546 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3192...  Training loss: 2.0778...  0.0586 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3193...  Training loss: 1.9814...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3194...  Training loss: 2.0029...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3195...  Training loss: 2.0061...  0.0592 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3196...  Training loss: 2.0210...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3197...  Training loss: 2.0151...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3198...  Training loss: 1.9433...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3199...  Training loss: 2.0604...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3200...  Training loss: 1.9771...  0.0522 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3201...  Training loss: 1.9622...  0.0595 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3202...  Training loss: 1.9741...  0.0591 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3203...  Training loss: 2.0206...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3204...  Training loss: 2.0127...  0.0556 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 3205...  Training loss: 1.9608...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3206...  Training loss: 1.9414...  0.0562 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3207...  Training loss: 2.0121...  0.0574 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3208...  Training loss: 1.9760...  0.0562 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3209...  Training loss: 1.9690...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3210...  Training loss: 1.9673...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3211...  Training loss: 1.9515...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3212...  Training loss: 1.9851...  0.0524 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3213...  Training loss: 1.9911...  0.0569 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3214...  Training loss: 1.9560...  0.0522 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3215...  Training loss: 2.0090...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3216...  Training loss: 2.0090...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3217...  Training loss: 1.9515...  0.0576 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3218...  Training loss: 2.0350...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3219...  Training loss: 1.9690...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3220...  Training loss: 1.9766...  0.0599 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3221...  Training loss: 1.9716...  0.0540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3222...  Training loss: 1.9365...  0.0559 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3223...  Training loss: 1.9715...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3224...  Training loss: 1.9906...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3225...  Training loss: 1.9933...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3226...  Training loss: 2.0105...  0.0542 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3227...  Training loss: 2.0191...  0.0599 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3228...  Training loss: 1.9378...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3229...  Training loss: 1.9688...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3230...  Training loss: 2.0466...  0.0572 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3231...  Training loss: 1.9612...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3232...  Training loss: 2.0275...  0.0547 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3233...  Training loss: 2.0158...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3234...  Training loss: 1.9750...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3235...  Training loss: 1.9522...  0.0584 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3236...  Training loss: 1.9736...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3237...  Training loss: 1.9828...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3238...  Training loss: 1.9883...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3239...  Training loss: 2.0366...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3240...  Training loss: 1.9815...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3241...  Training loss: 2.0117...  0.0524 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3242...  Training loss: 1.9236...  0.0580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3243...  Training loss: 1.9826...  0.0590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3244...  Training loss: 1.9519...  0.0540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3245...  Training loss: 1.9442...  0.0571 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3246...  Training loss: 1.9947...  0.0523 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3247...  Training loss: 1.9882...  0.0587 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3248...  Training loss: 1.9983...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3249...  Training loss: 1.9671...  0.0584 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3250...  Training loss: 2.0166...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3251...  Training loss: 2.0017...  0.0544 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3252...  Training loss: 1.9845...  0.0546 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3253...  Training loss: 1.9662...  0.0524 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3254...  Training loss: 2.0098...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3255...  Training loss: 1.9707...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3256...  Training loss: 2.0097...  0.0592 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3257...  Training loss: 1.9604...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3258...  Training loss: 2.0264...  0.0563 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3259...  Training loss: 2.0239...  0.0567 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3260...  Training loss: 1.9642...  0.0525 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3261...  Training loss: 1.9363...  0.0599 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3262...  Training loss: 1.9665...  0.0566 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3263...  Training loss: 1.9903...  0.0564 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3264...  Training loss: 1.9814...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3265...  Training loss: 1.9879...  0.0542 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3266...  Training loss: 1.9784...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3267...  Training loss: 2.0123...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3268...  Training loss: 1.9690...  0.0520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3269...  Training loss: 1.9540...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3270...  Training loss: 1.9331...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3271...  Training loss: 1.9525...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3272...  Training loss: 1.9712...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3273...  Training loss: 1.9816...  0.0569 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3274...  Training loss: 1.9628...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3275...  Training loss: 1.9636...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3276...  Training loss: 1.9883...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3277...  Training loss: 1.9949...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3278...  Training loss: 1.9541...  0.0580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3279...  Training loss: 1.9496...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3280...  Training loss: 1.9710...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3281...  Training loss: 1.9717...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3282...  Training loss: 1.9977...  0.0587 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3283...  Training loss: 1.9995...  0.0573 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3284...  Training loss: 1.9240...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3285...  Training loss: 1.9311...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3286...  Training loss: 1.9814...  0.0564 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3287...  Training loss: 1.9665...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3288...  Training loss: 1.9848...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3289...  Training loss: 1.9704...  0.0523 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3290...  Training loss: 2.0602...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3291...  Training loss: 2.0068...  0.0594 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3292...  Training loss: 2.0447...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3293...  Training loss: 2.0061...  0.0572 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3294...  Training loss: 1.9458...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3295...  Training loss: 1.9732...  0.0522 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3296...  Training loss: 2.0709...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3297...  Training loss: 1.9819...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3298...  Training loss: 2.0502...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3299...  Training loss: 1.9620...  0.0549 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3300...  Training loss: 1.9864...  0.0544 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3301...  Training loss: 1.9724...  0.0587 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3302...  Training loss: 1.9761...  0.0583 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3303...  Training loss: 1.9741...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3304...  Training loss: 1.9644...  0.0555 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 3305...  Training loss: 1.9875...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3306...  Training loss: 1.9304...  0.0594 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3307...  Training loss: 1.9920...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3308...  Training loss: 1.9792...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3309...  Training loss: 1.9816...  0.0564 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3310...  Training loss: 1.9689...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3311...  Training loss: 1.9778...  0.0535 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3312...  Training loss: 1.9818...  0.0571 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3313...  Training loss: 2.0187...  0.0574 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3314...  Training loss: 2.0197...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3315...  Training loss: 1.9909...  0.0579 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3316...  Training loss: 2.0191...  0.0580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3317...  Training loss: 1.9881...  0.0562 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3318...  Training loss: 1.9407...  0.0593 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3319...  Training loss: 2.0720...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3320...  Training loss: 2.0031...  0.0534 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3321...  Training loss: 1.9656...  0.0577 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3322...  Training loss: 1.9988...  0.0537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3323...  Training loss: 2.0572...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3324...  Training loss: 1.9420...  0.0594 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3325...  Training loss: 1.9702...  0.0571 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3326...  Training loss: 2.0195...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3327...  Training loss: 2.0209...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3328...  Training loss: 1.9457...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3329...  Training loss: 1.9999...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3330...  Training loss: 1.9752...  0.0524 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3331...  Training loss: 2.0266...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3332...  Training loss: 1.9758...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3333...  Training loss: 1.9414...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3334...  Training loss: 1.9511...  0.0609 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3335...  Training loss: 1.9596...  0.0535 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3336...  Training loss: 2.0223...  0.0525 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3337...  Training loss: 1.9772...  0.0536 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3338...  Training loss: 1.9481...  0.0549 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3339...  Training loss: 1.9701...  0.0546 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3340...  Training loss: 2.0145...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3341...  Training loss: 1.9755...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3342...  Training loss: 1.9604...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3343...  Training loss: 1.9430...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3344...  Training loss: 1.9725...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3345...  Training loss: 1.9794...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3346...  Training loss: 1.9619...  0.0561 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3347...  Training loss: 1.9918...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3348...  Training loss: 2.0065...  0.0547 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3349...  Training loss: 1.9318...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3350...  Training loss: 1.9612...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3351...  Training loss: 1.9869...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3352...  Training loss: 1.9554...  0.0545 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3353...  Training loss: 1.9971...  0.0587 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3354...  Training loss: 1.9970...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3355...  Training loss: 2.0125...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3356...  Training loss: 1.9732...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3357...  Training loss: 1.9584...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3358...  Training loss: 1.9873...  0.0565 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3359...  Training loss: 1.9885...  0.0582 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3360...  Training loss: 1.9707...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3361...  Training loss: 1.9791...  0.0538 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3362...  Training loss: 1.9545...  0.0562 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3363...  Training loss: 1.9900...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3364...  Training loss: 1.9785...  0.0538 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3365...  Training loss: 1.9937...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3366...  Training loss: 1.9263...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3367...  Training loss: 1.9866...  0.0522 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3368...  Training loss: 1.9966...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3369...  Training loss: 1.9663...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3370...  Training loss: 1.9437...  0.0616 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3371...  Training loss: 1.9215...  0.0561 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3372...  Training loss: 1.9753...  0.0537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3373...  Training loss: 1.9631...  0.0578 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3374...  Training loss: 1.9524...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3375...  Training loss: 1.9829...  0.0592 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3376...  Training loss: 2.0058...  0.0525 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3377...  Training loss: 2.0436...  0.0547 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3378...  Training loss: 1.9872...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3379...  Training loss: 2.0151...  0.0522 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3380...  Training loss: 2.0339...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3381...  Training loss: 1.9749...  0.0540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3382...  Training loss: 1.9130...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3383...  Training loss: 1.9508...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3384...  Training loss: 1.9824...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3385...  Training loss: 1.9584...  0.0597 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3386...  Training loss: 1.9786...  0.0585 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3387...  Training loss: 1.9427...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3388...  Training loss: 1.9523...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3389...  Training loss: 1.9802...  0.0571 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3390...  Training loss: 1.9992...  0.0523 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3391...  Training loss: 1.9595...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3392...  Training loss: 1.9842...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3393...  Training loss: 1.9879...  0.0585 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3394...  Training loss: 1.9904...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3395...  Training loss: 1.9570...  0.0522 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3396...  Training loss: 1.9361...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3397...  Training loss: 2.0037...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3398...  Training loss: 2.0165...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3399...  Training loss: 2.0334...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3400...  Training loss: 2.0005...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3401...  Training loss: 1.9930...  0.0574 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3402...  Training loss: 2.0388...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3403...  Training loss: 1.9308...  0.0576 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3404...  Training loss: 1.9886...  0.0525 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 3405...  Training loss: 1.9800...  0.0594 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3406...  Training loss: 1.9934...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3407...  Training loss: 1.9701...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3408...  Training loss: 1.9606...  0.0567 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3409...  Training loss: 1.9858...  0.0523 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3410...  Training loss: 1.9416...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3411...  Training loss: 1.9347...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3412...  Training loss: 1.9631...  0.0522 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3413...  Training loss: 1.9410...  0.0571 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3414...  Training loss: 1.9355...  0.0543 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3415...  Training loss: 1.9655...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3416...  Training loss: 1.9984...  0.0534 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3417...  Training loss: 1.9393...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3418...  Training loss: 1.9163...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3419...  Training loss: 1.9541...  0.0606 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3420...  Training loss: 2.0169...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3421...  Training loss: 1.9682...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3422...  Training loss: 1.9119...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3423...  Training loss: 1.9892...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3424...  Training loss: 1.9686...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3425...  Training loss: 1.9374...  0.0569 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3426...  Training loss: 1.9484...  0.0545 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3427...  Training loss: 1.9335...  0.0549 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3428...  Training loss: 1.9296...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3429...  Training loss: 1.9808...  0.0549 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3430...  Training loss: 1.9486...  0.0549 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3431...  Training loss: 1.9596...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3432...  Training loss: 1.9510...  0.0524 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3433...  Training loss: 1.9263...  0.0594 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3434...  Training loss: 1.9736...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3435...  Training loss: 1.9886...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3436...  Training loss: 1.9747...  0.0564 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3437...  Training loss: 1.9566...  0.0603 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3438...  Training loss: 1.9411...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3439...  Training loss: 1.9408...  0.0524 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3440...  Training loss: 1.9617...  0.0573 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3441...  Training loss: 1.9938...  0.0578 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3442...  Training loss: 1.9489...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3443...  Training loss: 1.9447...  0.0559 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3444...  Training loss: 1.9449...  0.0574 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3445...  Training loss: 1.9783...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3446...  Training loss: 1.9674...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3447...  Training loss: 1.9732...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3448...  Training loss: 1.9627...  0.0549 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3449...  Training loss: 1.9502...  0.0561 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3450...  Training loss: 1.9386...  0.0546 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3451...  Training loss: 1.9956...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3452...  Training loss: 1.9521...  0.0524 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3453...  Training loss: 1.9836...  0.0581 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3454...  Training loss: 1.9530...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3455...  Training loss: 1.9534...  0.0597 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3456...  Training loss: 1.9992...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3457...  Training loss: 2.0438...  0.0559 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3458...  Training loss: 2.0099...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3459...  Training loss: 1.9808...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3460...  Training loss: 1.9752...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3461...  Training loss: 2.0029...  0.0564 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3462...  Training loss: 1.9405...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3463...  Training loss: 1.9045...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3464...  Training loss: 1.9070...  0.0580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3465...  Training loss: 1.9423...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3466...  Training loss: 1.9616...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3467...  Training loss: 1.9522...  0.0535 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3468...  Training loss: 2.0281...  0.0564 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3469...  Training loss: 1.9876...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3470...  Training loss: 1.9434...  0.0584 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3471...  Training loss: 1.9973...  0.0572 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3472...  Training loss: 2.0624...  0.0579 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3473...  Training loss: 1.9849...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3474...  Training loss: 1.9658...  0.0592 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3475...  Training loss: 1.9725...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3476...  Training loss: 1.9816...  0.0596 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3477...  Training loss: 1.9383...  0.0561 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3478...  Training loss: 2.0130...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3479...  Training loss: 1.9736...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3480...  Training loss: 1.9585...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3481...  Training loss: 1.8936...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3482...  Training loss: 2.0209...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3483...  Training loss: 1.9288...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3484...  Training loss: 1.9695...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3485...  Training loss: 1.9749...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3486...  Training loss: 1.8960...  0.0534 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3487...  Training loss: 1.9122...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3488...  Training loss: 1.9620...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3489...  Training loss: 1.8968...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3490...  Training loss: 1.9453...  0.0586 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3491...  Training loss: 1.9977...  0.0573 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3492...  Training loss: 1.9027...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3493...  Training loss: 1.9709...  0.0594 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3494...  Training loss: 1.9788...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3495...  Training loss: 1.9433...  0.0593 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3496...  Training loss: 1.9668...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3497...  Training loss: 1.9643...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3498...  Training loss: 1.9876...  0.0585 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3499...  Training loss: 1.9470...  0.0564 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3500...  Training loss: 1.9687...  0.0566 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3501...  Training loss: 2.0083...  0.0538 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3502...  Training loss: 1.9589...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3503...  Training loss: 1.9374...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3504...  Training loss: 2.0083...  0.0527 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 3505...  Training loss: 1.9954...  0.0579 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3506...  Training loss: 2.0022...  0.0567 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3507...  Training loss: 2.0255...  0.0536 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3508...  Training loss: 1.9680...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3509...  Training loss: 2.0289...  0.0525 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3510...  Training loss: 1.9927...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3511...  Training loss: 1.9526...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3512...  Training loss: 1.9846...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3513...  Training loss: 1.9684...  0.0612 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3514...  Training loss: 1.9504...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3515...  Training loss: 1.9201...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3516...  Training loss: 1.9101...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3517...  Training loss: 1.9676...  0.0602 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3518...  Training loss: 1.9720...  0.0520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3519...  Training loss: 2.0032...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3520...  Training loss: 1.9628...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3521...  Training loss: 1.9307...  0.0537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3522...  Training loss: 1.9424...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3523...  Training loss: 1.9837...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3524...  Training loss: 1.9466...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3525...  Training loss: 1.9829...  0.0534 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3526...  Training loss: 1.9926...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3527...  Training loss: 1.9456...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3528...  Training loss: 1.9663...  0.0562 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3529...  Training loss: 1.9668...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3530...  Training loss: 1.9227...  0.0543 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3531...  Training loss: 1.9524...  0.0562 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3532...  Training loss: 2.0087...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3533...  Training loss: 1.9767...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3534...  Training loss: 1.9270...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3535...  Training loss: 1.9501...  0.0564 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3536...  Training loss: 1.9709...  0.0604 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3537...  Training loss: 1.9164...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3538...  Training loss: 1.9596...  0.0588 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3539...  Training loss: 1.9862...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3540...  Training loss: 1.9349...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3541...  Training loss: 1.9508...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3542...  Training loss: 1.9404...  0.0578 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3543...  Training loss: 1.9981...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3544...  Training loss: 1.9497...  0.0598 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3545...  Training loss: 1.9076...  0.0539 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3546...  Training loss: 1.9505...  0.0584 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3547...  Training loss: 1.9271...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3548...  Training loss: 1.9134...  0.0538 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3549...  Training loss: 1.9616...  0.0572 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3550...  Training loss: 1.9940...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3551...  Training loss: 2.0154...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3552...  Training loss: 1.9779...  0.0576 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3553...  Training loss: 1.9286...  0.0536 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3554...  Training loss: 1.9470...  0.0522 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3555...  Training loss: 1.9283...  0.0586 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3556...  Training loss: 1.9737...  0.0594 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3557...  Training loss: 1.9546...  0.0536 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3558...  Training loss: 1.9784...  0.0583 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3559...  Training loss: 1.9516...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3560...  Training loss: 1.9573...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3561...  Training loss: 1.9049...  0.0565 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3562...  Training loss: 1.9774...  0.0606 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3563...  Training loss: 1.9415...  0.0534 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3564...  Training loss: 1.9348...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3565...  Training loss: 1.9936...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3566...  Training loss: 1.9588...  0.0561 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3567...  Training loss: 1.9438...  0.0534 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3568...  Training loss: 1.9305...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3569...  Training loss: 1.9741...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3570...  Training loss: 1.9599...  0.0521 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3571...  Training loss: 1.9690...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3572...  Training loss: 1.9534...  0.0581 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3573...  Training loss: 1.9233...  0.0575 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3574...  Training loss: 1.9060...  0.0588 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3575...  Training loss: 2.0077...  0.0534 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3576...  Training loss: 1.9991...  0.0584 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3577...  Training loss: 1.9693...  0.0582 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3578...  Training loss: 2.0200...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3579...  Training loss: 1.9299...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3580...  Training loss: 2.0111...  0.0617 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3581...  Training loss: 1.9612...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3582...  Training loss: 1.9424...  0.0580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3583...  Training loss: 1.9988...  0.0536 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3584...  Training loss: 1.9779...  0.0537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3585...  Training loss: 2.0188...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3586...  Training loss: 1.9464...  0.0563 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3587...  Training loss: 1.9845...  0.0563 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3588...  Training loss: 1.9532...  0.0582 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3589...  Training loss: 1.9697...  0.0596 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3590...  Training loss: 1.9574...  0.0524 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3591...  Training loss: 1.9442...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3592...  Training loss: 1.9976...  0.0567 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3593...  Training loss: 1.9605...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3594...  Training loss: 1.9565...  0.0561 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3595...  Training loss: 1.9316...  0.0537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3596...  Training loss: 1.9785...  0.0566 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3597...  Training loss: 1.9558...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3598...  Training loss: 1.9805...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3599...  Training loss: 1.8859...  0.0538 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3600...  Training loss: 1.9686...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3601...  Training loss: 1.9745...  0.0609 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3602...  Training loss: 1.9943...  0.0567 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3603...  Training loss: 2.0341...  0.0580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3604...  Training loss: 1.9763...  0.0530 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 3605...  Training loss: 1.9609...  0.0539 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3606...  Training loss: 1.9257...  0.0545 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3607...  Training loss: 1.9763...  0.0537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3608...  Training loss: 1.9355...  0.0617 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3609...  Training loss: 1.9729...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3610...  Training loss: 1.9913...  0.0614 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3611...  Training loss: 1.9919...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3612...  Training loss: 1.9685...  0.0609 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3613...  Training loss: 2.0127...  0.0573 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3614...  Training loss: 1.9897...  0.0541 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3615...  Training loss: 1.9864...  0.0543 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3616...  Training loss: 1.9472...  0.0568 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3617...  Training loss: 1.9447...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3618...  Training loss: 1.9609...  0.0585 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3619...  Training loss: 1.9420...  0.0541 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3620...  Training loss: 1.9385...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3621...  Training loss: 1.9312...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3622...  Training loss: 1.9205...  0.0566 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3623...  Training loss: 1.9531...  0.0562 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3624...  Training loss: 1.8564...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3625...  Training loss: 1.9660...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3626...  Training loss: 2.0020...  0.0580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3627...  Training loss: 2.0006...  0.0562 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3628...  Training loss: 1.9821...  0.0534 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3629...  Training loss: 1.9617...  0.0566 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3630...  Training loss: 1.9193...  0.0534 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3631...  Training loss: 1.9431...  0.0579 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3632...  Training loss: 1.9253...  0.0561 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3633...  Training loss: 1.9657...  0.0562 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3634...  Training loss: 1.9467...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3635...  Training loss: 1.9661...  0.0588 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3636...  Training loss: 1.9602...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3637...  Training loss: 1.9282...  0.0579 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3638...  Training loss: 1.9953...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3639...  Training loss: 1.8879...  0.0566 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3640...  Training loss: 1.9860...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3641...  Training loss: 1.9766...  0.0580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3642...  Training loss: 1.9240...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3643...  Training loss: 1.9365...  0.0562 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3644...  Training loss: 1.9276...  0.0561 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3645...  Training loss: 1.9856...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3646...  Training loss: 1.9687...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3647...  Training loss: 1.9634...  0.0574 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3648...  Training loss: 2.0113...  0.0525 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3649...  Training loss: 2.0036...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3650...  Training loss: 1.9318...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3651...  Training loss: 1.9736...  0.0537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3652...  Training loss: 1.9084...  0.0525 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3653...  Training loss: 1.9458...  0.0535 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3654...  Training loss: 1.9607...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3655...  Training loss: 1.9286...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3656...  Training loss: 1.9388...  0.0596 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3657...  Training loss: 1.9603...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3658...  Training loss: 1.9314...  0.0587 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3659...  Training loss: 1.9579...  0.0536 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3660...  Training loss: 1.9612...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3661...  Training loss: 1.9739...  0.0589 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3662...  Training loss: 1.9714...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3663...  Training loss: 1.9675...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3664...  Training loss: 2.0092...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3665...  Training loss: 2.0537...  0.0535 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3666...  Training loss: 2.0097...  0.0569 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3667...  Training loss: 1.9553...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3668...  Training loss: 2.0471...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3669...  Training loss: 1.9585...  0.0566 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3670...  Training loss: 1.9683...  0.0581 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3671...  Training loss: 1.9634...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3672...  Training loss: 2.0173...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3673...  Training loss: 1.9538...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3674...  Training loss: 1.9391...  0.0564 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3675...  Training loss: 1.9399...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3676...  Training loss: 2.0039...  0.0578 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3677...  Training loss: 1.9188...  0.0546 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3678...  Training loss: 1.9657...  0.0533 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3679...  Training loss: 2.0136...  0.0575 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3680...  Training loss: 1.9792...  0.0534 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3681...  Training loss: 1.9867...  0.0545 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3682...  Training loss: 1.9740...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3683...  Training loss: 1.9512...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3684...  Training loss: 2.0035...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3685...  Training loss: 1.9513...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3686...  Training loss: 1.9715...  0.0525 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3687...  Training loss: 1.9384...  0.0579 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3688...  Training loss: 1.9749...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3689...  Training loss: 1.9262...  0.0557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3690...  Training loss: 1.9877...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3691...  Training loss: 1.9121...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3692...  Training loss: 1.9807...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3693...  Training loss: 1.9263...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3694...  Training loss: 1.9271...  0.0528 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3695...  Training loss: 1.9325...  0.0576 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3696...  Training loss: 1.9406...  0.0529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3697...  Training loss: 1.9001...  0.0595 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3698...  Training loss: 1.9517...  0.0588 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3699...  Training loss: 1.9482...  0.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3700...  Training loss: 1.9706...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3701...  Training loss: 1.9504...  0.0579 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3702...  Training loss: 1.9728...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3703...  Training loss: 1.9818...  0.0540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3704...  Training loss: 1.9205...  0.0550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 3705...  Training loss: 1.9608...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3706...  Training loss: 1.9411...  0.0567 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3707...  Training loss: 1.9102...  0.0542 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3708...  Training loss: 1.9358...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3709...  Training loss: 1.9367...  0.0576 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3710...  Training loss: 1.9849...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3711...  Training loss: 1.9951...  0.0556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3712...  Training loss: 1.9752...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3713...  Training loss: 1.9099...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3714...  Training loss: 1.9803...  0.0552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3715...  Training loss: 1.9248...  0.0566 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3716...  Training loss: 1.9783...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3717...  Training loss: 1.9824...  0.0577 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3718...  Training loss: 1.9279...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3719...  Training loss: 1.9134...  0.0563 sec/batch\n",
      "Epoch: 6/20...  Training Step: 3720...  Training loss: 1.9245...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3721...  Training loss: 2.0344...  0.0562 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3722...  Training loss: 2.0289...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3723...  Training loss: 1.9909...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3724...  Training loss: 1.9216...  0.0571 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3725...  Training loss: 1.9576...  0.0564 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3726...  Training loss: 1.9338...  0.0552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3727...  Training loss: 1.9282...  0.0584 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3728...  Training loss: 1.9088...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3729...  Training loss: 1.8917...  0.0551 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3730...  Training loss: 1.9245...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3731...  Training loss: 1.9586...  0.0558 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3732...  Training loss: 1.9001...  0.0564 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3733...  Training loss: 1.9746...  0.0624 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3734...  Training loss: 1.9341...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3735...  Training loss: 1.9608...  0.0597 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3736...  Training loss: 1.9940...  0.0592 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3737...  Training loss: 1.9817...  0.0562 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3738...  Training loss: 1.9654...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3739...  Training loss: 1.9123...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3740...  Training loss: 1.9583...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3741...  Training loss: 2.0153...  0.0525 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3742...  Training loss: 1.9378...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3743...  Training loss: 1.9244...  0.0573 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3744...  Training loss: 1.9429...  0.0576 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3745...  Training loss: 1.9383...  0.0605 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3746...  Training loss: 1.9321...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3747...  Training loss: 1.9415...  0.0586 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3748...  Training loss: 1.9541...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3749...  Training loss: 1.9517...  0.0563 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3750...  Training loss: 1.9005...  0.0561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3751...  Training loss: 1.9104...  0.0536 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3752...  Training loss: 1.9660...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3753...  Training loss: 1.9389...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3754...  Training loss: 1.9194...  0.0588 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3755...  Training loss: 1.9333...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3756...  Training loss: 1.9596...  0.0526 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3757...  Training loss: 1.9461...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3758...  Training loss: 1.9360...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3759...  Training loss: 1.9526...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3760...  Training loss: 1.9057...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3761...  Training loss: 1.9389...  0.0541 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3762...  Training loss: 1.9802...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3763...  Training loss: 1.9445...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3764...  Training loss: 1.9662...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3765...  Training loss: 1.9510...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3766...  Training loss: 1.9256...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3767...  Training loss: 1.8464...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3768...  Training loss: 1.9472...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3769...  Training loss: 1.9298...  0.0567 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3770...  Training loss: 1.9586...  0.0536 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3771...  Training loss: 1.9129...  0.0563 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3772...  Training loss: 1.9134...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3773...  Training loss: 1.9544...  0.0542 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3774...  Training loss: 1.9680...  0.0576 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3775...  Training loss: 1.9766...  0.0561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3776...  Training loss: 1.9311...  0.0563 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3777...  Training loss: 1.9047...  0.0536 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3778...  Training loss: 1.9470...  0.0605 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3779...  Training loss: 1.9351...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3780...  Training loss: 1.9927...  0.0558 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3781...  Training loss: 1.9525...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3782...  Training loss: 1.9270...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3783...  Training loss: 1.9584...  0.0538 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3784...  Training loss: 1.9136...  0.0585 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3785...  Training loss: 1.9040...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3786...  Training loss: 1.8601...  0.0602 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3787...  Training loss: 1.9095...  0.0601 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3788...  Training loss: 1.9112...  0.0540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3789...  Training loss: 1.9741...  0.0541 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3790...  Training loss: 1.9447...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3791...  Training loss: 1.9800...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3792...  Training loss: 1.9580...  0.0562 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3793...  Training loss: 1.9014...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3794...  Training loss: 1.9343...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3795...  Training loss: 1.9748...  0.0568 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3796...  Training loss: 1.9630...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3797...  Training loss: 1.9560...  0.0541 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3798...  Training loss: 1.9266...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3799...  Training loss: 1.9327...  0.0566 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3800...  Training loss: 1.9571...  0.0587 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3801...  Training loss: 1.9100...  0.0602 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3802...  Training loss: 1.9412...  0.0564 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3803...  Training loss: 1.8936...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3804...  Training loss: 1.9399...  0.0537 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 3805...  Training loss: 1.9291...  0.0573 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3806...  Training loss: 1.9763...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3807...  Training loss: 1.8999...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3808...  Training loss: 1.9807...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3809...  Training loss: 1.9626...  0.0545 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3810...  Training loss: 1.9556...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3811...  Training loss: 1.9387...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3812...  Training loss: 1.9844...  0.0551 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3813...  Training loss: 1.9573...  0.0607 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3814...  Training loss: 1.9349...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3815...  Training loss: 1.9532...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3816...  Training loss: 1.9652...  0.0536 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3817...  Training loss: 1.9670...  0.0603 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3818...  Training loss: 1.8988...  0.0569 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3819...  Training loss: 1.9973...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3820...  Training loss: 1.9161...  0.0588 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3821...  Training loss: 1.9365...  0.0566 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3822...  Training loss: 1.9275...  0.0591 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3823...  Training loss: 1.9779...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3824...  Training loss: 1.9629...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3825...  Training loss: 1.9445...  0.0598 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3826...  Training loss: 1.8939...  0.0527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3827...  Training loss: 1.9560...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3828...  Training loss: 1.9440...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3829...  Training loss: 1.9171...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3830...  Training loss: 1.9006...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3831...  Training loss: 1.9121...  0.0596 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3832...  Training loss: 1.9232...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3833...  Training loss: 1.9492...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3834...  Training loss: 1.9282...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3835...  Training loss: 1.9480...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3836...  Training loss: 1.9632...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3837...  Training loss: 1.8951...  0.0539 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3838...  Training loss: 1.9696...  0.0536 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3839...  Training loss: 1.9198...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3840...  Training loss: 1.9238...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3841...  Training loss: 1.9007...  0.0588 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3842...  Training loss: 1.8839...  0.0578 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3843...  Training loss: 1.9042...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3844...  Training loss: 1.9524...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3845...  Training loss: 1.9395...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3846...  Training loss: 1.9648...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3847...  Training loss: 1.9856...  0.0573 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3848...  Training loss: 1.9010...  0.0573 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3849...  Training loss: 1.9196...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3850...  Training loss: 1.9936...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3851...  Training loss: 1.9265...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3852...  Training loss: 1.9781...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3853...  Training loss: 1.9823...  0.0611 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3854...  Training loss: 1.9249...  0.0536 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3855...  Training loss: 1.8934...  0.0597 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3856...  Training loss: 1.9335...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3857...  Training loss: 1.9318...  0.0539 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3858...  Training loss: 1.9381...  0.0561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3859...  Training loss: 1.9899...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3860...  Training loss: 1.9213...  0.0567 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3861...  Training loss: 1.9620...  0.0538 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3862...  Training loss: 1.8480...  0.0537 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3863...  Training loss: 1.9492...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3864...  Training loss: 1.9168...  0.0563 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3865...  Training loss: 1.9104...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3866...  Training loss: 1.9741...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3867...  Training loss: 1.9522...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3868...  Training loss: 1.9449...  0.0539 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3869...  Training loss: 1.9332...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3870...  Training loss: 1.9557...  0.0600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3871...  Training loss: 1.9549...  0.0547 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3872...  Training loss: 1.9104...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3873...  Training loss: 1.9215...  0.0603 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3874...  Training loss: 1.9661...  0.0564 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3875...  Training loss: 1.9285...  0.0558 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3876...  Training loss: 1.9551...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3877...  Training loss: 1.9296...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3878...  Training loss: 1.9615...  0.0584 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3879...  Training loss: 1.9610...  0.0563 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3880...  Training loss: 1.8864...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3881...  Training loss: 1.8955...  0.0546 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3882...  Training loss: 1.8905...  0.0562 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3883...  Training loss: 1.9326...  0.0537 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3884...  Training loss: 1.9268...  0.0541 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3885...  Training loss: 1.9579...  0.0552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3886...  Training loss: 1.9395...  0.0536 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3887...  Training loss: 1.9532...  0.0558 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3888...  Training loss: 1.9311...  0.0538 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3889...  Training loss: 1.9301...  0.0612 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3890...  Training loss: 1.8973...  0.0552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3891...  Training loss: 1.9120...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3892...  Training loss: 1.9308...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3893...  Training loss: 1.9377...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3894...  Training loss: 1.9355...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3895...  Training loss: 1.9091...  0.0562 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3896...  Training loss: 1.9392...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3897...  Training loss: 1.9532...  0.0537 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3898...  Training loss: 1.9304...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3899...  Training loss: 1.9091...  0.0543 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3900...  Training loss: 1.9192...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3901...  Training loss: 1.9063...  0.0571 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3902...  Training loss: 1.9421...  0.0586 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3903...  Training loss: 1.9507...  0.0566 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3904...  Training loss: 1.8890...  0.0551 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 3905...  Training loss: 1.8843...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3906...  Training loss: 1.9265...  0.0542 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3907...  Training loss: 1.9227...  0.0591 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3908...  Training loss: 1.9197...  0.0600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3909...  Training loss: 1.9266...  0.0603 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3910...  Training loss: 2.0080...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3911...  Training loss: 1.9616...  0.0603 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3912...  Training loss: 1.9719...  0.0584 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3913...  Training loss: 1.9604...  0.0558 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3914...  Training loss: 1.9139...  0.0591 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3915...  Training loss: 1.9320...  0.0566 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3916...  Training loss: 2.0060...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3917...  Training loss: 1.9285...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3918...  Training loss: 2.0011...  0.0564 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3919...  Training loss: 1.9002...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3920...  Training loss: 1.9498...  0.0565 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3921...  Training loss: 1.9298...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3922...  Training loss: 1.9056...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3923...  Training loss: 1.9312...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3924...  Training loss: 1.9141...  0.0601 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3925...  Training loss: 1.9340...  0.0543 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3926...  Training loss: 1.8836...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3927...  Training loss: 1.9444...  0.0567 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3928...  Training loss: 1.9448...  0.0593 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3929...  Training loss: 1.9177...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3930...  Training loss: 1.9200...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3931...  Training loss: 1.9374...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3932...  Training loss: 1.9391...  0.0591 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3933...  Training loss: 1.9540...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3934...  Training loss: 1.9608...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3935...  Training loss: 1.9609...  0.0551 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3936...  Training loss: 1.9499...  0.0575 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3937...  Training loss: 1.9329...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3938...  Training loss: 1.9078...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3939...  Training loss: 2.0275...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3940...  Training loss: 1.9608...  0.0524 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3941...  Training loss: 1.9446...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3942...  Training loss: 1.9502...  0.0564 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3943...  Training loss: 1.9774...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3944...  Training loss: 1.9121...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3945...  Training loss: 1.9225...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3946...  Training loss: 1.9739...  0.0597 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3947...  Training loss: 1.9893...  0.0581 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3948...  Training loss: 1.9006...  0.0561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3949...  Training loss: 1.9696...  0.0561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3950...  Training loss: 1.9108...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3951...  Training loss: 1.9966...  0.0585 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3952...  Training loss: 1.9157...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3953...  Training loss: 1.9097...  0.0546 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3954...  Training loss: 1.9060...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3955...  Training loss: 1.9163...  0.0594 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3956...  Training loss: 1.9677...  0.0581 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3957...  Training loss: 1.9313...  0.0580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3958...  Training loss: 1.8769...  0.0567 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3959...  Training loss: 1.9119...  0.0564 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3960...  Training loss: 1.9623...  0.0606 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3961...  Training loss: 1.9336...  0.0570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3962...  Training loss: 1.9117...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3963...  Training loss: 1.8996...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3964...  Training loss: 1.9116...  0.0593 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3965...  Training loss: 1.9310...  0.0561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3966...  Training loss: 1.9500...  0.0525 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3967...  Training loss: 1.9493...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3968...  Training loss: 1.9326...  0.0578 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3969...  Training loss: 1.8944...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3970...  Training loss: 1.9259...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3971...  Training loss: 1.9228...  0.0539 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3972...  Training loss: 1.9181...  0.0577 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3973...  Training loss: 1.9320...  0.0558 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3974...  Training loss: 1.9440...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3975...  Training loss: 1.9562...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3976...  Training loss: 1.9282...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3977...  Training loss: 1.9306...  0.0596 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3978...  Training loss: 1.9244...  0.0536 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3979...  Training loss: 1.9356...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3980...  Training loss: 1.9161...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3981...  Training loss: 1.9413...  0.0593 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3982...  Training loss: 1.8942...  0.0600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3983...  Training loss: 1.9438...  0.0546 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3984...  Training loss: 1.9495...  0.0544 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3985...  Training loss: 1.9526...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3986...  Training loss: 1.8792...  0.0544 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3987...  Training loss: 1.9378...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3988...  Training loss: 1.9309...  0.0547 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3989...  Training loss: 1.9252...  0.0540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3990...  Training loss: 1.8910...  0.0552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3991...  Training loss: 1.9026...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3992...  Training loss: 1.9277...  0.0539 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3993...  Training loss: 1.9262...  0.0567 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3994...  Training loss: 1.8943...  0.0538 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3995...  Training loss: 1.9572...  0.0597 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3996...  Training loss: 1.9754...  0.0551 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3997...  Training loss: 2.0107...  0.0575 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3998...  Training loss: 1.9344...  0.0595 sec/batch\n",
      "Epoch: 7/20...  Training Step: 3999...  Training loss: 1.9699...  0.0575 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4000...  Training loss: 1.9763...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4001...  Training loss: 1.9431...  0.0562 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4002...  Training loss: 1.8756...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4003...  Training loss: 1.9051...  0.0524 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4004...  Training loss: 1.9348...  0.0535 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 4005...  Training loss: 1.9221...  0.0574 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4006...  Training loss: 1.9557...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4007...  Training loss: 1.9204...  0.0584 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4008...  Training loss: 1.9238...  0.0576 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4009...  Training loss: 1.9436...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4010...  Training loss: 1.9730...  0.0589 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4011...  Training loss: 1.9234...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4012...  Training loss: 1.9406...  0.0522 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4013...  Training loss: 1.9437...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4014...  Training loss: 1.9469...  0.0570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4015...  Training loss: 1.9084...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4016...  Training loss: 1.8885...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4017...  Training loss: 1.9503...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4018...  Training loss: 1.9659...  0.0598 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4019...  Training loss: 1.9585...  0.0527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4020...  Training loss: 1.9255...  0.0552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4021...  Training loss: 1.9606...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4022...  Training loss: 2.0004...  0.0537 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4023...  Training loss: 1.8760...  0.0629 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4024...  Training loss: 1.9625...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4025...  Training loss: 1.9276...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4026...  Training loss: 1.9138...  0.0579 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4027...  Training loss: 1.9226...  0.0525 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4028...  Training loss: 1.9067...  0.0564 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4029...  Training loss: 1.9341...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4030...  Training loss: 1.9005...  0.0600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4031...  Training loss: 1.9035...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4032...  Training loss: 1.9009...  0.0551 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4033...  Training loss: 1.9159...  0.0563 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4034...  Training loss: 1.9002...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4035...  Training loss: 1.9346...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4036...  Training loss: 1.9531...  0.0552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4037...  Training loss: 1.8989...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4038...  Training loss: 1.8784...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4039...  Training loss: 1.9164...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4040...  Training loss: 1.9785...  0.0527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4041...  Training loss: 1.9149...  0.0588 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4042...  Training loss: 1.8842...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4043...  Training loss: 1.9366...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4044...  Training loss: 1.9193...  0.0575 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4045...  Training loss: 1.8741...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4046...  Training loss: 1.9133...  0.0524 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4047...  Training loss: 1.9013...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4048...  Training loss: 1.8757...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4049...  Training loss: 1.9266...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4050...  Training loss: 1.8948...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4051...  Training loss: 1.9278...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4052...  Training loss: 1.8970...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4053...  Training loss: 1.8871...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4054...  Training loss: 1.9143...  0.0563 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4055...  Training loss: 1.9327...  0.0561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4056...  Training loss: 1.9207...  0.0574 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4057...  Training loss: 1.9049...  0.0562 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4058...  Training loss: 1.8874...  0.0558 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4059...  Training loss: 1.9080...  0.0538 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4060...  Training loss: 1.9151...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4061...  Training loss: 1.9167...  0.0527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4062...  Training loss: 1.9048...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4063...  Training loss: 1.9001...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4064...  Training loss: 1.8954...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4065...  Training loss: 1.9128...  0.0584 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4066...  Training loss: 1.9307...  0.0524 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4067...  Training loss: 1.9268...  0.0584 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4068...  Training loss: 1.9164...  0.0526 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4069...  Training loss: 1.9178...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4070...  Training loss: 1.9048...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4071...  Training loss: 1.9557...  0.0571 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4072...  Training loss: 1.9017...  0.0525 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4073...  Training loss: 1.9356...  0.0551 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4074...  Training loss: 1.9080...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4075...  Training loss: 1.8922...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4076...  Training loss: 1.9740...  0.0552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4077...  Training loss: 2.0151...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4078...  Training loss: 1.9641...  0.0552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4079...  Training loss: 1.9318...  0.0562 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4080...  Training loss: 1.9346...  0.0540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4081...  Training loss: 1.9499...  0.0591 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4082...  Training loss: 1.9015...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4083...  Training loss: 1.8734...  0.0540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4084...  Training loss: 1.8550...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4085...  Training loss: 1.9083...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4086...  Training loss: 1.9308...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4087...  Training loss: 1.8909...  0.0578 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4088...  Training loss: 1.9663...  0.0584 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4089...  Training loss: 1.9359...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4090...  Training loss: 1.8979...  0.0588 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4091...  Training loss: 1.9258...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4092...  Training loss: 1.9995...  0.0537 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4093...  Training loss: 1.9409...  0.0608 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4094...  Training loss: 1.9283...  0.0539 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4095...  Training loss: 1.9043...  0.0577 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4096...  Training loss: 1.9274...  0.0566 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4097...  Training loss: 1.8930...  0.0582 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4098...  Training loss: 1.9765...  0.0563 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4099...  Training loss: 1.9364...  0.0545 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4100...  Training loss: 1.9101...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4101...  Training loss: 1.8509...  0.0570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4102...  Training loss: 1.9809...  0.0586 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4103...  Training loss: 1.8841...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4104...  Training loss: 1.9139...  0.0525 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 4105...  Training loss: 1.9260...  0.0562 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4106...  Training loss: 1.8514...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4107...  Training loss: 1.8635...  0.0566 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4108...  Training loss: 1.9241...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4109...  Training loss: 1.8634...  0.0537 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4110...  Training loss: 1.8984...  0.0586 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4111...  Training loss: 1.9464...  0.0565 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4112...  Training loss: 1.8847...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4113...  Training loss: 1.9233...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4114...  Training loss: 1.9368...  0.0526 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4115...  Training loss: 1.8916...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4116...  Training loss: 1.9291...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4117...  Training loss: 1.9004...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4118...  Training loss: 1.9284...  0.0580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4119...  Training loss: 1.9071...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4120...  Training loss: 1.9325...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4121...  Training loss: 1.9386...  0.0565 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4122...  Training loss: 1.9174...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4123...  Training loss: 1.8999...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4124...  Training loss: 1.9452...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4125...  Training loss: 1.9734...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4126...  Training loss: 1.9422...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4127...  Training loss: 1.9829...  0.0526 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4128...  Training loss: 1.9234...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4129...  Training loss: 1.9828...  0.0524 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4130...  Training loss: 1.9409...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4131...  Training loss: 1.9099...  0.0633 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4132...  Training loss: 1.9580...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4133...  Training loss: 1.9350...  0.0573 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4134...  Training loss: 1.9124...  0.0566 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4135...  Training loss: 1.8706...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4136...  Training loss: 1.8774...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4137...  Training loss: 1.9242...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4138...  Training loss: 1.9299...  0.0540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4139...  Training loss: 1.9564...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4140...  Training loss: 1.9264...  0.0526 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4141...  Training loss: 1.8994...  0.0526 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4142...  Training loss: 1.9018...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4143...  Training loss: 1.9319...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4144...  Training loss: 1.9144...  0.0592 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4145...  Training loss: 1.9718...  0.0586 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4146...  Training loss: 1.9519...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4147...  Training loss: 1.8797...  0.0526 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4148...  Training loss: 1.9085...  0.0545 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4149...  Training loss: 1.8964...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4150...  Training loss: 1.8858...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4151...  Training loss: 1.9111...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4152...  Training loss: 1.9596...  0.0594 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4153...  Training loss: 1.9578...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4154...  Training loss: 1.8899...  0.0552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4155...  Training loss: 1.8923...  0.0597 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4156...  Training loss: 1.9005...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4157...  Training loss: 1.8891...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4158...  Training loss: 1.9105...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4159...  Training loss: 1.9413...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4160...  Training loss: 1.9025...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4161...  Training loss: 1.9129...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4162...  Training loss: 1.9231...  0.0548 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4163...  Training loss: 1.9556...  0.0547 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4164...  Training loss: 1.9371...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4165...  Training loss: 1.8434...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4166...  Training loss: 1.9051...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4167...  Training loss: 1.8733...  0.0569 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4168...  Training loss: 1.8803...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4169...  Training loss: 1.9367...  0.0608 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4170...  Training loss: 1.9629...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4171...  Training loss: 1.9759...  0.0536 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4172...  Training loss: 1.9439...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4173...  Training loss: 1.8849...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4174...  Training loss: 1.9164...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4175...  Training loss: 1.8905...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4176...  Training loss: 1.9264...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4177...  Training loss: 1.8948...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4178...  Training loss: 1.9356...  0.0527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4179...  Training loss: 1.8902...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4180...  Training loss: 1.9209...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4181...  Training loss: 1.8902...  0.0558 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4182...  Training loss: 1.9192...  0.0604 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4183...  Training loss: 1.8880...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4184...  Training loss: 1.8939...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4185...  Training loss: 1.9600...  0.0579 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4186...  Training loss: 1.9367...  0.0525 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4187...  Training loss: 1.8984...  0.0570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4188...  Training loss: 1.9179...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4189...  Training loss: 1.9519...  0.0575 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4190...  Training loss: 1.9188...  0.0580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4191...  Training loss: 1.9061...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4192...  Training loss: 1.9236...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4193...  Training loss: 1.9087...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4194...  Training loss: 1.8537...  0.0558 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4195...  Training loss: 1.9665...  0.0589 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4196...  Training loss: 1.9558...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4197...  Training loss: 1.9372...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4198...  Training loss: 2.0031...  0.0591 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4199...  Training loss: 1.8840...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4200...  Training loss: 1.9761...  0.0548 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4201...  Training loss: 1.9205...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4202...  Training loss: 1.8754...  0.0527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4203...  Training loss: 1.9562...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4204...  Training loss: 1.9405...  0.0584 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 4205...  Training loss: 1.9778...  0.0539 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4206...  Training loss: 1.9174...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4207...  Training loss: 1.9502...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4208...  Training loss: 1.8909...  0.0573 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4209...  Training loss: 1.9269...  0.0565 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4210...  Training loss: 1.9294...  0.0522 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4211...  Training loss: 1.8948...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4212...  Training loss: 1.9329...  0.0524 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4213...  Training loss: 1.9229...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4214...  Training loss: 1.9027...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4215...  Training loss: 1.8971...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4216...  Training loss: 1.9173...  0.0585 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4217...  Training loss: 1.9220...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4218...  Training loss: 1.9444...  0.0561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4219...  Training loss: 1.8528...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4220...  Training loss: 1.9485...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4221...  Training loss: 1.9340...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4222...  Training loss: 1.9504...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4223...  Training loss: 1.9938...  0.0524 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4224...  Training loss: 1.9348...  0.0591 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4225...  Training loss: 1.8924...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4226...  Training loss: 1.8854...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4227...  Training loss: 1.9398...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4228...  Training loss: 1.9050...  0.0576 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4229...  Training loss: 1.9359...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4230...  Training loss: 1.9484...  0.0567 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4231...  Training loss: 1.9607...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4232...  Training loss: 1.9321...  0.0577 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4233...  Training loss: 1.9982...  0.0569 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4234...  Training loss: 1.9462...  0.0522 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4235...  Training loss: 1.9588...  0.0561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4236...  Training loss: 1.9165...  0.0669 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4237...  Training loss: 1.8988...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4238...  Training loss: 1.8899...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4239...  Training loss: 1.8978...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4240...  Training loss: 1.9029...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4241...  Training loss: 1.9145...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4242...  Training loss: 1.8675...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4243...  Training loss: 1.9127...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4244...  Training loss: 1.8155...  0.0571 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4245...  Training loss: 1.9222...  0.0575 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4246...  Training loss: 1.9646...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4247...  Training loss: 1.9614...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4248...  Training loss: 1.9424...  0.0607 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4249...  Training loss: 1.9241...  0.0566 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4250...  Training loss: 1.8940...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4251...  Training loss: 1.9143...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4252...  Training loss: 1.8920...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4253...  Training loss: 1.9114...  0.0593 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4254...  Training loss: 1.8975...  0.0633 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4255...  Training loss: 1.9187...  0.0544 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4256...  Training loss: 1.9441...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4257...  Training loss: 1.8773...  0.0593 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4258...  Training loss: 1.9341...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4259...  Training loss: 1.8618...  0.0525 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4260...  Training loss: 1.9402...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4261...  Training loss: 1.9327...  0.0568 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4262...  Training loss: 1.8759...  0.0547 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4263...  Training loss: 1.8802...  0.0546 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4264...  Training loss: 1.8837...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4265...  Training loss: 1.9293...  0.0561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4266...  Training loss: 1.9157...  0.0527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4267...  Training loss: 1.9461...  0.0561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4268...  Training loss: 1.9529...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4269...  Training loss: 1.9482...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4270...  Training loss: 1.9080...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4271...  Training loss: 1.9239...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4272...  Training loss: 1.8637...  0.0630 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4273...  Training loss: 1.9301...  0.0591 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4274...  Training loss: 1.9436...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4275...  Training loss: 1.9022...  0.0526 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4276...  Training loss: 1.8851...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4277...  Training loss: 1.8913...  0.0594 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4278...  Training loss: 1.8699...  0.0565 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4279...  Training loss: 1.9220...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4280...  Training loss: 1.9053...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4281...  Training loss: 1.9431...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4282...  Training loss: 1.9505...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4283...  Training loss: 1.9276...  0.0528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4284...  Training loss: 1.9935...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4285...  Training loss: 2.0177...  0.0558 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4286...  Training loss: 1.9631...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4287...  Training loss: 1.9266...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4288...  Training loss: 1.9712...  0.0552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4289...  Training loss: 1.9029...  0.0566 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4290...  Training loss: 1.9210...  0.0564 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4291...  Training loss: 1.9228...  0.0576 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4292...  Training loss: 1.9798...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4293...  Training loss: 1.9142...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4294...  Training loss: 1.9038...  0.0555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4295...  Training loss: 1.8989...  0.0526 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4296...  Training loss: 1.9540...  0.0526 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4297...  Training loss: 1.8836...  0.0552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4298...  Training loss: 1.9362...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4299...  Training loss: 1.9937...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4300...  Training loss: 1.9213...  0.0521 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4301...  Training loss: 1.9495...  0.0584 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4302...  Training loss: 1.9222...  0.0527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4303...  Training loss: 1.9390...  0.0559 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4304...  Training loss: 1.9617...  0.0531 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 4305...  Training loss: 1.9402...  0.0581 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4306...  Training loss: 1.9060...  0.0523 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4307...  Training loss: 1.8700...  0.0568 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4308...  Training loss: 1.9274...  0.0588 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4309...  Training loss: 1.8980...  0.0568 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4310...  Training loss: 1.9524...  0.0527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4311...  Training loss: 1.8731...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4312...  Training loss: 1.9576...  0.0533 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4313...  Training loss: 1.8981...  0.0583 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4314...  Training loss: 1.9031...  0.0548 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4315...  Training loss: 1.9003...  0.0588 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4316...  Training loss: 1.8917...  0.0549 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4317...  Training loss: 1.8610...  0.0588 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4318...  Training loss: 1.9117...  0.0565 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4319...  Training loss: 1.9062...  0.0586 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4320...  Training loss: 1.9298...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4321...  Training loss: 1.9248...  0.0527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4322...  Training loss: 1.9206...  0.0527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4323...  Training loss: 1.9193...  0.0578 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4324...  Training loss: 1.8825...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4325...  Training loss: 1.9169...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4326...  Training loss: 1.9070...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4327...  Training loss: 1.8735...  0.0595 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4328...  Training loss: 1.9116...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4329...  Training loss: 1.8968...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4330...  Training loss: 1.9740...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4331...  Training loss: 1.9415...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4332...  Training loss: 1.9347...  0.0598 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4333...  Training loss: 1.8834...  0.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4334...  Training loss: 1.9050...  0.0548 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4335...  Training loss: 1.8720...  0.0557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4336...  Training loss: 1.9348...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4337...  Training loss: 1.9359...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4338...  Training loss: 1.8853...  0.0563 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4339...  Training loss: 1.8761...  0.0548 sec/batch\n",
      "Epoch: 7/20...  Training Step: 4340...  Training loss: 1.8636...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4341...  Training loss: 1.9933...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4342...  Training loss: 1.9805...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4343...  Training loss: 1.9637...  0.0575 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4344...  Training loss: 1.8671...  0.0542 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4345...  Training loss: 1.9032...  0.0535 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4346...  Training loss: 1.9385...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4347...  Training loss: 1.8912...  0.0555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4348...  Training loss: 1.8802...  0.0548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4349...  Training loss: 1.8693...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4350...  Training loss: 1.8869...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4351...  Training loss: 1.9060...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4352...  Training loss: 1.8571...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4353...  Training loss: 1.9328...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4354...  Training loss: 1.9051...  0.0581 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4355...  Training loss: 1.9316...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4356...  Training loss: 1.9462...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4357...  Training loss: 1.9328...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4358...  Training loss: 1.9129...  0.0555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4359...  Training loss: 1.8613...  0.0554 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4360...  Training loss: 1.9212...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4361...  Training loss: 1.9587...  0.0614 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4362...  Training loss: 1.8970...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4363...  Training loss: 1.8762...  0.0524 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4364...  Training loss: 1.9322...  0.0564 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4365...  Training loss: 1.8819...  0.0541 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4366...  Training loss: 1.8748...  0.0535 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4367...  Training loss: 1.9118...  0.0536 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4368...  Training loss: 1.9167...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4369...  Training loss: 1.9226...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4370...  Training loss: 1.8786...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4371...  Training loss: 1.8690...  0.0555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4372...  Training loss: 1.9238...  0.0524 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4373...  Training loss: 1.8959...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4374...  Training loss: 1.8814...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4375...  Training loss: 1.9180...  0.0558 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4376...  Training loss: 1.9022...  0.0588 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4377...  Training loss: 1.8900...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4378...  Training loss: 1.9276...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4379...  Training loss: 1.9161...  0.0567 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4380...  Training loss: 1.8664...  0.0555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4381...  Training loss: 1.8925...  0.0547 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4382...  Training loss: 1.9256...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4383...  Training loss: 1.9026...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4384...  Training loss: 1.9254...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4385...  Training loss: 1.8930...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4386...  Training loss: 1.8933...  0.0558 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4387...  Training loss: 1.7642...  0.0554 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4388...  Training loss: 1.9205...  0.0535 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4389...  Training loss: 1.8826...  0.0606 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4390...  Training loss: 1.8989...  0.0586 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4391...  Training loss: 1.8696...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4392...  Training loss: 1.8816...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4393...  Training loss: 1.9095...  0.0571 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4394...  Training loss: 1.9391...  0.0554 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4395...  Training loss: 1.9238...  0.0569 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4396...  Training loss: 1.9065...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4397...  Training loss: 1.8666...  0.0581 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4398...  Training loss: 1.9063...  0.0602 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4399...  Training loss: 1.8955...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4400...  Training loss: 1.9346...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4401...  Training loss: 1.9108...  0.0594 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4402...  Training loss: 1.8865...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4403...  Training loss: 1.9047...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4404...  Training loss: 1.9042...  0.0589 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 4405...  Training loss: 1.8751...  0.0554 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4406...  Training loss: 1.8489...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4407...  Training loss: 1.8685...  0.0544 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4408...  Training loss: 1.8759...  0.0560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4409...  Training loss: 1.9266...  0.0565 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4410...  Training loss: 1.9049...  0.0581 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4411...  Training loss: 1.9309...  0.0599 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4412...  Training loss: 1.9115...  0.0548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4413...  Training loss: 1.8474...  0.0562 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4414...  Training loss: 1.8941...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4415...  Training loss: 1.9399...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4416...  Training loss: 1.9118...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4417...  Training loss: 1.9138...  0.0567 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4418...  Training loss: 1.8844...  0.0554 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4419...  Training loss: 1.9177...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4420...  Training loss: 1.9315...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4421...  Training loss: 1.8559...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4422...  Training loss: 1.9037...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4423...  Training loss: 1.8460...  0.0536 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4424...  Training loss: 1.8704...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4425...  Training loss: 1.8934...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4426...  Training loss: 1.9460...  0.0565 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4427...  Training loss: 1.8549...  0.0557 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4428...  Training loss: 1.9600...  0.0562 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4429...  Training loss: 1.9089...  0.0534 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4430...  Training loss: 1.9217...  0.0580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4431...  Training loss: 1.8851...  0.0580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4432...  Training loss: 1.9511...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4433...  Training loss: 1.8971...  0.0592 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4434...  Training loss: 1.8905...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4435...  Training loss: 1.9016...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4436...  Training loss: 1.9338...  0.0591 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4437...  Training loss: 1.9215...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4438...  Training loss: 1.8368...  0.0563 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4439...  Training loss: 1.9552...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4440...  Training loss: 1.8846...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4441...  Training loss: 1.8873...  0.0560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4442...  Training loss: 1.8919...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4443...  Training loss: 1.9230...  0.0554 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4444...  Training loss: 1.9357...  0.0573 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4445...  Training loss: 1.8857...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4446...  Training loss: 1.8561...  0.0596 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4447...  Training loss: 1.9197...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4448...  Training loss: 1.8887...  0.0588 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4449...  Training loss: 1.8896...  0.0575 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4450...  Training loss: 1.8842...  0.0557 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4451...  Training loss: 1.8502...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4452...  Training loss: 1.8848...  0.0522 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4453...  Training loss: 1.8800...  0.0571 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4454...  Training loss: 1.8828...  0.0534 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4455...  Training loss: 1.9301...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4456...  Training loss: 1.9273...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4457...  Training loss: 1.8692...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4458...  Training loss: 1.9200...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4459...  Training loss: 1.8844...  0.0578 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4460...  Training loss: 1.8878...  0.0558 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4461...  Training loss: 1.8727...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4462...  Training loss: 1.8531...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4463...  Training loss: 1.8772...  0.0557 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4464...  Training loss: 1.9202...  0.0623 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4465...  Training loss: 1.9119...  0.0555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4466...  Training loss: 1.9354...  0.0547 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4467...  Training loss: 1.9483...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4468...  Training loss: 1.8367...  0.0594 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4469...  Training loss: 1.8910...  0.0560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4470...  Training loss: 1.9569...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4471...  Training loss: 1.8945...  0.0559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4472...  Training loss: 1.9432...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4473...  Training loss: 1.9284...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4474...  Training loss: 1.8884...  0.0586 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4475...  Training loss: 1.8683...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4476...  Training loss: 1.8788...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4477...  Training loss: 1.8726...  0.0600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4478...  Training loss: 1.9008...  0.0542 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4479...  Training loss: 1.9445...  0.0579 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4480...  Training loss: 1.8948...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4481...  Training loss: 1.9223...  0.0535 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4482...  Training loss: 1.8254...  0.0568 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4483...  Training loss: 1.9006...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4484...  Training loss: 1.8841...  0.0604 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4485...  Training loss: 1.8440...  0.0535 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4486...  Training loss: 1.9480...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4487...  Training loss: 1.9063...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4488...  Training loss: 1.9008...  0.0598 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4489...  Training loss: 1.9016...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4490...  Training loss: 1.9210...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4491...  Training loss: 1.9141...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4492...  Training loss: 1.8880...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4493...  Training loss: 1.8883...  0.0569 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4494...  Training loss: 1.9371...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4495...  Training loss: 1.8828...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4496...  Training loss: 1.9023...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4497...  Training loss: 1.8823...  0.0522 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4498...  Training loss: 1.9254...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4499...  Training loss: 1.9152...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4500...  Training loss: 1.8619...  0.0590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4501...  Training loss: 1.8488...  0.0543 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4502...  Training loss: 1.8683...  0.0591 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4503...  Training loss: 1.9044...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4504...  Training loss: 1.8960...  0.0566 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 4505...  Training loss: 1.8992...  0.0543 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4506...  Training loss: 1.9119...  0.0543 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4507...  Training loss: 1.9349...  0.0568 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4508...  Training loss: 1.8990...  0.0591 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4509...  Training loss: 1.8811...  0.0544 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4510...  Training loss: 1.8464...  0.0598 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4511...  Training loss: 1.8709...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4512...  Training loss: 1.9029...  0.0583 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4513...  Training loss: 1.8877...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4514...  Training loss: 1.9006...  0.0559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4515...  Training loss: 1.8670...  0.0564 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4516...  Training loss: 1.8947...  0.0561 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4517...  Training loss: 1.8953...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4518...  Training loss: 1.8885...  0.0617 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4519...  Training loss: 1.8845...  0.0542 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4520...  Training loss: 1.8744...  0.0587 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4521...  Training loss: 1.8855...  0.0540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4522...  Training loss: 1.9146...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4523...  Training loss: 1.8994...  0.0564 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4524...  Training loss: 1.8553...  0.0560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4525...  Training loss: 1.8536...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4526...  Training loss: 1.8931...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4527...  Training loss: 1.8949...  0.0565 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4528...  Training loss: 1.8888...  0.0571 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4529...  Training loss: 1.8795...  0.0563 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4530...  Training loss: 1.9605...  0.0566 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4531...  Training loss: 1.8996...  0.0583 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4532...  Training loss: 1.9432...  0.0536 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4533...  Training loss: 1.9323...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4534...  Training loss: 1.8691...  0.0598 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4535...  Training loss: 1.9007...  0.0564 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4536...  Training loss: 1.9661...  0.0592 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4537...  Training loss: 1.8809...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4538...  Training loss: 1.9562...  0.0577 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4539...  Training loss: 1.8547...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4540...  Training loss: 1.9157...  0.0597 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4541...  Training loss: 1.8861...  0.0617 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4542...  Training loss: 1.8873...  0.0534 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4543...  Training loss: 1.8863...  0.0554 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4544...  Training loss: 1.8948...  0.0590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4545...  Training loss: 1.9088...  0.0557 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4546...  Training loss: 1.8726...  0.0591 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4547...  Training loss: 1.9123...  0.0588 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4548...  Training loss: 1.8801...  0.0563 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4549...  Training loss: 1.8868...  0.0565 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4550...  Training loss: 1.8594...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4551...  Training loss: 1.8936...  0.0569 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4552...  Training loss: 1.9078...  0.0565 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4553...  Training loss: 1.9421...  0.0574 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4554...  Training loss: 1.9428...  0.0593 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4555...  Training loss: 1.9289...  0.0572 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4556...  Training loss: 1.9199...  0.0539 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4557...  Training loss: 1.9016...  0.0642 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4558...  Training loss: 1.8777...  0.0536 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4559...  Training loss: 1.9806...  0.0538 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4560...  Training loss: 1.9445...  0.0598 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4561...  Training loss: 1.9100...  0.0578 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4562...  Training loss: 1.9256...  0.0559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4563...  Training loss: 1.9457...  0.0548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4564...  Training loss: 1.8522...  0.0570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4565...  Training loss: 1.8830...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4566...  Training loss: 1.9391...  0.0535 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4567...  Training loss: 1.9642...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4568...  Training loss: 1.8650...  0.0547 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4569...  Training loss: 1.9194...  0.0566 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4570...  Training loss: 1.8858...  0.0600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4571...  Training loss: 1.9435...  0.0565 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4572...  Training loss: 1.8743...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4573...  Training loss: 1.8743...  0.0567 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4574...  Training loss: 1.8706...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4575...  Training loss: 1.8594...  0.0558 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4576...  Training loss: 1.9354...  0.0586 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4577...  Training loss: 1.8951...  0.0593 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4578...  Training loss: 1.8539...  0.0590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4579...  Training loss: 1.8598...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4580...  Training loss: 1.9056...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4581...  Training loss: 1.8718...  0.0567 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4582...  Training loss: 1.8861...  0.0560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4583...  Training loss: 1.8611...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4584...  Training loss: 1.8938...  0.0571 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4585...  Training loss: 1.9021...  0.0590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4586...  Training loss: 1.8945...  0.0558 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4587...  Training loss: 1.8973...  0.0598 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4588...  Training loss: 1.9054...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4589...  Training loss: 1.8488...  0.0579 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4590...  Training loss: 1.8676...  0.0618 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4591...  Training loss: 1.8937...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4592...  Training loss: 1.8717...  0.0557 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4593...  Training loss: 1.9025...  0.0535 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4594...  Training loss: 1.9174...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4595...  Training loss: 1.9458...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4596...  Training loss: 1.9195...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4597...  Training loss: 1.8909...  0.0591 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4598...  Training loss: 1.8875...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4599...  Training loss: 1.8897...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4600...  Training loss: 1.8874...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4601...  Training loss: 1.9009...  0.0612 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4602...  Training loss: 1.8535...  0.0537 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4603...  Training loss: 1.9107...  0.0586 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4604...  Training loss: 1.9076...  0.0530 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 4605...  Training loss: 1.9467...  0.0542 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4606...  Training loss: 1.8207...  0.0596 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4607...  Training loss: 1.9034...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4608...  Training loss: 1.9009...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4609...  Training loss: 1.8750...  0.0543 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4610...  Training loss: 1.8729...  0.0536 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4611...  Training loss: 1.8489...  0.0536 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4612...  Training loss: 1.8989...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4613...  Training loss: 1.8588...  0.0608 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4614...  Training loss: 1.8819...  0.0534 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4615...  Training loss: 1.9144...  0.0605 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4616...  Training loss: 1.9442...  0.0569 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4617...  Training loss: 1.9641...  0.0542 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4618...  Training loss: 1.9113...  0.0600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4619...  Training loss: 1.9396...  0.0537 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4620...  Training loss: 1.9403...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4621...  Training loss: 1.8917...  0.0562 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4622...  Training loss: 1.8612...  0.0537 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4623...  Training loss: 1.8662...  0.0562 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4624...  Training loss: 1.9032...  0.0557 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4625...  Training loss: 1.8772...  0.0537 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4626...  Training loss: 1.8993...  0.0565 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4627...  Training loss: 1.8682...  0.0559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4628...  Training loss: 1.8902...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4629...  Training loss: 1.8919...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4630...  Training loss: 1.9266...  0.0591 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4631...  Training loss: 1.8882...  0.0578 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4632...  Training loss: 1.8856...  0.0587 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4633...  Training loss: 1.8829...  0.0548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4634...  Training loss: 1.9086...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4635...  Training loss: 1.8827...  0.0567 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4636...  Training loss: 1.8297...  0.0576 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4637...  Training loss: 1.9060...  0.0537 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4638...  Training loss: 1.9364...  0.0562 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4639...  Training loss: 1.9409...  0.0572 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4640...  Training loss: 1.9158...  0.0561 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4641...  Training loss: 1.9160...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4642...  Training loss: 1.9610...  0.0574 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4643...  Training loss: 1.8425...  0.0559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4644...  Training loss: 1.9113...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4645...  Training loss: 1.8982...  0.0536 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4646...  Training loss: 1.8861...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4647...  Training loss: 1.8878...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4648...  Training loss: 1.8725...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4649...  Training loss: 1.8811...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4650...  Training loss: 1.8568...  0.0564 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4651...  Training loss: 1.8540...  0.0567 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4652...  Training loss: 1.8743...  0.0565 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4653...  Training loss: 1.8894...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4654...  Training loss: 1.8518...  0.0548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4655...  Training loss: 1.8865...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4656...  Training loss: 1.9040...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4657...  Training loss: 1.8587...  0.0560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4658...  Training loss: 1.8374...  0.0590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4659...  Training loss: 1.8783...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4660...  Training loss: 1.9352...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4661...  Training loss: 1.8819...  0.0537 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4662...  Training loss: 1.8507...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4663...  Training loss: 1.8810...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4664...  Training loss: 1.8859...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4665...  Training loss: 1.8608...  0.0557 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4666...  Training loss: 1.8619...  0.0555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4667...  Training loss: 1.8406...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4668...  Training loss: 1.8434...  0.0559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4669...  Training loss: 1.8902...  0.0535 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4670...  Training loss: 1.8917...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4671...  Training loss: 1.8835...  0.0579 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4672...  Training loss: 1.8684...  0.0631 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4673...  Training loss: 1.8553...  0.0554 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4674...  Training loss: 1.8672...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4675...  Training loss: 1.8741...  0.0520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4676...  Training loss: 1.8777...  0.0547 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4677...  Training loss: 1.8511...  0.0593 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4678...  Training loss: 1.8504...  0.0544 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4679...  Training loss: 1.8552...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4680...  Training loss: 1.8882...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4681...  Training loss: 1.8905...  0.0548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4682...  Training loss: 1.8772...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4683...  Training loss: 1.8628...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4684...  Training loss: 1.8688...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4685...  Training loss: 1.8771...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4686...  Training loss: 1.8946...  0.0565 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4687...  Training loss: 1.8947...  0.0548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4688...  Training loss: 1.8894...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4689...  Training loss: 1.8807...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4690...  Training loss: 1.8692...  0.0545 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4691...  Training loss: 1.8967...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4692...  Training loss: 1.8857...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4693...  Training loss: 1.9110...  0.0578 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4694...  Training loss: 1.8563...  0.0544 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4695...  Training loss: 1.8595...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4696...  Training loss: 1.9199...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4697...  Training loss: 1.9734...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4698...  Training loss: 1.9247...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4699...  Training loss: 1.8989...  0.0558 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4700...  Training loss: 1.8992...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4701...  Training loss: 1.9298...  0.0535 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4702...  Training loss: 1.8651...  0.0575 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4703...  Training loss: 1.8357...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4704...  Training loss: 1.8206...  0.0556 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 4705...  Training loss: 1.8761...  0.0559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4706...  Training loss: 1.8932...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4707...  Training loss: 1.8693...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4708...  Training loss: 1.9434...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4709...  Training loss: 1.9200...  0.0559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4710...  Training loss: 1.8765...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4711...  Training loss: 1.9110...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4712...  Training loss: 1.9632...  0.0584 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4713...  Training loss: 1.8851...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4714...  Training loss: 1.8821...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4715...  Training loss: 1.8683...  0.0566 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4716...  Training loss: 1.8857...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4717...  Training loss: 1.8474...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4718...  Training loss: 1.9637...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4719...  Training loss: 1.8966...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4720...  Training loss: 1.8982...  0.0579 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4721...  Training loss: 1.8110...  0.0588 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4722...  Training loss: 1.9517...  0.0524 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4723...  Training loss: 1.8530...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4724...  Training loss: 1.8823...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4725...  Training loss: 1.8836...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4726...  Training loss: 1.8154...  0.0583 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4727...  Training loss: 1.8040...  0.0579 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4728...  Training loss: 1.8852...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4729...  Training loss: 1.8460...  0.0573 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4730...  Training loss: 1.8502...  0.0578 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4731...  Training loss: 1.9258...  0.0578 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4732...  Training loss: 1.8467...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4733...  Training loss: 1.8856...  0.0591 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4734...  Training loss: 1.8818...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4735...  Training loss: 1.8605...  0.0545 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4736...  Training loss: 1.8814...  0.0585 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4737...  Training loss: 1.8655...  0.0568 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4738...  Training loss: 1.8917...  0.0589 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4739...  Training loss: 1.8675...  0.0524 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4740...  Training loss: 1.8971...  0.0534 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4741...  Training loss: 1.9280...  0.0555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4742...  Training loss: 1.8976...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4743...  Training loss: 1.8532...  0.0608 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4744...  Training loss: 1.9133...  0.0587 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4745...  Training loss: 1.9342...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4746...  Training loss: 1.9142...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4747...  Training loss: 1.9387...  0.0534 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4748...  Training loss: 1.8865...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4749...  Training loss: 1.9489...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4750...  Training loss: 1.9030...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4751...  Training loss: 1.8879...  0.0575 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4752...  Training loss: 1.9335...  0.0558 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4753...  Training loss: 1.9078...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4754...  Training loss: 1.8631...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4755...  Training loss: 1.8580...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4756...  Training loss: 1.8439...  0.0557 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4757...  Training loss: 1.8646...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4758...  Training loss: 1.8828...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4759...  Training loss: 1.9253...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4760...  Training loss: 1.8631...  0.0568 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4761...  Training loss: 1.8486...  0.0574 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4762...  Training loss: 1.8932...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4763...  Training loss: 1.9135...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4764...  Training loss: 1.8592...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4765...  Training loss: 1.9155...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4766...  Training loss: 1.9085...  0.0603 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4767...  Training loss: 1.8700...  0.0539 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4768...  Training loss: 1.8965...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4769...  Training loss: 1.8617...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4770...  Training loss: 1.8357...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4771...  Training loss: 1.8632...  0.0561 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4772...  Training loss: 1.9090...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4773...  Training loss: 1.9030...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4774...  Training loss: 1.8545...  0.0577 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4775...  Training loss: 1.8436...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4776...  Training loss: 1.8764...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4777...  Training loss: 1.8449...  0.0588 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4778...  Training loss: 1.8887...  0.0570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4779...  Training loss: 1.8712...  0.0640 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4780...  Training loss: 1.8756...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4781...  Training loss: 1.8639...  0.0559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4782...  Training loss: 1.8943...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4783...  Training loss: 1.9142...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4784...  Training loss: 1.8805...  0.0524 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4785...  Training loss: 1.7860...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4786...  Training loss: 1.8941...  0.0542 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4787...  Training loss: 1.8477...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4788...  Training loss: 1.8166...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4789...  Training loss: 1.9202...  0.0545 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4790...  Training loss: 1.9288...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4791...  Training loss: 1.9449...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4792...  Training loss: 1.9228...  0.0524 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4793...  Training loss: 1.8609...  0.0572 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4794...  Training loss: 1.8893...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4795...  Training loss: 1.8555...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4796...  Training loss: 1.8934...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4797...  Training loss: 1.8916...  0.0584 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4798...  Training loss: 1.8920...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4799...  Training loss: 1.8433...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4800...  Training loss: 1.8779...  0.0548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4801...  Training loss: 1.8560...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4802...  Training loss: 1.8968...  0.0522 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4803...  Training loss: 1.8474...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4804...  Training loss: 1.8624...  0.0584 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 4805...  Training loss: 1.9261...  0.0583 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4806...  Training loss: 1.9062...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4807...  Training loss: 1.8741...  0.0578 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4808...  Training loss: 1.8615...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4809...  Training loss: 1.9151...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4810...  Training loss: 1.8796...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4811...  Training loss: 1.8659...  0.0589 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4812...  Training loss: 1.8790...  0.0558 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4813...  Training loss: 1.8553...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4814...  Training loss: 1.8236...  0.0545 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4815...  Training loss: 1.9249...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4816...  Training loss: 1.9193...  0.0547 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4817...  Training loss: 1.9006...  0.0573 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4818...  Training loss: 1.9540...  0.0543 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4819...  Training loss: 1.8618...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4820...  Training loss: 1.9432...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4821...  Training loss: 1.8954...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4822...  Training loss: 1.8696...  0.0564 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4823...  Training loss: 1.8786...  0.0555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4824...  Training loss: 1.9056...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4825...  Training loss: 1.9517...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4826...  Training loss: 1.8647...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4827...  Training loss: 1.9065...  0.0570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4828...  Training loss: 1.8529...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4829...  Training loss: 1.8948...  0.0587 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4830...  Training loss: 1.8802...  0.0536 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4831...  Training loss: 1.8766...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4832...  Training loss: 1.9127...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4833...  Training loss: 1.8800...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4834...  Training loss: 1.8533...  0.0548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4835...  Training loss: 1.8416...  0.0562 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4836...  Training loss: 1.8808...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4837...  Training loss: 1.8730...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4838...  Training loss: 1.9015...  0.0576 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4839...  Training loss: 1.8043...  0.0555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4840...  Training loss: 1.9074...  0.0521 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4841...  Training loss: 1.9032...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4842...  Training loss: 1.9207...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4843...  Training loss: 1.9469...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4844...  Training loss: 1.8957...  0.0554 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4845...  Training loss: 1.8711...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4846...  Training loss: 1.8444...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4847...  Training loss: 1.9215...  0.0591 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4848...  Training loss: 1.8629...  0.0547 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4849...  Training loss: 1.9234...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4850...  Training loss: 1.9009...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4851...  Training loss: 1.9307...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4852...  Training loss: 1.8820...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4853...  Training loss: 1.9416...  0.0570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4854...  Training loss: 1.9185...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4855...  Training loss: 1.8988...  0.0583 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4856...  Training loss: 1.8752...  0.0558 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4857...  Training loss: 1.8743...  0.0578 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4858...  Training loss: 1.8393...  0.0572 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4859...  Training loss: 1.8764...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4860...  Training loss: 1.8732...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4861...  Training loss: 1.8752...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4862...  Training loss: 1.8327...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4863...  Training loss: 1.8743...  0.0568 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4864...  Training loss: 1.7859...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4865...  Training loss: 1.9112...  0.0593 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4866...  Training loss: 1.9181...  0.0579 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4867...  Training loss: 1.9148...  0.0524 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4868...  Training loss: 1.9267...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4869...  Training loss: 1.8620...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4870...  Training loss: 1.8504...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4871...  Training loss: 1.8855...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4872...  Training loss: 1.8419...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4873...  Training loss: 1.8913...  0.0579 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4874...  Training loss: 1.8677...  0.0582 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4875...  Training loss: 1.8779...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4876...  Training loss: 1.8877...  0.0553 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4877...  Training loss: 1.8407...  0.0534 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4878...  Training loss: 1.8991...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4879...  Training loss: 1.8162...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4880...  Training loss: 1.9072...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4881...  Training loss: 1.8910...  0.0568 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4882...  Training loss: 1.8627...  0.0557 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4883...  Training loss: 1.8520...  0.0595 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4884...  Training loss: 1.8484...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4885...  Training loss: 1.9294...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4886...  Training loss: 1.8755...  0.0578 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4887...  Training loss: 1.9125...  0.0564 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4888...  Training loss: 1.9174...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4889...  Training loss: 1.9076...  0.0536 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4890...  Training loss: 1.8868...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4891...  Training loss: 1.9253...  0.0538 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4892...  Training loss: 1.8304...  0.0560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4893...  Training loss: 1.8822...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4894...  Training loss: 1.8741...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4895...  Training loss: 1.8560...  0.0568 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4896...  Training loss: 1.8674...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4897...  Training loss: 1.8917...  0.0604 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4898...  Training loss: 1.8373...  0.0522 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4899...  Training loss: 1.8749...  0.0784 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4900...  Training loss: 1.8626...  0.0566 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4901...  Training loss: 1.9165...  0.0641 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4902...  Training loss: 1.9129...  0.0576 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4903...  Training loss: 1.8801...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4904...  Training loss: 1.9492...  0.0535 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 4905...  Training loss: 1.9921...  0.0561 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4906...  Training loss: 1.9376...  0.0581 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4907...  Training loss: 1.8847...  0.0542 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4908...  Training loss: 1.9586...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4909...  Training loss: 1.8603...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4910...  Training loss: 1.9060...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4911...  Training loss: 1.8978...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4912...  Training loss: 1.9470...  0.0569 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4913...  Training loss: 1.8869...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4914...  Training loss: 1.8620...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4915...  Training loss: 1.8826...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4916...  Training loss: 1.9337...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4917...  Training loss: 1.8403...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4918...  Training loss: 1.9002...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4919...  Training loss: 1.9380...  0.0559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4920...  Training loss: 1.9041...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4921...  Training loss: 1.9148...  0.0599 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4922...  Training loss: 1.8701...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4923...  Training loss: 1.8909...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4924...  Training loss: 1.9099...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4925...  Training loss: 1.9111...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4926...  Training loss: 1.8819...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4927...  Training loss: 1.8312...  0.0566 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4928...  Training loss: 1.8911...  0.0555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4929...  Training loss: 1.8543...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4930...  Training loss: 1.9213...  0.0543 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4931...  Training loss: 1.8302...  0.0531 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4932...  Training loss: 1.9362...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4933...  Training loss: 1.8534...  0.0525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4934...  Training loss: 1.8458...  0.0568 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4935...  Training loss: 1.8733...  0.0540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4936...  Training loss: 1.8431...  0.0589 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4937...  Training loss: 1.8335...  0.0563 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4938...  Training loss: 1.8823...  0.0578 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4939...  Training loss: 1.9152...  0.0576 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4940...  Training loss: 1.8975...  0.0547 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4941...  Training loss: 1.8805...  0.0573 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4942...  Training loss: 1.8730...  0.0565 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4943...  Training loss: 1.8893...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4944...  Training loss: 1.8399...  0.0567 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4945...  Training loss: 1.8844...  0.0580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4946...  Training loss: 1.8763...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4947...  Training loss: 1.8486...  0.0551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4948...  Training loss: 1.8550...  0.0526 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4949...  Training loss: 1.8514...  0.0543 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4950...  Training loss: 1.9175...  0.0580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4951...  Training loss: 1.9186...  0.0575 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4952...  Training loss: 1.8972...  0.0549 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4953...  Training loss: 1.8503...  0.0558 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4954...  Training loss: 1.9038...  0.0588 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4955...  Training loss: 1.8546...  0.0523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4956...  Training loss: 1.9125...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4957...  Training loss: 1.9284...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4958...  Training loss: 1.8391...  0.0544 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4959...  Training loss: 1.8430...  0.0532 sec/batch\n",
      "Epoch: 8/20...  Training Step: 4960...  Training loss: 1.8423...  0.0525 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4961...  Training loss: 1.9755...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4962...  Training loss: 1.9607...  0.0557 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4963...  Training loss: 1.9316...  0.0589 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4964...  Training loss: 1.8464...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4965...  Training loss: 1.9062...  0.0587 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4966...  Training loss: 1.9064...  0.0544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4967...  Training loss: 1.8433...  0.0596 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4968...  Training loss: 1.8435...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4969...  Training loss: 1.8344...  0.0560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4970...  Training loss: 1.8668...  0.0533 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4971...  Training loss: 1.8654...  0.0534 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4972...  Training loss: 1.8286...  0.0583 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4973...  Training loss: 1.8747...  0.0564 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4974...  Training loss: 1.8624...  0.0567 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4975...  Training loss: 1.9043...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4976...  Training loss: 1.9288...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4977...  Training loss: 1.9004...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4978...  Training loss: 1.8882...  0.0554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4979...  Training loss: 1.8425...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4980...  Training loss: 1.8740...  0.0554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4981...  Training loss: 1.9408...  0.0577 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4982...  Training loss: 1.8692...  0.0538 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4983...  Training loss: 1.8353...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4984...  Training loss: 1.8682...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4985...  Training loss: 1.8727...  0.0565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4986...  Training loss: 1.8250...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4987...  Training loss: 1.8727...  0.0523 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4988...  Training loss: 1.8793...  0.0562 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4989...  Training loss: 1.8725...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4990...  Training loss: 1.8415...  0.0567 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4991...  Training loss: 1.8303...  0.0555 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4992...  Training loss: 1.8939...  0.0563 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4993...  Training loss: 1.8528...  0.0556 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4994...  Training loss: 1.8602...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4995...  Training loss: 1.8749...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4996...  Training loss: 1.8701...  0.0579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4997...  Training loss: 1.8759...  0.0565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4998...  Training loss: 1.8681...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 4999...  Training loss: 1.8769...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5000...  Training loss: 1.8446...  0.0570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5001...  Training loss: 1.8599...  0.0532 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5002...  Training loss: 1.9057...  0.0522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5003...  Training loss: 1.8594...  0.0557 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5004...  Training loss: 1.9017...  0.0542 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 5005...  Training loss: 1.8427...  0.0532 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5006...  Training loss: 1.8611...  0.0522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5007...  Training loss: 1.7546...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5008...  Training loss: 1.8860...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5009...  Training loss: 1.8384...  0.0544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5010...  Training loss: 1.8970...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5011...  Training loss: 1.8418...  0.0537 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5012...  Training loss: 1.8456...  0.0585 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5013...  Training loss: 1.8582...  0.0568 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5014...  Training loss: 1.8831...  0.0543 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5015...  Training loss: 1.8834...  0.0525 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5016...  Training loss: 1.8652...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5017...  Training loss: 1.8306...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5018...  Training loss: 1.8793...  0.0539 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5019...  Training loss: 1.8488...  0.0527 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5020...  Training loss: 1.8958...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5021...  Training loss: 1.8549...  0.0594 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5022...  Training loss: 1.8286...  0.0566 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5023...  Training loss: 1.9124...  0.0570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5024...  Training loss: 1.8669...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5025...  Training loss: 1.8233...  0.0595 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5026...  Training loss: 1.8190...  0.0537 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5027...  Training loss: 1.8287...  0.0559 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5028...  Training loss: 1.8420...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5029...  Training loss: 1.8685...  0.0536 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5030...  Training loss: 1.8752...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5031...  Training loss: 1.8933...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5032...  Training loss: 1.8909...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5033...  Training loss: 1.8189...  0.0555 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5034...  Training loss: 1.8621...  0.0557 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5035...  Training loss: 1.9225...  0.0545 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5036...  Training loss: 1.8925...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5037...  Training loss: 1.8913...  0.0527 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5038...  Training loss: 1.8569...  0.0564 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5039...  Training loss: 1.8632...  0.0565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5040...  Training loss: 1.8815...  0.0563 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5041...  Training loss: 1.8127...  0.0592 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5042...  Training loss: 1.8673...  0.0544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5043...  Training loss: 1.8254...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5044...  Training loss: 1.8526...  0.0565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5045...  Training loss: 1.8328...  0.0592 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5046...  Training loss: 1.9101...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5047...  Training loss: 1.8395...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5048...  Training loss: 1.9348...  0.0539 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5049...  Training loss: 1.8665...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5050...  Training loss: 1.8926...  0.0573 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5051...  Training loss: 1.8454...  0.0545 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5052...  Training loss: 1.9239...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5053...  Training loss: 1.8444...  0.0585 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5054...  Training loss: 1.8665...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5055...  Training loss: 1.8552...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5056...  Training loss: 1.8780...  0.0579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5057...  Training loss: 1.8819...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5058...  Training loss: 1.8071...  0.0569 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5059...  Training loss: 1.9107...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5060...  Training loss: 1.8459...  0.0560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5061...  Training loss: 1.8511...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5062...  Training loss: 1.8353...  0.0598 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5063...  Training loss: 1.8841...  0.0557 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5064...  Training loss: 1.8996...  0.0533 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5065...  Training loss: 1.8679...  0.0564 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5066...  Training loss: 1.8408...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5067...  Training loss: 1.8959...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5068...  Training loss: 1.8557...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5069...  Training loss: 1.8644...  0.0578 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5070...  Training loss: 1.8414...  0.0534 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5071...  Training loss: 1.8329...  0.0566 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5072...  Training loss: 1.8448...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5073...  Training loss: 1.8635...  0.0596 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5074...  Training loss: 1.8538...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5075...  Training loss: 1.8741...  0.0602 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5076...  Training loss: 1.9048...  0.0527 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5077...  Training loss: 1.8137...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5078...  Training loss: 1.9125...  0.0556 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5079...  Training loss: 1.8492...  0.0565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5080...  Training loss: 1.8521...  0.0555 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5081...  Training loss: 1.8404...  0.0532 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5082...  Training loss: 1.8182...  0.0619 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5083...  Training loss: 1.8450...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5084...  Training loss: 1.9088...  0.0570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5085...  Training loss: 1.8825...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5086...  Training loss: 1.9222...  0.0525 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5087...  Training loss: 1.9221...  0.0574 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5088...  Training loss: 1.8356...  0.0567 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5089...  Training loss: 1.8506...  0.0545 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5090...  Training loss: 1.9035...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5091...  Training loss: 1.8531...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5092...  Training loss: 1.9210...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5093...  Training loss: 1.9144...  0.0564 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5094...  Training loss: 1.8577...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5095...  Training loss: 1.8457...  0.0532 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5096...  Training loss: 1.8720...  0.0577 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5097...  Training loss: 1.8491...  0.0560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5098...  Training loss: 1.8694...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5099...  Training loss: 1.9232...  0.0544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5100...  Training loss: 1.8704...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5101...  Training loss: 1.9049...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5102...  Training loss: 1.7807...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5103...  Training loss: 1.8586...  0.0542 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5104...  Training loss: 1.8382...  0.0538 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 5105...  Training loss: 1.8100...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5106...  Training loss: 1.8903...  0.0587 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5107...  Training loss: 1.8885...  0.0595 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5108...  Training loss: 1.8912...  0.0527 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5109...  Training loss: 1.8630...  0.0578 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5110...  Training loss: 1.8973...  0.0545 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5111...  Training loss: 1.8668...  0.0581 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5112...  Training loss: 1.8510...  0.0591 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5113...  Training loss: 1.8562...  0.0558 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5114...  Training loss: 1.9033...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5115...  Training loss: 1.8616...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5116...  Training loss: 1.8818...  0.0580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5117...  Training loss: 1.8727...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5118...  Training loss: 1.8837...  0.0534 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5119...  Training loss: 1.9073...  0.0521 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5120...  Training loss: 1.8393...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5121...  Training loss: 1.8282...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5122...  Training loss: 1.8440...  0.0555 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5123...  Training loss: 1.8558...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5124...  Training loss: 1.8687...  0.0555 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5125...  Training loss: 1.8718...  0.0584 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5126...  Training loss: 1.8656...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5127...  Training loss: 1.8921...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5128...  Training loss: 1.8716...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5129...  Training loss: 1.8646...  0.0564 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5130...  Training loss: 1.8271...  0.0595 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5131...  Training loss: 1.8446...  0.0579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5132...  Training loss: 1.8732...  0.0580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5133...  Training loss: 1.8678...  0.0582 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5134...  Training loss: 1.8487...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5135...  Training loss: 1.8586...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5136...  Training loss: 1.8540...  0.0538 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5137...  Training loss: 1.8785...  0.0555 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5138...  Training loss: 1.8367...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5139...  Training loss: 1.8322...  0.0527 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5140...  Training loss: 1.8502...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5141...  Training loss: 1.8413...  0.0572 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5142...  Training loss: 1.8948...  0.0554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5143...  Training loss: 1.8833...  0.0534 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5144...  Training loss: 1.8382...  0.0579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5145...  Training loss: 1.8107...  0.0539 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5146...  Training loss: 1.8764...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5147...  Training loss: 1.8335...  0.0586 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5148...  Training loss: 1.8600...  0.0554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5149...  Training loss: 1.8434...  0.0572 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5150...  Training loss: 1.9259...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5151...  Training loss: 1.8790...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5152...  Training loss: 1.9246...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5153...  Training loss: 1.8760...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5154...  Training loss: 1.8140...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5155...  Training loss: 1.8611...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5156...  Training loss: 1.9192...  0.0533 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5157...  Training loss: 1.8773...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5158...  Training loss: 1.9407...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5159...  Training loss: 1.8390...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5160...  Training loss: 1.8936...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5161...  Training loss: 1.8550...  0.0562 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5162...  Training loss: 1.8542...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5163...  Training loss: 1.8558...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5164...  Training loss: 1.8462...  0.0545 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5165...  Training loss: 1.8627...  0.0576 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5166...  Training loss: 1.8434...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5167...  Training loss: 1.8837...  0.0535 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5168...  Training loss: 1.8579...  0.0554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5169...  Training loss: 1.8599...  0.0535 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5170...  Training loss: 1.8296...  0.0545 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5171...  Training loss: 1.8812...  0.0570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5172...  Training loss: 1.8527...  0.0583 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5173...  Training loss: 1.9040...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5174...  Training loss: 1.8932...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5175...  Training loss: 1.8994...  0.0572 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5176...  Training loss: 1.9078...  0.0572 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5177...  Training loss: 1.8814...  0.0580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5178...  Training loss: 1.8441...  0.0561 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5179...  Training loss: 1.9310...  0.0533 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5180...  Training loss: 1.9035...  0.0586 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5181...  Training loss: 1.8795...  0.0538 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5182...  Training loss: 1.8813...  0.0586 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5183...  Training loss: 1.9105...  0.0543 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5184...  Training loss: 1.8399...  0.0522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5185...  Training loss: 1.8369...  0.0535 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5186...  Training loss: 1.8941...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5187...  Training loss: 1.9065...  0.0565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5188...  Training loss: 1.8291...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5189...  Training loss: 1.8919...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5190...  Training loss: 1.8487...  0.0593 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5191...  Training loss: 1.9184...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5192...  Training loss: 1.8470...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5193...  Training loss: 1.8394...  0.0532 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5194...  Training loss: 1.8292...  0.0541 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5195...  Training loss: 1.8407...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5196...  Training loss: 1.8888...  0.0558 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5197...  Training loss: 1.8503...  0.0559 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5198...  Training loss: 1.8132...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5199...  Training loss: 1.8428...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5200...  Training loss: 1.8824...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5201...  Training loss: 1.8543...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5202...  Training loss: 1.8294...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5203...  Training loss: 1.8376...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5204...  Training loss: 1.8567...  0.0546 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 5205...  Training loss: 1.8773...  0.0537 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5206...  Training loss: 1.8534...  0.0562 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5207...  Training loss: 1.8825...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5208...  Training loss: 1.8868...  0.0579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5209...  Training loss: 1.8130...  0.0558 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5210...  Training loss: 1.8453...  0.0577 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5211...  Training loss: 1.8689...  0.0571 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5212...  Training loss: 1.8492...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5213...  Training loss: 1.8951...  0.0554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5214...  Training loss: 1.8695...  0.0591 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5215...  Training loss: 1.9044...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5216...  Training loss: 1.8647...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5217...  Training loss: 1.8756...  0.0570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5218...  Training loss: 1.8584...  0.0563 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5219...  Training loss: 1.8565...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5220...  Training loss: 1.8444...  0.0566 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5221...  Training loss: 1.8508...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5222...  Training loss: 1.8312...  0.0521 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5223...  Training loss: 1.8743...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5224...  Training loss: 1.8667...  0.0562 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5225...  Training loss: 1.8859...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5226...  Training loss: 1.7972...  0.0523 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5227...  Training loss: 1.8659...  0.0555 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5228...  Training loss: 1.8723...  0.0518 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5229...  Training loss: 1.8460...  0.0560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5230...  Training loss: 1.8175...  0.0543 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5231...  Training loss: 1.8154...  0.0577 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5232...  Training loss: 1.8459...  0.0594 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5233...  Training loss: 1.8415...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5234...  Training loss: 1.8653...  0.0542 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5235...  Training loss: 1.8900...  0.0527 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5236...  Training loss: 1.8911...  0.0527 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5237...  Training loss: 1.9348...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5238...  Training loss: 1.8853...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5239...  Training loss: 1.9106...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5240...  Training loss: 1.9076...  0.0532 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5241...  Training loss: 1.8646...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5242...  Training loss: 1.8044...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5243...  Training loss: 1.8304...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5244...  Training loss: 1.8774...  0.0522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5245...  Training loss: 1.8344...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5246...  Training loss: 1.8770...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5247...  Training loss: 1.8530...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5248...  Training loss: 1.8663...  0.0584 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5249...  Training loss: 1.8716...  0.0600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5250...  Training loss: 1.9118...  0.0557 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5251...  Training loss: 1.8498...  0.0523 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5252...  Training loss: 1.8837...  0.0556 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5253...  Training loss: 1.8508...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5254...  Training loss: 1.8879...  0.0560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5255...  Training loss: 1.8159...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5256...  Training loss: 1.8242...  0.0532 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5257...  Training loss: 1.8841...  0.0519 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5258...  Training loss: 1.8968...  0.0582 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5259...  Training loss: 1.9097...  0.0563 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5260...  Training loss: 1.8569...  0.0584 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5261...  Training loss: 1.9055...  0.0544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5262...  Training loss: 1.9196...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5263...  Training loss: 1.8187...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5264...  Training loss: 1.8932...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5265...  Training loss: 1.8501...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5266...  Training loss: 1.8550...  0.0527 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5267...  Training loss: 1.8521...  0.0583 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5268...  Training loss: 1.8499...  0.0560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5269...  Training loss: 1.8671...  0.0574 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5270...  Training loss: 1.8262...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5271...  Training loss: 1.8245...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5272...  Training loss: 1.8308...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5273...  Training loss: 1.8497...  0.0591 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5274...  Training loss: 1.8204...  0.0562 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5275...  Training loss: 1.8633...  0.0582 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5276...  Training loss: 1.8689...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5277...  Training loss: 1.8176...  0.0544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5278...  Training loss: 1.8186...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5279...  Training loss: 1.8341...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5280...  Training loss: 1.9102...  0.0571 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5281...  Training loss: 1.8734...  0.0588 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5282...  Training loss: 1.8153...  0.0586 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5283...  Training loss: 1.8593...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5284...  Training loss: 1.8528...  0.0574 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5285...  Training loss: 1.8250...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5286...  Training loss: 1.8449...  0.0574 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5287...  Training loss: 1.8292...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5288...  Training loss: 1.8068...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5289...  Training loss: 1.8706...  0.0581 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5290...  Training loss: 1.8334...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5291...  Training loss: 1.8197...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5292...  Training loss: 1.8408...  0.0537 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5293...  Training loss: 1.8381...  0.0556 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5294...  Training loss: 1.8565...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5295...  Training loss: 1.8580...  0.0523 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5296...  Training loss: 1.8506...  0.0543 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5297...  Training loss: 1.8322...  0.0596 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5298...  Training loss: 1.8217...  0.0541 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5299...  Training loss: 1.8261...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5300...  Training loss: 1.8369...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5301...  Training loss: 1.8683...  0.0557 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5302...  Training loss: 1.8408...  0.0521 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5303...  Training loss: 1.8121...  0.0555 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5304...  Training loss: 1.8430...  0.0616 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 5305...  Training loss: 1.8615...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5306...  Training loss: 1.8579...  0.0554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5307...  Training loss: 1.8665...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5308...  Training loss: 1.8723...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5309...  Training loss: 1.8482...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5310...  Training loss: 1.8563...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5311...  Training loss: 1.8901...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5312...  Training loss: 1.8486...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5313...  Training loss: 1.8742...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5314...  Training loss: 1.8439...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5315...  Training loss: 1.8244...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5316...  Training loss: 1.9205...  0.0543 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5317...  Training loss: 1.9394...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5318...  Training loss: 1.8855...  0.0542 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5319...  Training loss: 1.8534...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5320...  Training loss: 1.8668...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5321...  Training loss: 1.8721...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5322...  Training loss: 1.8316...  0.0599 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5323...  Training loss: 1.7998...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5324...  Training loss: 1.8048...  0.0521 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5325...  Training loss: 1.8644...  0.0533 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5326...  Training loss: 1.8640...  0.0585 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5327...  Training loss: 1.8379...  0.0525 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5328...  Training loss: 1.8929...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5329...  Training loss: 1.8820...  0.0609 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5330...  Training loss: 1.8438...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5331...  Training loss: 1.8889...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5332...  Training loss: 1.9419...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5333...  Training loss: 1.8621...  0.0587 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5334...  Training loss: 1.8466...  0.0565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5335...  Training loss: 1.8464...  0.0554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5336...  Training loss: 1.8383...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5337...  Training loss: 1.8323...  0.0561 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5338...  Training loss: 1.9127...  0.0523 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5339...  Training loss: 1.8677...  0.0571 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5340...  Training loss: 1.8487...  0.0628 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5341...  Training loss: 1.7882...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5342...  Training loss: 1.9119...  0.0532 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5343...  Training loss: 1.8314...  0.0522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5344...  Training loss: 1.8526...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5345...  Training loss: 1.8713...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5346...  Training loss: 1.8008...  0.0575 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5347...  Training loss: 1.8006...  0.0521 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5348...  Training loss: 1.8638...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5349...  Training loss: 1.7850...  0.0555 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5350...  Training loss: 1.8158...  0.0567 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5351...  Training loss: 1.8940...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5352...  Training loss: 1.7992...  0.0527 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5353...  Training loss: 1.8556...  0.0566 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5354...  Training loss: 1.8928...  0.0544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5355...  Training loss: 1.8205...  0.0587 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5356...  Training loss: 1.8624...  0.0586 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5357...  Training loss: 1.8392...  0.0583 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5358...  Training loss: 1.8606...  0.0535 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5359...  Training loss: 1.8337...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5360...  Training loss: 1.8812...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5361...  Training loss: 1.8876...  0.0537 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5362...  Training loss: 1.8700...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5363...  Training loss: 1.8279...  0.0565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5364...  Training loss: 1.8935...  0.0554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5365...  Training loss: 1.9067...  0.0586 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5366...  Training loss: 1.8840...  0.0571 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5367...  Training loss: 1.9265...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5368...  Training loss: 1.8779...  0.0540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5369...  Training loss: 1.9152...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5370...  Training loss: 1.8782...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5371...  Training loss: 1.8341...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5372...  Training loss: 1.8798...  0.0579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5373...  Training loss: 1.8809...  0.0572 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5374...  Training loss: 1.8354...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5375...  Training loss: 1.8315...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5376...  Training loss: 1.8060...  0.0589 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5377...  Training loss: 1.8445...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5378...  Training loss: 1.8662...  0.0575 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5379...  Training loss: 1.8942...  0.0545 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5380...  Training loss: 1.8542...  0.0576 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5381...  Training loss: 1.8180...  0.0560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5382...  Training loss: 1.8505...  0.0568 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5383...  Training loss: 1.8652...  0.0525 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5384...  Training loss: 1.8287...  0.0565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5385...  Training loss: 1.8913...  0.0575 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5386...  Training loss: 1.8880...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5387...  Training loss: 1.8387...  0.0545 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5388...  Training loss: 1.8565...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5389...  Training loss: 1.8550...  0.0586 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5390...  Training loss: 1.8170...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5391...  Training loss: 1.8407...  0.0572 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5392...  Training loss: 1.8886...  0.0579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5393...  Training loss: 1.8641...  0.0618 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5394...  Training loss: 1.8517...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5395...  Training loss: 1.8261...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5396...  Training loss: 1.8365...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5397...  Training loss: 1.8069...  0.0557 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5398...  Training loss: 1.8535...  0.0559 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5399...  Training loss: 1.8587...  0.0522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5400...  Training loss: 1.8335...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5401...  Training loss: 1.8749...  0.0560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5402...  Training loss: 1.8495...  0.0558 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5403...  Training loss: 1.8882...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5404...  Training loss: 1.8623...  0.0549 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 5405...  Training loss: 1.7536...  0.0558 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5406...  Training loss: 1.8564...  0.0584 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5407...  Training loss: 1.8254...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5408...  Training loss: 1.8206...  0.0554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5409...  Training loss: 1.8497...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5410...  Training loss: 1.9069...  0.0527 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5411...  Training loss: 1.9063...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5412...  Training loss: 1.8833...  0.0575 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5413...  Training loss: 1.8300...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5414...  Training loss: 1.8670...  0.0566 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5415...  Training loss: 1.8327...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5416...  Training loss: 1.8394...  0.0544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5417...  Training loss: 1.8682...  0.0583 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5418...  Training loss: 1.8765...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5419...  Training loss: 1.8540...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5420...  Training loss: 1.8498...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5421...  Training loss: 1.8061...  0.0569 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5422...  Training loss: 1.8580...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5423...  Training loss: 1.8257...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5424...  Training loss: 1.8503...  0.0543 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5425...  Training loss: 1.9103...  0.0544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5426...  Training loss: 1.8694...  0.0556 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5427...  Training loss: 1.8193...  0.0576 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5428...  Training loss: 1.8455...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5429...  Training loss: 1.8486...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5430...  Training loss: 1.8376...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5431...  Training loss: 1.8298...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5432...  Training loss: 1.8664...  0.0522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5433...  Training loss: 1.8317...  0.0541 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5434...  Training loss: 1.8078...  0.0580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5435...  Training loss: 1.9082...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5436...  Training loss: 1.9026...  0.0523 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5437...  Training loss: 1.8689...  0.0582 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5438...  Training loss: 1.9233...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5439...  Training loss: 1.8135...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5440...  Training loss: 1.9169...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5441...  Training loss: 1.8450...  0.0558 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5442...  Training loss: 1.8273...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5443...  Training loss: 1.8933...  0.0580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5444...  Training loss: 1.8390...  0.0591 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5445...  Training loss: 1.9012...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5446...  Training loss: 1.8300...  0.0559 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5447...  Training loss: 1.8748...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5448...  Training loss: 1.8611...  0.0542 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5449...  Training loss: 1.8696...  0.0535 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5450...  Training loss: 1.8584...  0.0554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5451...  Training loss: 1.8466...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5452...  Training loss: 1.8801...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5453...  Training loss: 1.8460...  0.0554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5454...  Training loss: 1.8397...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5455...  Training loss: 1.8376...  0.0575 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5456...  Training loss: 1.8720...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5457...  Training loss: 1.8446...  0.0534 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5458...  Training loss: 1.9082...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5459...  Training loss: 1.7964...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5460...  Training loss: 1.8623...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5461...  Training loss: 1.8697...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5462...  Training loss: 1.8888...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5463...  Training loss: 1.9292...  0.0522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5464...  Training loss: 1.8795...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5465...  Training loss: 1.8281...  0.0570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5466...  Training loss: 1.8219...  0.0579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5467...  Training loss: 1.8703...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5468...  Training loss: 1.8294...  0.0523 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5469...  Training loss: 1.8853...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5470...  Training loss: 1.8739...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5471...  Training loss: 1.9129...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5472...  Training loss: 1.8815...  0.0582 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5473...  Training loss: 1.9075...  0.0645 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5474...  Training loss: 1.8981...  0.0531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5475...  Training loss: 1.8895...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5476...  Training loss: 1.8453...  0.0533 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5477...  Training loss: 1.8703...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5478...  Training loss: 1.8669...  0.0571 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5479...  Training loss: 1.8447...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5480...  Training loss: 1.8331...  0.0579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5481...  Training loss: 1.8386...  0.0577 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5482...  Training loss: 1.8182...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5483...  Training loss: 1.8421...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5484...  Training loss: 1.7666...  0.0542 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5485...  Training loss: 1.8752...  0.0587 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5486...  Training loss: 1.8985...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5487...  Training loss: 1.8989...  0.0556 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5488...  Training loss: 1.8740...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5489...  Training loss: 1.8558...  0.0556 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5490...  Training loss: 1.8237...  0.0541 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5491...  Training loss: 1.8521...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5492...  Training loss: 1.8293...  0.0525 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5493...  Training loss: 1.8204...  0.0527 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5494...  Training loss: 1.8455...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5495...  Training loss: 1.8759...  0.0522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5496...  Training loss: 1.8590...  0.0585 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5497...  Training loss: 1.8199...  0.0543 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5498...  Training loss: 1.8490...  0.0577 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5499...  Training loss: 1.7894...  0.0541 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5500...  Training loss: 1.8649...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5501...  Training loss: 1.8723...  0.0542 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5502...  Training loss: 1.8266...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5503...  Training loss: 1.8411...  0.0578 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5504...  Training loss: 1.8012...  0.0525 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 5505...  Training loss: 1.8974...  0.0532 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5506...  Training loss: 1.8515...  0.0540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5507...  Training loss: 1.8863...  0.0544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5508...  Training loss: 1.9061...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5509...  Training loss: 1.8974...  0.0572 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5510...  Training loss: 1.8420...  0.0541 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5511...  Training loss: 1.8726...  0.0587 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5512...  Training loss: 1.8056...  0.0544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5513...  Training loss: 1.8609...  0.0568 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5514...  Training loss: 1.8447...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5515...  Training loss: 1.8263...  0.0601 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5516...  Training loss: 1.8285...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5517...  Training loss: 1.8426...  0.0607 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5518...  Training loss: 1.8170...  0.0525 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5519...  Training loss: 1.8545...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5520...  Training loss: 1.8340...  0.0565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5521...  Training loss: 1.8935...  0.0566 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5522...  Training loss: 1.8549...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5523...  Training loss: 1.8654...  0.0566 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5524...  Training loss: 1.9273...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5525...  Training loss: 1.9534...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5526...  Training loss: 1.9213...  0.0564 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5527...  Training loss: 1.8448...  0.0579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5528...  Training loss: 1.9175...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5529...  Training loss: 1.8270...  0.0528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5530...  Training loss: 1.8775...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5531...  Training loss: 1.8682...  0.0542 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5532...  Training loss: 1.9239...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5533...  Training loss: 1.8421...  0.0525 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5534...  Training loss: 1.8436...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5535...  Training loss: 1.8533...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5536...  Training loss: 1.8720...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5537...  Training loss: 1.8147...  0.0523 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5538...  Training loss: 1.8715...  0.0533 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5539...  Training loss: 1.8787...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5540...  Training loss: 1.8794...  0.0581 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5541...  Training loss: 1.8930...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5542...  Training loss: 1.8534...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5543...  Training loss: 1.8612...  0.0557 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5544...  Training loss: 1.8869...  0.0522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5545...  Training loss: 1.8695...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5546...  Training loss: 1.8619...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5547...  Training loss: 1.8220...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5548...  Training loss: 1.8662...  0.0522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5549...  Training loss: 1.8326...  0.0579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5550...  Training loss: 1.8857...  0.0575 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5551...  Training loss: 1.8046...  0.0632 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5552...  Training loss: 1.9059...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5553...  Training loss: 1.8515...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5554...  Training loss: 1.8302...  0.0583 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5555...  Training loss: 1.8156...  0.0523 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5556...  Training loss: 1.8428...  0.0557 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5557...  Training loss: 1.8224...  0.0588 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5558...  Training loss: 1.8512...  0.0522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5559...  Training loss: 1.8783...  0.0548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5560...  Training loss: 1.8897...  0.0583 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5561...  Training loss: 1.8395...  0.0536 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5562...  Training loss: 1.8515...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5563...  Training loss: 1.8416...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5564...  Training loss: 1.8116...  0.0525 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5565...  Training loss: 1.8606...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5566...  Training loss: 1.8457...  0.0566 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5567...  Training loss: 1.8233...  0.0546 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5568...  Training loss: 1.8198...  0.0579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5569...  Training loss: 1.8349...  0.0521 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5570...  Training loss: 1.9018...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5571...  Training loss: 1.8902...  0.0549 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5572...  Training loss: 1.8877...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5573...  Training loss: 1.8186...  0.0588 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5574...  Training loss: 1.8767...  0.0557 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5575...  Training loss: 1.8105...  0.0545 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5576...  Training loss: 1.8705...  0.0524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5577...  Training loss: 1.9129...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5578...  Training loss: 1.8233...  0.0543 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5579...  Training loss: 1.8217...  0.0547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 5580...  Training loss: 1.8031...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5581...  Training loss: 1.9415...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5582...  Training loss: 1.9349...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5583...  Training loss: 1.8912...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5584...  Training loss: 1.8233...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5585...  Training loss: 1.8364...  0.0533 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5586...  Training loss: 1.8765...  0.0533 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5587...  Training loss: 1.8161...  0.0601 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5588...  Training loss: 1.8075...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5589...  Training loss: 1.7904...  0.0572 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5590...  Training loss: 1.8270...  0.0581 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5591...  Training loss: 1.8427...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5592...  Training loss: 1.8175...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5593...  Training loss: 1.8639...  0.0520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5594...  Training loss: 1.8164...  0.0578 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5595...  Training loss: 1.8851...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5596...  Training loss: 1.8841...  0.0541 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5597...  Training loss: 1.8836...  0.0536 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5598...  Training loss: 1.8565...  0.0543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5599...  Training loss: 1.8214...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5600...  Training loss: 1.8751...  0.0543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5601...  Training loss: 1.9076...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5602...  Training loss: 1.8362...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5603...  Training loss: 1.8371...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5604...  Training loss: 1.8731...  0.0557 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 5605...  Training loss: 1.8333...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5606...  Training loss: 1.8178...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5607...  Training loss: 1.8401...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5608...  Training loss: 1.8399...  0.0561 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5609...  Training loss: 1.8433...  0.0532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5610...  Training loss: 1.8217...  0.0585 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5611...  Training loss: 1.8075...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5612...  Training loss: 1.8742...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5613...  Training loss: 1.8310...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5614...  Training loss: 1.8316...  0.0584 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5615...  Training loss: 1.8369...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5616...  Training loss: 1.8464...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5617...  Training loss: 1.8550...  0.0590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5618...  Training loss: 1.8686...  0.0595 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5619...  Training loss: 1.8565...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5620...  Training loss: 1.8057...  0.0522 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5621...  Training loss: 1.8517...  0.0559 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5622...  Training loss: 1.8706...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5623...  Training loss: 1.8402...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5624...  Training loss: 1.8668...  0.0577 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5625...  Training loss: 1.8275...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5626...  Training loss: 1.8310...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5627...  Training loss: 1.7248...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5628...  Training loss: 1.8484...  0.0523 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5629...  Training loss: 1.8138...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5630...  Training loss: 1.8486...  0.0579 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5631...  Training loss: 1.8182...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5632...  Training loss: 1.8130...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5633...  Training loss: 1.8447...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5634...  Training loss: 1.8643...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5635...  Training loss: 1.8879...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5636...  Training loss: 1.8451...  0.0588 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5637...  Training loss: 1.8191...  0.0520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5638...  Training loss: 1.8507...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5639...  Training loss: 1.8524...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5640...  Training loss: 1.8829...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5641...  Training loss: 1.8552...  0.0565 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5642...  Training loss: 1.8219...  0.0522 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5643...  Training loss: 1.8725...  0.0533 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5644...  Training loss: 1.8148...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5645...  Training loss: 1.8258...  0.0543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5646...  Training loss: 1.7918...  0.0574 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5647...  Training loss: 1.8171...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5648...  Training loss: 1.8088...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5649...  Training loss: 1.8538...  0.0587 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5650...  Training loss: 1.8489...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5651...  Training loss: 1.8740...  0.0542 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5652...  Training loss: 1.8722...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5653...  Training loss: 1.7815...  0.0596 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5654...  Training loss: 1.8345...  0.0583 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5655...  Training loss: 1.8911...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5656...  Training loss: 1.8467...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5657...  Training loss: 1.8493...  0.0519 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5658...  Training loss: 1.8279...  0.0538 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5659...  Training loss: 1.8461...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5660...  Training loss: 1.8481...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5661...  Training loss: 1.8043...  0.0540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5662...  Training loss: 1.8617...  0.0540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5663...  Training loss: 1.7902...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5664...  Training loss: 1.8218...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5665...  Training loss: 1.8346...  0.0535 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5666...  Training loss: 1.8861...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5667...  Training loss: 1.8078...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5668...  Training loss: 1.8804...  0.0578 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5669...  Training loss: 1.8321...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5670...  Training loss: 1.8511...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5671...  Training loss: 1.8026...  0.0540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5672...  Training loss: 1.8975...  0.0577 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5673...  Training loss: 1.8380...  0.0536 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5674...  Training loss: 1.8480...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5675...  Training loss: 1.8374...  0.0582 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5676...  Training loss: 1.8693...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5677...  Training loss: 1.8624...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5678...  Training loss: 1.7823...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5679...  Training loss: 1.8918...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5680...  Training loss: 1.8227...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5681...  Training loss: 1.8378...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5682...  Training loss: 1.7934...  0.0568 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5683...  Training loss: 1.8727...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5684...  Training loss: 1.8910...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5685...  Training loss: 1.8447...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5686...  Training loss: 1.8181...  0.0562 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5687...  Training loss: 1.8535...  0.0624 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5688...  Training loss: 1.8239...  0.0539 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5689...  Training loss: 1.8289...  0.0532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5690...  Training loss: 1.8093...  0.0577 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5691...  Training loss: 1.7968...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5692...  Training loss: 1.8296...  0.0532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5693...  Training loss: 1.8316...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5694...  Training loss: 1.8053...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5695...  Training loss: 1.8555...  0.0541 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5696...  Training loss: 1.8802...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5697...  Training loss: 1.7894...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5698...  Training loss: 1.8752...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5699...  Training loss: 1.8215...  0.0561 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5700...  Training loss: 1.8152...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5701...  Training loss: 1.8292...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5702...  Training loss: 1.7914...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5703...  Training loss: 1.8196...  0.0559 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5704...  Training loss: 1.8755...  0.0525 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 5705...  Training loss: 1.8648...  0.0534 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5706...  Training loss: 1.8778...  0.0520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5707...  Training loss: 1.8908...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5708...  Training loss: 1.8102...  0.0582 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5709...  Training loss: 1.8372...  0.0588 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5710...  Training loss: 1.8898...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5711...  Training loss: 1.8177...  0.0523 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5712...  Training loss: 1.8689...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5713...  Training loss: 1.8774...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5714...  Training loss: 1.8275...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5715...  Training loss: 1.8088...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5716...  Training loss: 1.8558...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5717...  Training loss: 1.8385...  0.0571 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5718...  Training loss: 1.8401...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5719...  Training loss: 1.8988...  0.0575 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5720...  Training loss: 1.8435...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5721...  Training loss: 1.8622...  0.0581 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5722...  Training loss: 1.7581...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5723...  Training loss: 1.8514...  0.0556 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5724...  Training loss: 1.8238...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5725...  Training loss: 1.7953...  0.0586 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5726...  Training loss: 1.8727...  0.0582 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5727...  Training loss: 1.8765...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5728...  Training loss: 1.8697...  0.0534 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5729...  Training loss: 1.8273...  0.0521 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5730...  Training loss: 1.8546...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5731...  Training loss: 1.8652...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5732...  Training loss: 1.8220...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5733...  Training loss: 1.8398...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5734...  Training loss: 1.8832...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5735...  Training loss: 1.8127...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5736...  Training loss: 1.8337...  0.0533 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5737...  Training loss: 1.8439...  0.0522 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5738...  Training loss: 1.8587...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5739...  Training loss: 1.8628...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5740...  Training loss: 1.8028...  0.0538 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5741...  Training loss: 1.7977...  0.0575 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5742...  Training loss: 1.8188...  0.0542 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5743...  Training loss: 1.8639...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5744...  Training loss: 1.8519...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5745...  Training loss: 1.8505...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5746...  Training loss: 1.8607...  0.0575 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5747...  Training loss: 1.8663...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5748...  Training loss: 1.8540...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5749...  Training loss: 1.8487...  0.0562 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5750...  Training loss: 1.7848...  0.0559 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5751...  Training loss: 1.8225...  0.0583 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5752...  Training loss: 1.8448...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5753...  Training loss: 1.8445...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5754...  Training loss: 1.8351...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5755...  Training loss: 1.8184...  0.0522 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5756...  Training loss: 1.8377...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5757...  Training loss: 1.8682...  0.0558 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5758...  Training loss: 1.8147...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5759...  Training loss: 1.8194...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5760...  Training loss: 1.8316...  0.0578 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5761...  Training loss: 1.8129...  0.0563 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5762...  Training loss: 1.8571...  0.0562 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5763...  Training loss: 1.8470...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5764...  Training loss: 1.8061...  0.0521 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5765...  Training loss: 1.7990...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5766...  Training loss: 1.8412...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5767...  Training loss: 1.8226...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5768...  Training loss: 1.8396...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5769...  Training loss: 1.8232...  0.0562 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5770...  Training loss: 1.9005...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5771...  Training loss: 1.8528...  0.0563 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5772...  Training loss: 1.8922...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5773...  Training loss: 1.8717...  0.0540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5774...  Training loss: 1.8251...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5775...  Training loss: 1.8363...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5776...  Training loss: 1.8970...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5777...  Training loss: 1.8416...  0.0521 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5778...  Training loss: 1.9234...  0.0532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5779...  Training loss: 1.8291...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5780...  Training loss: 1.8589...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5781...  Training loss: 1.8247...  0.0561 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5782...  Training loss: 1.8261...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5783...  Training loss: 1.8209...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5784...  Training loss: 1.8136...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5785...  Training loss: 1.8525...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5786...  Training loss: 1.8077...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5787...  Training loss: 1.8655...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5788...  Training loss: 1.8357...  0.0522 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5789...  Training loss: 1.8283...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5790...  Training loss: 1.7996...  0.0574 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5791...  Training loss: 1.8457...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5792...  Training loss: 1.8330...  0.0582 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5793...  Training loss: 1.8482...  0.0521 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5794...  Training loss: 1.8716...  0.0535 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5795...  Training loss: 1.8685...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5796...  Training loss: 1.8658...  0.0558 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5797...  Training loss: 1.8740...  0.0605 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5798...  Training loss: 1.7949...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5799...  Training loss: 1.9106...  0.0585 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5800...  Training loss: 1.8755...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5801...  Training loss: 1.8668...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5802...  Training loss: 1.8974...  0.0591 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5803...  Training loss: 1.8884...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5804...  Training loss: 1.8022...  0.0563 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 5805...  Training loss: 1.8052...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5806...  Training loss: 1.8789...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5807...  Training loss: 1.8807...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5808...  Training loss: 1.8085...  0.0542 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5809...  Training loss: 1.8817...  0.0555 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5810...  Training loss: 1.8286...  0.0567 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5811...  Training loss: 1.8992...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5812...  Training loss: 1.8025...  0.0561 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5813...  Training loss: 1.8059...  0.0537 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5814...  Training loss: 1.8181...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5815...  Training loss: 1.8219...  0.0559 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5816...  Training loss: 1.8460...  0.0580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5817...  Training loss: 1.8394...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5818...  Training loss: 1.7997...  0.0555 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5819...  Training loss: 1.8145...  0.0523 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5820...  Training loss: 1.8667...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5821...  Training loss: 1.8331...  0.0522 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5822...  Training loss: 1.8036...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5823...  Training loss: 1.8268...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5824...  Training loss: 1.8070...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5825...  Training loss: 1.8332...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5826...  Training loss: 1.8494...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5827...  Training loss: 1.8613...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5828...  Training loss: 1.8346...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5829...  Training loss: 1.7896...  0.0559 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5830...  Training loss: 1.8154...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5831...  Training loss: 1.8430...  0.0558 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5832...  Training loss: 1.8031...  0.0522 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5833...  Training loss: 1.8625...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5834...  Training loss: 1.8633...  0.0523 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5835...  Training loss: 1.8895...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5836...  Training loss: 1.8433...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5837...  Training loss: 1.8298...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5838...  Training loss: 1.8278...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5839...  Training loss: 1.8570...  0.0533 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5840...  Training loss: 1.8299...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5841...  Training loss: 1.8458...  0.0543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5842...  Training loss: 1.8216...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5843...  Training loss: 1.8305...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5844...  Training loss: 1.8372...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5845...  Training loss: 1.8589...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5846...  Training loss: 1.7854...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5847...  Training loss: 1.8404...  0.0586 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5848...  Training loss: 1.8564...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5849...  Training loss: 1.8320...  0.0597 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5850...  Training loss: 1.8105...  0.0580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5851...  Training loss: 1.8044...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5852...  Training loss: 1.8251...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5853...  Training loss: 1.8252...  0.0569 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5854...  Training loss: 1.8218...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5855...  Training loss: 1.8527...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5856...  Training loss: 1.8774...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5857...  Training loss: 1.9238...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5858...  Training loss: 1.8436...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5859...  Training loss: 1.8741...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5860...  Training loss: 1.8859...  0.0583 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5861...  Training loss: 1.8390...  0.0575 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5862...  Training loss: 1.7692...  0.0534 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5863...  Training loss: 1.7890...  0.0577 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5864...  Training loss: 1.8565...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5865...  Training loss: 1.8152...  0.0591 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5866...  Training loss: 1.8539...  0.0594 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5867...  Training loss: 1.8133...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5868...  Training loss: 1.8490...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5869...  Training loss: 1.8476...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5870...  Training loss: 1.8882...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5871...  Training loss: 1.8251...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5872...  Training loss: 1.8086...  0.0568 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5873...  Training loss: 1.8342...  0.0538 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5874...  Training loss: 1.8572...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5875...  Training loss: 1.8156...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5876...  Training loss: 1.7960...  0.0579 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5877...  Training loss: 1.8388...  0.0558 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5878...  Training loss: 1.8864...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5879...  Training loss: 1.8739...  0.0565 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5880...  Training loss: 1.8579...  0.0570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5881...  Training loss: 1.8557...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5882...  Training loss: 1.9317...  0.0543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5883...  Training loss: 1.7996...  0.0566 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5884...  Training loss: 1.8727...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5885...  Training loss: 1.7976...  0.0556 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5886...  Training loss: 1.8344...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5887...  Training loss: 1.8316...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5888...  Training loss: 1.8282...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5889...  Training loss: 1.8216...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5890...  Training loss: 1.7983...  0.0570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5891...  Training loss: 1.7829...  0.0537 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5892...  Training loss: 1.8005...  0.0539 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5893...  Training loss: 1.8073...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5894...  Training loss: 1.7840...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5895...  Training loss: 1.8348...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5896...  Training loss: 1.8656...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5897...  Training loss: 1.8105...  0.0532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5898...  Training loss: 1.8081...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5899...  Training loss: 1.7991...  0.0556 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5900...  Training loss: 1.8657...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5901...  Training loss: 1.8391...  0.0560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5902...  Training loss: 1.7857...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5903...  Training loss: 1.8395...  0.0534 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5904...  Training loss: 1.8197...  0.0524 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 5905...  Training loss: 1.8183...  0.0556 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5906...  Training loss: 1.8322...  0.0521 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5907...  Training loss: 1.8047...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5908...  Training loss: 1.7811...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5909...  Training loss: 1.8359...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5910...  Training loss: 1.8124...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5911...  Training loss: 1.7969...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5912...  Training loss: 1.8196...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5913...  Training loss: 1.8100...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5914...  Training loss: 1.8165...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5915...  Training loss: 1.8349...  0.0533 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5916...  Training loss: 1.8197...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5917...  Training loss: 1.7947...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5918...  Training loss: 1.8141...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5919...  Training loss: 1.7997...  0.0523 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5920...  Training loss: 1.8332...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5921...  Training loss: 1.8257...  0.0569 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5922...  Training loss: 1.8097...  0.0594 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5923...  Training loss: 1.7886...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5924...  Training loss: 1.8065...  0.0532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5925...  Training loss: 1.8258...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5926...  Training loss: 1.8391...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5927...  Training loss: 1.8379...  0.0533 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5928...  Training loss: 1.8661...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5929...  Training loss: 1.8322...  0.0531 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5930...  Training loss: 1.8015...  0.0523 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5931...  Training loss: 1.8874...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5932...  Training loss: 1.8335...  0.0586 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5933...  Training loss: 1.8458...  0.0557 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5934...  Training loss: 1.8120...  0.0581 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5935...  Training loss: 1.8107...  0.0562 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5936...  Training loss: 1.8792...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5937...  Training loss: 1.9102...  0.0555 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5938...  Training loss: 1.8815...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5939...  Training loss: 1.8302...  0.0576 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5940...  Training loss: 1.8379...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5941...  Training loss: 1.8618...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5942...  Training loss: 1.8086...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5943...  Training loss: 1.7690...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5944...  Training loss: 1.7715...  0.0543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5945...  Training loss: 1.8304...  0.0534 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5946...  Training loss: 1.8253...  0.0541 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5947...  Training loss: 1.7896...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5948...  Training loss: 1.8756...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5949...  Training loss: 1.8441...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5950...  Training loss: 1.8155...  0.0543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5951...  Training loss: 1.8281...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5952...  Training loss: 1.8937...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5953...  Training loss: 1.8430...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5954...  Training loss: 1.8300...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5955...  Training loss: 1.8301...  0.0562 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5956...  Training loss: 1.8402...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5957...  Training loss: 1.7987...  0.0541 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5958...  Training loss: 1.8971...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5959...  Training loss: 1.8309...  0.0583 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5960...  Training loss: 1.8296...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5961...  Training loss: 1.7632...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5962...  Training loss: 1.8834...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5963...  Training loss: 1.8007...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5964...  Training loss: 1.8389...  0.0543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5965...  Training loss: 1.8387...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5966...  Training loss: 1.7580...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5967...  Training loss: 1.7684...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5968...  Training loss: 1.8309...  0.0579 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5969...  Training loss: 1.7772...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5970...  Training loss: 1.8117...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5971...  Training loss: 1.8356...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5972...  Training loss: 1.7786...  0.0563 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5973...  Training loss: 1.8311...  0.0561 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5974...  Training loss: 1.8396...  0.0589 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5975...  Training loss: 1.7985...  0.0593 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5976...  Training loss: 1.8413...  0.0533 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5977...  Training loss: 1.8220...  0.0593 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5978...  Training loss: 1.8402...  0.0555 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5979...  Training loss: 1.8078...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5980...  Training loss: 1.8496...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5981...  Training loss: 1.8690...  0.0558 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5982...  Training loss: 1.8502...  0.0577 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5983...  Training loss: 1.7932...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5984...  Training loss: 1.8689...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5985...  Training loss: 1.8907...  0.0543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5986...  Training loss: 1.8669...  0.0534 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5987...  Training loss: 1.9176...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5988...  Training loss: 1.8439...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5989...  Training loss: 1.8727...  0.0556 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5990...  Training loss: 1.8520...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5991...  Training loss: 1.8029...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5992...  Training loss: 1.8622...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5993...  Training loss: 1.8521...  0.0591 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5994...  Training loss: 1.8260...  0.0561 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5995...  Training loss: 1.7920...  0.0565 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5996...  Training loss: 1.7692...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5997...  Training loss: 1.8300...  0.0556 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5998...  Training loss: 1.8361...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 5999...  Training loss: 1.8648...  0.0563 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6000...  Training loss: 1.8176...  0.0570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6001...  Training loss: 1.7857...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6002...  Training loss: 1.8284...  0.0586 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6003...  Training loss: 1.8420...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6004...  Training loss: 1.8005...  0.0529 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 6005...  Training loss: 1.8555...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6006...  Training loss: 1.8499...  0.0570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6007...  Training loss: 1.8123...  0.0539 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6008...  Training loss: 1.8319...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6009...  Training loss: 1.8209...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6010...  Training loss: 1.7947...  0.0539 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6011...  Training loss: 1.8232...  0.0542 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6012...  Training loss: 1.8882...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6013...  Training loss: 1.8459...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6014...  Training loss: 1.8027...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6015...  Training loss: 1.7850...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6016...  Training loss: 1.8258...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6017...  Training loss: 1.7914...  0.0585 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6018...  Training loss: 1.8335...  0.0589 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6019...  Training loss: 1.8178...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6020...  Training loss: 1.8316...  0.0573 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6021...  Training loss: 1.8319...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6022...  Training loss: 1.8380...  0.0532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6023...  Training loss: 1.8837...  0.0568 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6024...  Training loss: 1.8449...  0.0558 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6025...  Training loss: 1.7604...  0.0570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6026...  Training loss: 1.8167...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6027...  Training loss: 1.8019...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6028...  Training loss: 1.7693...  0.0541 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6029...  Training loss: 1.8506...  0.0569 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6030...  Training loss: 1.8550...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6031...  Training loss: 1.8896...  0.0580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6032...  Training loss: 1.8565...  0.0532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6033...  Training loss: 1.7808...  0.0583 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6034...  Training loss: 1.8410...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6035...  Training loss: 1.8113...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6036...  Training loss: 1.8184...  0.0562 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6037...  Training loss: 1.8253...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6038...  Training loss: 1.8462...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6039...  Training loss: 1.8104...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6040...  Training loss: 1.8277...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6041...  Training loss: 1.7756...  0.0543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6042...  Training loss: 1.8525...  0.0591 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6043...  Training loss: 1.8067...  0.0560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6044...  Training loss: 1.8254...  0.0562 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6045...  Training loss: 1.8518...  0.0588 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6046...  Training loss: 1.8440...  0.0543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6047...  Training loss: 1.8094...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6048...  Training loss: 1.8360...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6049...  Training loss: 1.8486...  0.0574 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6050...  Training loss: 1.8336...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6051...  Training loss: 1.8270...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6052...  Training loss: 1.8383...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6053...  Training loss: 1.8137...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6054...  Training loss: 1.7839...  0.0567 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6055...  Training loss: 1.8976...  0.0546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6056...  Training loss: 1.8690...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6057...  Training loss: 1.8424...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6058...  Training loss: 1.8855...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6059...  Training loss: 1.7994...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6060...  Training loss: 1.8627...  0.0564 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6061...  Training loss: 1.8291...  0.0556 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6062...  Training loss: 1.8011...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6063...  Training loss: 1.8450...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6064...  Training loss: 1.8490...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6065...  Training loss: 1.8865...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6066...  Training loss: 1.8223...  0.0522 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6067...  Training loss: 1.8317...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6068...  Training loss: 1.8234...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6069...  Training loss: 1.8442...  0.0542 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6070...  Training loss: 1.8536...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6071...  Training loss: 1.8077...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6072...  Training loss: 1.8522...  0.0576 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6073...  Training loss: 1.8117...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6074...  Training loss: 1.8059...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6075...  Training loss: 1.7962...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6076...  Training loss: 1.8384...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6077...  Training loss: 1.8205...  0.0540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6078...  Training loss: 1.8693...  0.0561 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6079...  Training loss: 1.7651...  0.0594 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6080...  Training loss: 1.8440...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6081...  Training loss: 1.8287...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6082...  Training loss: 1.8629...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6083...  Training loss: 1.9029...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6084...  Training loss: 1.8330...  0.0584 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6085...  Training loss: 1.8007...  0.0588 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6086...  Training loss: 1.7825...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6087...  Training loss: 1.8618...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6088...  Training loss: 1.8217...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6089...  Training loss: 1.8734...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6090...  Training loss: 1.8558...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6091...  Training loss: 1.8776...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6092...  Training loss: 1.8452...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6093...  Training loss: 1.9111...  0.0570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6094...  Training loss: 1.8585...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6095...  Training loss: 1.8536...  0.0556 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6096...  Training loss: 1.8211...  0.0561 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6097...  Training loss: 1.8091...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6098...  Training loss: 1.8134...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6099...  Training loss: 1.8196...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6100...  Training loss: 1.8105...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6101...  Training loss: 1.8097...  0.0583 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6102...  Training loss: 1.7755...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6103...  Training loss: 1.8337...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6104...  Training loss: 1.7471...  0.0576 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 6105...  Training loss: 1.8436...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6106...  Training loss: 1.8799...  0.0543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6107...  Training loss: 1.8567...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6108...  Training loss: 1.8430...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6109...  Training loss: 1.8288...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6110...  Training loss: 1.7874...  0.0581 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6111...  Training loss: 1.8215...  0.0576 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6112...  Training loss: 1.8064...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6113...  Training loss: 1.8149...  0.0523 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6114...  Training loss: 1.8214...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6115...  Training loss: 1.8262...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6116...  Training loss: 1.8342...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6117...  Training loss: 1.7992...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6118...  Training loss: 1.8098...  0.0575 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6119...  Training loss: 1.7652...  0.0582 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6120...  Training loss: 1.8448...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6121...  Training loss: 1.8443...  0.0562 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6122...  Training loss: 1.7944...  0.0526 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6123...  Training loss: 1.8100...  0.0532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6124...  Training loss: 1.7772...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6125...  Training loss: 1.8584...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6126...  Training loss: 1.8346...  0.0591 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6127...  Training loss: 1.8499...  0.0576 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6128...  Training loss: 1.8710...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6129...  Training loss: 1.8692...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6130...  Training loss: 1.7974...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6131...  Training loss: 1.8487...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6132...  Training loss: 1.7830...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6133...  Training loss: 1.8134...  0.0560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6134...  Training loss: 1.8343...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6135...  Training loss: 1.8218...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6136...  Training loss: 1.8293...  0.0532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6137...  Training loss: 1.8270...  0.0524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6138...  Training loss: 1.7837...  0.0542 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6139...  Training loss: 1.8210...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6140...  Training loss: 1.8084...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6141...  Training loss: 1.8612...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6142...  Training loss: 1.8416...  0.0539 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6143...  Training loss: 1.8252...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6144...  Training loss: 1.8977...  0.0582 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6145...  Training loss: 1.9176...  0.0584 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6146...  Training loss: 1.8944...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6147...  Training loss: 1.8493...  0.0528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6148...  Training loss: 1.8945...  0.0521 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6149...  Training loss: 1.8234...  0.0580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6150...  Training loss: 1.8489...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6151...  Training loss: 1.8453...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6152...  Training loss: 1.8760...  0.0589 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6153...  Training loss: 1.8218...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6154...  Training loss: 1.8132...  0.0552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6155...  Training loss: 1.8261...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6156...  Training loss: 1.8476...  0.0555 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6157...  Training loss: 1.7864...  0.0555 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6158...  Training loss: 1.8386...  0.0583 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6159...  Training loss: 1.8919...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6160...  Training loss: 1.8569...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6161...  Training loss: 1.8792...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6162...  Training loss: 1.8257...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6163...  Training loss: 1.8365...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6164...  Training loss: 1.8719...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6165...  Training loss: 1.8639...  0.0557 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6166...  Training loss: 1.8310...  0.0521 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6167...  Training loss: 1.8048...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6168...  Training loss: 1.8352...  0.0575 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6169...  Training loss: 1.8067...  0.0585 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6170...  Training loss: 1.8579...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6171...  Training loss: 1.8090...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6172...  Training loss: 1.8583...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6173...  Training loss: 1.8377...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6174...  Training loss: 1.8025...  0.0557 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6175...  Training loss: 1.8066...  0.0577 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6176...  Training loss: 1.8204...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6177...  Training loss: 1.7730...  0.0562 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6178...  Training loss: 1.8331...  0.0545 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6179...  Training loss: 1.8632...  0.0532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6180...  Training loss: 1.8731...  0.0544 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6181...  Training loss: 1.8277...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6182...  Training loss: 1.8247...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6183...  Training loss: 1.8339...  0.0534 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6184...  Training loss: 1.8081...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6185...  Training loss: 1.8567...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6186...  Training loss: 1.8340...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6187...  Training loss: 1.8020...  0.0533 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6188...  Training loss: 1.7954...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6189...  Training loss: 1.8245...  0.0565 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6190...  Training loss: 1.8909...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6191...  Training loss: 1.8672...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6192...  Training loss: 1.8479...  0.0572 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6193...  Training loss: 1.7749...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6194...  Training loss: 1.8391...  0.0581 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6195...  Training loss: 1.7992...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6196...  Training loss: 1.8732...  0.0547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6197...  Training loss: 1.8672...  0.0541 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6198...  Training loss: 1.7910...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6199...  Training loss: 1.7768...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 6200...  Training loss: 1.7976...  0.0540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6201...  Training loss: 1.9096...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6202...  Training loss: 1.9020...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6203...  Training loss: 1.8863...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6204...  Training loss: 1.7906...  0.0572 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 6205...  Training loss: 1.8142...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6206...  Training loss: 1.8305...  0.0524 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6207...  Training loss: 1.7932...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6208...  Training loss: 1.7682...  0.0566 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6209...  Training loss: 1.7661...  0.0589 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6210...  Training loss: 1.7992...  0.0561 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6211...  Training loss: 1.8154...  0.0532 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6212...  Training loss: 1.7950...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6213...  Training loss: 1.8268...  0.0589 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6214...  Training loss: 1.8036...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6215...  Training loss: 1.8447...  0.0586 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6216...  Training loss: 1.8532...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6217...  Training loss: 1.8498...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6218...  Training loss: 1.8473...  0.0591 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6219...  Training loss: 1.7944...  0.0563 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6220...  Training loss: 1.8449...  0.0575 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6221...  Training loss: 1.8769...  0.0574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6222...  Training loss: 1.8256...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6223...  Training loss: 1.8069...  0.0536 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6224...  Training loss: 1.8359...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6225...  Training loss: 1.8067...  0.0562 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6226...  Training loss: 1.7940...  0.0523 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6227...  Training loss: 1.8022...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6228...  Training loss: 1.8358...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6229...  Training loss: 1.8326...  0.0541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6230...  Training loss: 1.7891...  0.0587 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6231...  Training loss: 1.8036...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6232...  Training loss: 1.8271...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6233...  Training loss: 1.8052...  0.0537 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6234...  Training loss: 1.8108...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6235...  Training loss: 1.8177...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6236...  Training loss: 1.8123...  0.0537 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6237...  Training loss: 1.8213...  0.0521 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6238...  Training loss: 1.8392...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6239...  Training loss: 1.8189...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6240...  Training loss: 1.7860...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6241...  Training loss: 1.8252...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6242...  Training loss: 1.8373...  0.0540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6243...  Training loss: 1.8326...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6244...  Training loss: 1.8417...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6245...  Training loss: 1.8139...  0.0555 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6246...  Training loss: 1.8156...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6247...  Training loss: 1.7116...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6248...  Training loss: 1.8234...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6249...  Training loss: 1.7861...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6250...  Training loss: 1.8584...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6251...  Training loss: 1.8005...  0.0557 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6252...  Training loss: 1.7795...  0.0524 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6253...  Training loss: 1.8175...  0.0524 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6254...  Training loss: 1.8515...  0.0587 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6255...  Training loss: 1.8422...  0.0579 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6256...  Training loss: 1.8226...  0.0572 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6257...  Training loss: 1.7695...  0.0535 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6258...  Training loss: 1.8332...  0.0535 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6259...  Training loss: 1.8281...  0.0545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6260...  Training loss: 1.8461...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6261...  Training loss: 1.8296...  0.0563 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6262...  Training loss: 1.7871...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6263...  Training loss: 1.8344...  0.0534 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6264...  Training loss: 1.8010...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6265...  Training loss: 1.7932...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6266...  Training loss: 1.7667...  0.0574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6267...  Training loss: 1.7837...  0.0585 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6268...  Training loss: 1.8053...  0.0545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6269...  Training loss: 1.8294...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6270...  Training loss: 1.8468...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6271...  Training loss: 1.8777...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6272...  Training loss: 1.8390...  0.0571 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6273...  Training loss: 1.7734...  0.0556 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6274...  Training loss: 1.8098...  0.0574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6275...  Training loss: 1.8716...  0.0520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6276...  Training loss: 1.8516...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6277...  Training loss: 1.8326...  0.0570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6278...  Training loss: 1.8002...  0.0567 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6279...  Training loss: 1.8217...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6280...  Training loss: 1.8680...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6281...  Training loss: 1.7591...  0.0557 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6282...  Training loss: 1.8282...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6283...  Training loss: 1.7603...  0.0534 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6284...  Training loss: 1.8234...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6285...  Training loss: 1.7879...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6286...  Training loss: 1.8638...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6287...  Training loss: 1.7870...  0.0577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6288...  Training loss: 1.8605...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6289...  Training loss: 1.8156...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6290...  Training loss: 1.8371...  0.0599 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6291...  Training loss: 1.7839...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6292...  Training loss: 1.8689...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6293...  Training loss: 1.8264...  0.0556 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6294...  Training loss: 1.8144...  0.0557 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6295...  Training loss: 1.8092...  0.0541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6296...  Training loss: 1.8454...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6297...  Training loss: 1.8605...  0.0523 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6298...  Training loss: 1.7642...  0.0541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6299...  Training loss: 1.8734...  0.0578 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6300...  Training loss: 1.7981...  0.0521 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6301...  Training loss: 1.8083...  0.0569 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6302...  Training loss: 1.8017...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6303...  Training loss: 1.8461...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6304...  Training loss: 1.8719...  0.0551 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 6305...  Training loss: 1.8235...  0.0582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6306...  Training loss: 1.7934...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6307...  Training loss: 1.8425...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6308...  Training loss: 1.7969...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6309...  Training loss: 1.8164...  0.0524 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6310...  Training loss: 1.7778...  0.0579 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6311...  Training loss: 1.7813...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6312...  Training loss: 1.8113...  0.0565 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6313...  Training loss: 1.8079...  0.0578 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6314...  Training loss: 1.7963...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6315...  Training loss: 1.8435...  0.0536 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6316...  Training loss: 1.8438...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6317...  Training loss: 1.7719...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6318...  Training loss: 1.8470...  0.0560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6319...  Training loss: 1.7870...  0.0565 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6320...  Training loss: 1.8179...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6321...  Training loss: 1.7978...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6322...  Training loss: 1.7497...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6323...  Training loss: 1.7972...  0.0552 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6324...  Training loss: 1.8324...  0.0522 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6325...  Training loss: 1.8345...  0.0557 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6326...  Training loss: 1.8676...  0.0606 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6327...  Training loss: 1.8651...  0.0570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6328...  Training loss: 1.7855...  0.0582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6329...  Training loss: 1.8191...  0.0565 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6330...  Training loss: 1.8701...  0.0532 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6331...  Training loss: 1.8187...  0.0579 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6332...  Training loss: 1.8749...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6333...  Training loss: 1.8714...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6334...  Training loss: 1.8114...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6335...  Training loss: 1.8019...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6336...  Training loss: 1.8158...  0.0574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6337...  Training loss: 1.8032...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6338...  Training loss: 1.8229...  0.0543 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6339...  Training loss: 1.8717...  0.0533 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6340...  Training loss: 1.8345...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6341...  Training loss: 1.8582...  0.0577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6342...  Training loss: 1.7428...  0.0521 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6343...  Training loss: 1.8379...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6344...  Training loss: 1.7932...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6345...  Training loss: 1.7777...  0.0543 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6346...  Training loss: 1.8413...  0.0559 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6347...  Training loss: 1.8316...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6348...  Training loss: 1.8469...  0.0552 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6349...  Training loss: 1.8222...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6350...  Training loss: 1.8377...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6351...  Training loss: 1.8396...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6352...  Training loss: 1.8055...  0.0532 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6353...  Training loss: 1.8168...  0.0520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6354...  Training loss: 1.8600...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6355...  Training loss: 1.8171...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6356...  Training loss: 1.8211...  0.0561 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6357...  Training loss: 1.8161...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6358...  Training loss: 1.8475...  0.0579 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6359...  Training loss: 1.8375...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6360...  Training loss: 1.7812...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6361...  Training loss: 1.7744...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6362...  Training loss: 1.7963...  0.0577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6363...  Training loss: 1.8286...  0.0555 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6364...  Training loss: 1.8022...  0.0582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6365...  Training loss: 1.8525...  0.0578 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6366...  Training loss: 1.8125...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6367...  Training loss: 1.8393...  0.0523 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6368...  Training loss: 1.8270...  0.0542 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6369...  Training loss: 1.7952...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6370...  Training loss: 1.7759...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6371...  Training loss: 1.8017...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6372...  Training loss: 1.8257...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6373...  Training loss: 1.8162...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6374...  Training loss: 1.8005...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6375...  Training loss: 1.7969...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6376...  Training loss: 1.8293...  0.0535 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6377...  Training loss: 1.8139...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6378...  Training loss: 1.7955...  0.0566 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6379...  Training loss: 1.8014...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6380...  Training loss: 1.8065...  0.0597 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6381...  Training loss: 1.7955...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6382...  Training loss: 1.8499...  0.0522 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6383...  Training loss: 1.8251...  0.0564 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6384...  Training loss: 1.7803...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6385...  Training loss: 1.7717...  0.0559 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6386...  Training loss: 1.8089...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6387...  Training loss: 1.7980...  0.0539 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6388...  Training loss: 1.7969...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6389...  Training loss: 1.8082...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6390...  Training loss: 1.8871...  0.0570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6391...  Training loss: 1.8166...  0.0579 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6392...  Training loss: 1.8510...  0.0541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6393...  Training loss: 1.8275...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6394...  Training loss: 1.8064...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6395...  Training loss: 1.8181...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6396...  Training loss: 1.8788...  0.0542 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6397...  Training loss: 1.8045...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6398...  Training loss: 1.8816...  0.0541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6399...  Training loss: 1.7881...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6400...  Training loss: 1.8415...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6401...  Training loss: 1.8154...  0.0557 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6402...  Training loss: 1.8099...  0.0542 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6403...  Training loss: 1.8251...  0.0565 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6404...  Training loss: 1.7923...  0.0548 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 6405...  Training loss: 1.8143...  0.0571 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6406...  Training loss: 1.7818...  0.0568 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6407...  Training loss: 1.8308...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6408...  Training loss: 1.8067...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6409...  Training loss: 1.8318...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6410...  Training loss: 1.7938...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6411...  Training loss: 1.8195...  0.0583 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6412...  Training loss: 1.8276...  0.0577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6413...  Training loss: 1.8313...  0.0564 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6414...  Training loss: 1.8478...  0.0568 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6415...  Training loss: 1.8456...  0.0523 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6416...  Training loss: 1.8263...  0.0575 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6417...  Training loss: 1.8314...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6418...  Training loss: 1.8036...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6419...  Training loss: 1.8927...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6420...  Training loss: 1.8791...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6421...  Training loss: 1.8286...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6422...  Training loss: 1.8449...  0.0524 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6423...  Training loss: 1.8859...  0.0555 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6424...  Training loss: 1.7843...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6425...  Training loss: 1.8189...  0.0540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6426...  Training loss: 1.8417...  0.0583 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6427...  Training loss: 1.8490...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6428...  Training loss: 1.7777...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6429...  Training loss: 1.8284...  0.0605 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6430...  Training loss: 1.7915...  0.0565 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6431...  Training loss: 1.8748...  0.0552 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6432...  Training loss: 1.7884...  0.0552 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6433...  Training loss: 1.8044...  0.0586 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6434...  Training loss: 1.7963...  0.0533 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6435...  Training loss: 1.7689...  0.0545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6436...  Training loss: 1.8386...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6437...  Training loss: 1.8100...  0.0532 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6438...  Training loss: 1.7786...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6439...  Training loss: 1.7939...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6440...  Training loss: 1.8346...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6441...  Training loss: 1.7764...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6442...  Training loss: 1.7852...  0.0577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6443...  Training loss: 1.7957...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6444...  Training loss: 1.8107...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6445...  Training loss: 1.8050...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6446...  Training loss: 1.8142...  0.0583 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6447...  Training loss: 1.8286...  0.0572 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6448...  Training loss: 1.8298...  0.0556 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6449...  Training loss: 1.7787...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6450...  Training loss: 1.7912...  0.0575 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6451...  Training loss: 1.8124...  0.0545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6452...  Training loss: 1.8078...  0.0545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6453...  Training loss: 1.8282...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6454...  Training loss: 1.8328...  0.0535 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6455...  Training loss: 1.8724...  0.0566 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6456...  Training loss: 1.8276...  0.0545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6457...  Training loss: 1.8094...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6458...  Training loss: 1.8158...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6459...  Training loss: 1.8174...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6460...  Training loss: 1.8062...  0.0587 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6461...  Training loss: 1.8452...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6462...  Training loss: 1.7693...  0.0524 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6463...  Training loss: 1.8265...  0.0579 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6464...  Training loss: 1.8070...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6465...  Training loss: 1.8476...  0.0584 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6466...  Training loss: 1.7483...  0.0572 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6467...  Training loss: 1.8234...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6468...  Training loss: 1.8384...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6469...  Training loss: 1.8101...  0.0521 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6470...  Training loss: 1.7807...  0.0543 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6471...  Training loss: 1.7922...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6472...  Training loss: 1.8214...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6473...  Training loss: 1.7961...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6474...  Training loss: 1.8200...  0.0572 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6475...  Training loss: 1.8417...  0.0561 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6476...  Training loss: 1.8343...  0.0545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6477...  Training loss: 1.8922...  0.0562 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6478...  Training loss: 1.8480...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6479...  Training loss: 1.8409...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6480...  Training loss: 1.8576...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6481...  Training loss: 1.8298...  0.0583 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6482...  Training loss: 1.7618...  0.0577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6483...  Training loss: 1.7990...  0.0568 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6484...  Training loss: 1.8495...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6485...  Training loss: 1.7845...  0.0556 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6486...  Training loss: 1.8361...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6487...  Training loss: 1.8061...  0.0522 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6488...  Training loss: 1.8162...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6489...  Training loss: 1.8214...  0.0585 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6490...  Training loss: 1.8548...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6491...  Training loss: 1.8266...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6492...  Training loss: 1.8003...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6493...  Training loss: 1.7922...  0.0532 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6494...  Training loss: 1.8226...  0.0541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6495...  Training loss: 1.8049...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6496...  Training loss: 1.7785...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6497...  Training loss: 1.8343...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6498...  Training loss: 1.8595...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6499...  Training loss: 1.8597...  0.0557 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6500...  Training loss: 1.7949...  0.0584 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6501...  Training loss: 1.8568...  0.0594 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6502...  Training loss: 1.8728...  0.0572 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6503...  Training loss: 1.7748...  0.0543 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6504...  Training loss: 1.8300...  0.0529 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 6505...  Training loss: 1.8106...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6506...  Training loss: 1.8068...  0.0587 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6507...  Training loss: 1.8116...  0.0569 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6508...  Training loss: 1.8019...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6509...  Training loss: 1.8051...  0.0533 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6510...  Training loss: 1.7868...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6511...  Training loss: 1.7726...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6512...  Training loss: 1.7742...  0.0543 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6513...  Training loss: 1.7780...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6514...  Training loss: 1.7719...  0.0545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6515...  Training loss: 1.8073...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6516...  Training loss: 1.8353...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6517...  Training loss: 1.7966...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6518...  Training loss: 1.7527...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6519...  Training loss: 1.7916...  0.0601 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6520...  Training loss: 1.8597...  0.0573 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6521...  Training loss: 1.7940...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6522...  Training loss: 1.7593...  0.0559 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6523...  Training loss: 1.8335...  0.0545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6524...  Training loss: 1.8061...  0.0577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6525...  Training loss: 1.7815...  0.0541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6526...  Training loss: 1.8002...  0.0555 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6527...  Training loss: 1.7739...  0.0578 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6528...  Training loss: 1.7605...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6529...  Training loss: 1.7947...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6530...  Training loss: 1.7916...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6531...  Training loss: 1.7866...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6532...  Training loss: 1.7925...  0.0556 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6533...  Training loss: 1.7949...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6534...  Training loss: 1.7938...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6535...  Training loss: 1.7812...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6536...  Training loss: 1.8033...  0.0542 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6537...  Training loss: 1.7625...  0.0580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6538...  Training loss: 1.7908...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6539...  Training loss: 1.7667...  0.0542 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6540...  Training loss: 1.8225...  0.0532 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6541...  Training loss: 1.8329...  0.0524 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6542...  Training loss: 1.8116...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6543...  Training loss: 1.7647...  0.0537 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6544...  Training loss: 1.8053...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6545...  Training loss: 1.7980...  0.0536 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6546...  Training loss: 1.8241...  0.0563 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6547...  Training loss: 1.8236...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6548...  Training loss: 1.8304...  0.0574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6549...  Training loss: 1.7992...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6550...  Training loss: 1.7892...  0.0568 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6551...  Training loss: 1.8295...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6552...  Training loss: 1.8216...  0.0552 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6553...  Training loss: 1.8280...  0.0589 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6554...  Training loss: 1.7820...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6555...  Training loss: 1.7823...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6556...  Training loss: 1.8473...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6557...  Training loss: 1.8896...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6558...  Training loss: 1.8499...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6559...  Training loss: 1.8067...  0.0543 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6560...  Training loss: 1.8099...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6561...  Training loss: 1.8404...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6562...  Training loss: 1.7856...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6563...  Training loss: 1.7572...  0.0570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6564...  Training loss: 1.7675...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6565...  Training loss: 1.7963...  0.0565 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6566...  Training loss: 1.8171...  0.0536 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6567...  Training loss: 1.7871...  0.0522 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6568...  Training loss: 1.8583...  0.0545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6569...  Training loss: 1.8382...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6570...  Training loss: 1.7890...  0.0541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6571...  Training loss: 1.8362...  0.0564 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6572...  Training loss: 1.8743...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6573...  Training loss: 1.8275...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6574...  Training loss: 1.8112...  0.0524 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6575...  Training loss: 1.7830...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6576...  Training loss: 1.8013...  0.0522 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6577...  Training loss: 1.7865...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6578...  Training loss: 1.8751...  0.0520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6579...  Training loss: 1.8125...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6580...  Training loss: 1.8216...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6581...  Training loss: 1.7420...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6582...  Training loss: 1.8510...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6583...  Training loss: 1.7807...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6584...  Training loss: 1.8176...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6585...  Training loss: 1.8260...  0.0545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6586...  Training loss: 1.7309...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6587...  Training loss: 1.7447...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6588...  Training loss: 1.8265...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6589...  Training loss: 1.7700...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6590...  Training loss: 1.7804...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6591...  Training loss: 1.8522...  0.0580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6592...  Training loss: 1.7633...  0.0567 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6593...  Training loss: 1.7863...  0.0552 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6594...  Training loss: 1.8064...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6595...  Training loss: 1.7590...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6596...  Training loss: 1.8174...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6597...  Training loss: 1.7990...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6598...  Training loss: 1.8347...  0.0582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6599...  Training loss: 1.7999...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6600...  Training loss: 1.8128...  0.0562 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6601...  Training loss: 1.8603...  0.0534 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6602...  Training loss: 1.8295...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6603...  Training loss: 1.7665...  0.0558 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6604...  Training loss: 1.8235...  0.0550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 6605...  Training loss: 1.8488...  0.0628 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6606...  Training loss: 1.8503...  0.0569 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6607...  Training loss: 1.8669...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6608...  Training loss: 1.8267...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6609...  Training loss: 1.8845...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6610...  Training loss: 1.8563...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6611...  Training loss: 1.7954...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6612...  Training loss: 1.8397...  0.0533 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6613...  Training loss: 1.8386...  0.0560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6614...  Training loss: 1.8065...  0.0560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6615...  Training loss: 1.7628...  0.0558 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6616...  Training loss: 1.7856...  0.0583 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6617...  Training loss: 1.7897...  0.0556 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6618...  Training loss: 1.8105...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6619...  Training loss: 1.8606...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6620...  Training loss: 1.8207...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6621...  Training loss: 1.7749...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6622...  Training loss: 1.8149...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6623...  Training loss: 1.8268...  0.0582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6624...  Training loss: 1.7718...  0.0573 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6625...  Training loss: 1.8642...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6626...  Training loss: 1.8308...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6627...  Training loss: 1.7996...  0.0583 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6628...  Training loss: 1.8248...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6629...  Training loss: 1.7972...  0.0621 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6630...  Training loss: 1.7721...  0.0532 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6631...  Training loss: 1.8011...  0.0582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6632...  Training loss: 1.8470...  0.0567 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6633...  Training loss: 1.8354...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6634...  Training loss: 1.7805...  0.0541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6635...  Training loss: 1.7692...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6636...  Training loss: 1.7859...  0.0541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6637...  Training loss: 1.7727...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6638...  Training loss: 1.8073...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6639...  Training loss: 1.7993...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6640...  Training loss: 1.7895...  0.0524 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6641...  Training loss: 1.7908...  0.0568 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6642...  Training loss: 1.7757...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6643...  Training loss: 1.8476...  0.0533 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6644...  Training loss: 1.8255...  0.0542 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6645...  Training loss: 1.7195...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6646...  Training loss: 1.8029...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6647...  Training loss: 1.7734...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6648...  Training loss: 1.7581...  0.0563 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6649...  Training loss: 1.8172...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6650...  Training loss: 1.8463...  0.0564 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6651...  Training loss: 1.8480...  0.0574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6652...  Training loss: 1.8318...  0.0561 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6653...  Training loss: 1.7769...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6654...  Training loss: 1.8189...  0.0552 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6655...  Training loss: 1.7851...  0.0571 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6656...  Training loss: 1.8111...  0.0564 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6657...  Training loss: 1.8011...  0.0534 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6658...  Training loss: 1.8327...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6659...  Training loss: 1.7915...  0.0605 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6660...  Training loss: 1.8141...  0.0559 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6661...  Training loss: 1.7747...  0.0583 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6662...  Training loss: 1.8232...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6663...  Training loss: 1.7581...  0.0557 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6664...  Training loss: 1.7727...  0.0556 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6665...  Training loss: 1.8382...  0.0592 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6666...  Training loss: 1.8347...  0.0538 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6667...  Training loss: 1.7898...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6668...  Training loss: 1.7982...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6669...  Training loss: 1.8175...  0.0558 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6670...  Training loss: 1.7997...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6671...  Training loss: 1.8081...  0.0579 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6672...  Training loss: 1.8195...  0.0521 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6673...  Training loss: 1.7935...  0.0575 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6674...  Training loss: 1.7734...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6675...  Training loss: 1.8667...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6676...  Training loss: 1.8511...  0.0552 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6677...  Training loss: 1.8328...  0.0543 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6678...  Training loss: 1.8805...  0.0521 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6679...  Training loss: 1.7714...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6680...  Training loss: 1.8707...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6681...  Training loss: 1.8155...  0.0539 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6682...  Training loss: 1.7686...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6683...  Training loss: 1.8127...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6684...  Training loss: 1.8200...  0.0579 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6685...  Training loss: 1.8722...  0.0542 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6686...  Training loss: 1.7962...  0.0535 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6687...  Training loss: 1.8289...  0.0582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6688...  Training loss: 1.8164...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6689...  Training loss: 1.8282...  0.0533 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6690...  Training loss: 1.8098...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6691...  Training loss: 1.8229...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6692...  Training loss: 1.8103...  0.0524 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6693...  Training loss: 1.7945...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6694...  Training loss: 1.7930...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6695...  Training loss: 1.7933...  0.0593 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6696...  Training loss: 1.8202...  0.0558 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6697...  Training loss: 1.8004...  0.0539 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6698...  Training loss: 1.8592...  0.0586 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6699...  Training loss: 1.7380...  0.0592 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6700...  Training loss: 1.7963...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6701...  Training loss: 1.8276...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6702...  Training loss: 1.8437...  0.0580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6703...  Training loss: 1.8746...  0.0544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6704...  Training loss: 1.8121...  0.0529 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 6705...  Training loss: 1.7896...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6706...  Training loss: 1.7750...  0.0532 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6707...  Training loss: 1.8305...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6708...  Training loss: 1.7856...  0.0574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6709...  Training loss: 1.8351...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6710...  Training loss: 1.8152...  0.0552 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6711...  Training loss: 1.8692...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6712...  Training loss: 1.8407...  0.0566 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6713...  Training loss: 1.8651...  0.0618 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6714...  Training loss: 1.8503...  0.0563 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6715...  Training loss: 1.8200...  0.0538 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6716...  Training loss: 1.8048...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6717...  Training loss: 1.7990...  0.0533 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6718...  Training loss: 1.7808...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6719...  Training loss: 1.7872...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6720...  Training loss: 1.7922...  0.0572 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6721...  Training loss: 1.8045...  0.0574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6722...  Training loss: 1.7665...  0.0564 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6723...  Training loss: 1.8041...  0.0555 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6724...  Training loss: 1.7266...  0.0590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6725...  Training loss: 1.8320...  0.0562 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6726...  Training loss: 1.8741...  0.0523 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6727...  Training loss: 1.8227...  0.0537 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6728...  Training loss: 1.8170...  0.0557 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6729...  Training loss: 1.8064...  0.0533 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6730...  Training loss: 1.7845...  0.0588 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6731...  Training loss: 1.8078...  0.0576 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6732...  Training loss: 1.7974...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6733...  Training loss: 1.8030...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6734...  Training loss: 1.8129...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6735...  Training loss: 1.8324...  0.0576 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6736...  Training loss: 1.7966...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6737...  Training loss: 1.7635...  0.0574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6738...  Training loss: 1.8232...  0.0534 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6739...  Training loss: 1.7516...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6740...  Training loss: 1.8377...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6741...  Training loss: 1.8313...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6742...  Training loss: 1.8100...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6743...  Training loss: 1.7901...  0.0522 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6744...  Training loss: 1.7703...  0.0574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6745...  Training loss: 1.8345...  0.0551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6746...  Training loss: 1.7903...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6747...  Training loss: 1.8607...  0.0533 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6748...  Training loss: 1.8517...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6749...  Training loss: 1.8360...  0.0568 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6750...  Training loss: 1.7766...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6751...  Training loss: 1.8342...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6752...  Training loss: 1.7473...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6753...  Training loss: 1.8062...  0.0578 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6754...  Training loss: 1.7943...  0.0533 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6755...  Training loss: 1.7794...  0.0577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6756...  Training loss: 1.8089...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6757...  Training loss: 1.8105...  0.0555 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6758...  Training loss: 1.7696...  0.0545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6759...  Training loss: 1.7988...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6760...  Training loss: 1.7841...  0.0569 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6761...  Training loss: 1.8517...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6762...  Training loss: 1.8182...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6763...  Training loss: 1.8053...  0.0525 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6764...  Training loss: 1.8582...  0.0560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6765...  Training loss: 1.9103...  0.0577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6766...  Training loss: 1.8845...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6767...  Training loss: 1.8028...  0.0609 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6768...  Training loss: 1.8738...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6769...  Training loss: 1.7836...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6770...  Training loss: 1.8251...  0.0546 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6771...  Training loss: 1.8246...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6772...  Training loss: 1.8646...  0.0577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6773...  Training loss: 1.8102...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6774...  Training loss: 1.7940...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6775...  Training loss: 1.8206...  0.0523 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6776...  Training loss: 1.8313...  0.0568 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6777...  Training loss: 1.7631...  0.0533 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6778...  Training loss: 1.8276...  0.0537 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6779...  Training loss: 1.8509...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6780...  Training loss: 1.8347...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6781...  Training loss: 1.8388...  0.0519 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6782...  Training loss: 1.8066...  0.0582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6783...  Training loss: 1.8161...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6784...  Training loss: 1.8541...  0.0542 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6785...  Training loss: 1.8378...  0.0578 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6786...  Training loss: 1.8092...  0.0547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6787...  Training loss: 1.7716...  0.0575 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6788...  Training loss: 1.8144...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6789...  Training loss: 1.7883...  0.0559 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6790...  Training loss: 1.8496...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6791...  Training loss: 1.7716...  0.0535 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6792...  Training loss: 1.8493...  0.0565 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6793...  Training loss: 1.7943...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6794...  Training loss: 1.7541...  0.0566 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6795...  Training loss: 1.7913...  0.0541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6796...  Training loss: 1.7659...  0.0533 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6797...  Training loss: 1.7585...  0.0581 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6798...  Training loss: 1.8263...  0.0534 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6799...  Training loss: 1.8276...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6800...  Training loss: 1.8193...  0.0560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6801...  Training loss: 1.7899...  0.0577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6802...  Training loss: 1.8006...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6803...  Training loss: 1.8156...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6804...  Training loss: 1.7869...  0.0546 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 6805...  Training loss: 1.8201...  0.0554 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6806...  Training loss: 1.8255...  0.0578 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6807...  Training loss: 1.7650...  0.0564 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6808...  Training loss: 1.7797...  0.0548 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6809...  Training loss: 1.7941...  0.0541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6810...  Training loss: 1.8663...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6811...  Training loss: 1.8715...  0.0534 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6812...  Training loss: 1.8300...  0.0555 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6813...  Training loss: 1.7823...  0.0557 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6814...  Training loss: 1.8179...  0.0529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6815...  Training loss: 1.7750...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6816...  Training loss: 1.8372...  0.0591 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6817...  Training loss: 1.8509...  0.0537 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6818...  Training loss: 1.7621...  0.0560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6819...  Training loss: 1.7490...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 6820...  Training loss: 1.7642...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6821...  Training loss: 1.8914...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6822...  Training loss: 1.8847...  0.0537 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6823...  Training loss: 1.8745...  0.0525 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6824...  Training loss: 1.7681...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6825...  Training loss: 1.8201...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6826...  Training loss: 1.8192...  0.0576 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6827...  Training loss: 1.7711...  0.0524 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6828...  Training loss: 1.7592...  0.0587 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6829...  Training loss: 1.7375...  0.0541 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6830...  Training loss: 1.7702...  0.0526 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6831...  Training loss: 1.7758...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6832...  Training loss: 1.7696...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6833...  Training loss: 1.7966...  0.0573 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6834...  Training loss: 1.7686...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6835...  Training loss: 1.8282...  0.0595 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6836...  Training loss: 1.8444...  0.0526 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6837...  Training loss: 1.8183...  0.0602 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6838...  Training loss: 1.8210...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6839...  Training loss: 1.7543...  0.0565 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6840...  Training loss: 1.8109...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6841...  Training loss: 1.8726...  0.0570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6842...  Training loss: 1.7849...  0.0584 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6843...  Training loss: 1.7668...  0.0552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6844...  Training loss: 1.8114...  0.0578 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6845...  Training loss: 1.7912...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6846...  Training loss: 1.7720...  0.0573 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6847...  Training loss: 1.7956...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6848...  Training loss: 1.8026...  0.0591 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6849...  Training loss: 1.8172...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6850...  Training loss: 1.7709...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6851...  Training loss: 1.7671...  0.0557 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6852...  Training loss: 1.8208...  0.0582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6853...  Training loss: 1.7934...  0.0563 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6854...  Training loss: 1.7860...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6855...  Training loss: 1.7876...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6856...  Training loss: 1.8003...  0.0577 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6857...  Training loss: 1.8034...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6858...  Training loss: 1.8160...  0.0552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6859...  Training loss: 1.8265...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6860...  Training loss: 1.7850...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6861...  Training loss: 1.8244...  0.0556 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6862...  Training loss: 1.8241...  0.0561 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6863...  Training loss: 1.8098...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6864...  Training loss: 1.8409...  0.0542 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6865...  Training loss: 1.7710...  0.0564 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6866...  Training loss: 1.7995...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6867...  Training loss: 1.6867...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6868...  Training loss: 1.7811...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6869...  Training loss: 1.7708...  0.0582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6870...  Training loss: 1.8195...  0.0565 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6871...  Training loss: 1.7665...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6872...  Training loss: 1.7802...  0.0536 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6873...  Training loss: 1.7858...  0.0575 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6874...  Training loss: 1.8176...  0.0535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6875...  Training loss: 1.8098...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6876...  Training loss: 1.8075...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6877...  Training loss: 1.7781...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6878...  Training loss: 1.7950...  0.0583 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6879...  Training loss: 1.7885...  0.0561 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6880...  Training loss: 1.8310...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6881...  Training loss: 1.7990...  0.0536 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6882...  Training loss: 1.7668...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6883...  Training loss: 1.8250...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6884...  Training loss: 1.7756...  0.0557 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6885...  Training loss: 1.7679...  0.0539 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6886...  Training loss: 1.7419...  0.0542 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6887...  Training loss: 1.7676...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6888...  Training loss: 1.7643...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6889...  Training loss: 1.8006...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6890...  Training loss: 1.8161...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6891...  Training loss: 1.8447...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6892...  Training loss: 1.8235...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6893...  Training loss: 1.7349...  0.0524 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6894...  Training loss: 1.7900...  0.0582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6895...  Training loss: 1.8602...  0.0526 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6896...  Training loss: 1.8363...  0.0569 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6897...  Training loss: 1.8006...  0.0587 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6898...  Training loss: 1.7748...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6899...  Training loss: 1.8200...  0.0526 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6900...  Training loss: 1.8046...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6901...  Training loss: 1.7473...  0.0537 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6902...  Training loss: 1.7948...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6903...  Training loss: 1.7496...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6904...  Training loss: 1.7739...  0.0527 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 6905...  Training loss: 1.7949...  0.0537 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6906...  Training loss: 1.8460...  0.0586 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6907...  Training loss: 1.7716...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6908...  Training loss: 1.8565...  0.0578 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6909...  Training loss: 1.7900...  0.0583 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6910...  Training loss: 1.7956...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6911...  Training loss: 1.7593...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6912...  Training loss: 1.8383...  0.0552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6913...  Training loss: 1.8090...  0.0555 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6914...  Training loss: 1.7981...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6915...  Training loss: 1.8049...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6916...  Training loss: 1.8355...  0.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6917...  Training loss: 1.8268...  0.0542 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6918...  Training loss: 1.7528...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6919...  Training loss: 1.8402...  0.0580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6920...  Training loss: 1.7964...  0.0578 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6921...  Training loss: 1.7795...  0.0557 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6922...  Training loss: 1.7751...  0.0569 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6923...  Training loss: 1.8072...  0.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6924...  Training loss: 1.8420...  0.0578 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6925...  Training loss: 1.7988...  0.0552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6926...  Training loss: 1.7458...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6927...  Training loss: 1.8204...  0.0582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6928...  Training loss: 1.7910...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6929...  Training loss: 1.7943...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6930...  Training loss: 1.7606...  0.0590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6931...  Training loss: 1.7436...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6932...  Training loss: 1.7828...  0.0558 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6933...  Training loss: 1.7832...  0.0573 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6934...  Training loss: 1.7761...  0.0542 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6935...  Training loss: 1.8079...  0.0564 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6936...  Training loss: 1.8270...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6937...  Training loss: 1.7531...  0.0536 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6938...  Training loss: 1.8514...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6939...  Training loss: 1.7827...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6940...  Training loss: 1.8067...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6941...  Training loss: 1.7825...  0.0582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6942...  Training loss: 1.7360...  0.0621 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6943...  Training loss: 1.7750...  0.0556 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6944...  Training loss: 1.8314...  0.0559 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6945...  Training loss: 1.8159...  0.0550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6946...  Training loss: 1.8291...  0.0541 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6947...  Training loss: 1.8538...  0.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6948...  Training loss: 1.7608...  0.0799 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6949...  Training loss: 1.7937...  0.0606 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6950...  Training loss: 1.8592...  0.0572 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6951...  Training loss: 1.7922...  0.0585 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6952...  Training loss: 1.8575...  0.0568 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6953...  Training loss: 1.8385...  0.0562 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6954...  Training loss: 1.8240...  0.0556 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6955...  Training loss: 1.7745...  0.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6956...  Training loss: 1.8003...  0.0557 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6957...  Training loss: 1.8142...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6958...  Training loss: 1.8205...  0.0558 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6959...  Training loss: 1.8374...  0.0580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6960...  Training loss: 1.8022...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6961...  Training loss: 1.8680...  0.0595 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6962...  Training loss: 1.7246...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6963...  Training loss: 1.8136...  0.0581 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6964...  Training loss: 1.7801...  0.0540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6965...  Training loss: 1.7366...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6966...  Training loss: 1.8150...  0.0600 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6967...  Training loss: 1.8117...  0.0535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6968...  Training loss: 1.8036...  0.0589 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6969...  Training loss: 1.8148...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6970...  Training loss: 1.8334...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6971...  Training loss: 1.8201...  0.0579 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6972...  Training loss: 1.7805...  0.0552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6973...  Training loss: 1.7926...  0.0600 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6974...  Training loss: 1.8415...  0.0539 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6975...  Training loss: 1.7816...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6976...  Training loss: 1.8105...  0.0561 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6977...  Training loss: 1.7832...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6978...  Training loss: 1.8446...  0.0552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6979...  Training loss: 1.8022...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6980...  Training loss: 1.7489...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6981...  Training loss: 1.7432...  0.0565 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6982...  Training loss: 1.7670...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6983...  Training loss: 1.8220...  0.0535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6984...  Training loss: 1.7996...  0.0537 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6985...  Training loss: 1.8223...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6986...  Training loss: 1.7959...  0.0577 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6987...  Training loss: 1.8230...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6988...  Training loss: 1.8235...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6989...  Training loss: 1.7812...  0.0559 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6990...  Training loss: 1.7780...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6991...  Training loss: 1.7819...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6992...  Training loss: 1.8104...  0.0583 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6993...  Training loss: 1.7857...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6994...  Training loss: 1.7667...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6995...  Training loss: 1.7895...  0.0567 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6996...  Training loss: 1.8072...  0.0535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6997...  Training loss: 1.8007...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6998...  Training loss: 1.7703...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 6999...  Training loss: 1.7622...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7000...  Training loss: 1.7644...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7001...  Training loss: 1.7736...  0.0584 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7002...  Training loss: 1.8179...  0.0535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7003...  Training loss: 1.7973...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7004...  Training loss: 1.7508...  0.0586 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 7005...  Training loss: 1.7623...  0.0588 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7006...  Training loss: 1.7860...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7007...  Training loss: 1.7840...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7008...  Training loss: 1.7974...  0.0570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7009...  Training loss: 1.7916...  0.0590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7010...  Training loss: 1.8527...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7011...  Training loss: 1.8098...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7012...  Training loss: 1.8413...  0.0525 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7013...  Training loss: 1.8191...  0.0565 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7014...  Training loss: 1.7799...  0.0537 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7015...  Training loss: 1.7633...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7016...  Training loss: 1.8620...  0.0541 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7017...  Training loss: 1.8156...  0.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7018...  Training loss: 1.8849...  0.0558 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7019...  Training loss: 1.7674...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7020...  Training loss: 1.8187...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7021...  Training loss: 1.7785...  0.0541 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7022...  Training loss: 1.7855...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7023...  Training loss: 1.7905...  0.0522 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7024...  Training loss: 1.7482...  0.0565 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7025...  Training loss: 1.8055...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7026...  Training loss: 1.7668...  0.0575 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7027...  Training loss: 1.8324...  0.0539 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7028...  Training loss: 1.7949...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7029...  Training loss: 1.7914...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7030...  Training loss: 1.7829...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7031...  Training loss: 1.7864...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7032...  Training loss: 1.8137...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7033...  Training loss: 1.8441...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7034...  Training loss: 1.8188...  0.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7035...  Training loss: 1.8107...  0.0526 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7036...  Training loss: 1.8142...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7037...  Training loss: 1.8265...  0.0590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7038...  Training loss: 1.7799...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7039...  Training loss: 1.8742...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7040...  Training loss: 1.8345...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7041...  Training loss: 1.8127...  0.0559 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7042...  Training loss: 1.8294...  0.0574 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7043...  Training loss: 1.8400...  0.0579 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7044...  Training loss: 1.7640...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7045...  Training loss: 1.7871...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7046...  Training loss: 1.8261...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7047...  Training loss: 1.8386...  0.0580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7048...  Training loss: 1.7485...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7049...  Training loss: 1.8024...  0.0581 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7050...  Training loss: 1.7778...  0.0525 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7051...  Training loss: 1.8641...  0.0583 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7052...  Training loss: 1.7651...  0.0550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7053...  Training loss: 1.7597...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7054...  Training loss: 1.7808...  0.0550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7055...  Training loss: 1.7647...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7056...  Training loss: 1.8360...  0.0552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7057...  Training loss: 1.8014...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7058...  Training loss: 1.7661...  0.0573 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7059...  Training loss: 1.7866...  0.0550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7060...  Training loss: 1.8171...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7061...  Training loss: 1.7582...  0.0561 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7062...  Training loss: 1.7541...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7063...  Training loss: 1.7632...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7064...  Training loss: 1.7642...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7065...  Training loss: 1.7833...  0.0559 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7066...  Training loss: 1.7769...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7067...  Training loss: 1.8013...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7068...  Training loss: 1.8236...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7069...  Training loss: 1.7529...  0.0578 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7070...  Training loss: 1.7820...  0.0555 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7071...  Training loss: 1.7834...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7072...  Training loss: 1.7632...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7073...  Training loss: 1.8248...  0.0543 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7074...  Training loss: 1.7939...  0.0550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7075...  Training loss: 1.8449...  0.0582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7076...  Training loss: 1.7769...  0.0594 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7077...  Training loss: 1.7944...  0.0561 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7078...  Training loss: 1.7626...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7079...  Training loss: 1.8031...  0.0575 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7080...  Training loss: 1.7766...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7081...  Training loss: 1.8385...  0.0585 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7082...  Training loss: 1.7622...  0.0576 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7083...  Training loss: 1.8168...  0.0590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7084...  Training loss: 1.8072...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7085...  Training loss: 1.8282...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7086...  Training loss: 1.7390...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7087...  Training loss: 1.8019...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7088...  Training loss: 1.7909...  0.0563 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7089...  Training loss: 1.7678...  0.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7090...  Training loss: 1.7572...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7091...  Training loss: 1.7537...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7092...  Training loss: 1.7983...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7093...  Training loss: 1.7875...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7094...  Training loss: 1.7773...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7095...  Training loss: 1.8162...  0.0566 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7096...  Training loss: 1.8371...  0.0542 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7097...  Training loss: 1.8852...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7098...  Training loss: 1.8190...  0.0593 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7099...  Training loss: 1.8427...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7100...  Training loss: 1.8523...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7101...  Training loss: 1.8286...  0.0564 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7102...  Training loss: 1.7413...  0.0563 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7103...  Training loss: 1.7434...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7104...  Training loss: 1.8245...  0.0539 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 7105...  Training loss: 1.7743...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7106...  Training loss: 1.8169...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7107...  Training loss: 1.7873...  0.0593 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7108...  Training loss: 1.7953...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7109...  Training loss: 1.8007...  0.0596 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7110...  Training loss: 1.8243...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7111...  Training loss: 1.7871...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7112...  Training loss: 1.8032...  0.0563 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7113...  Training loss: 1.7686...  0.0561 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7114...  Training loss: 1.8184...  0.0567 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7115...  Training loss: 1.7797...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7116...  Training loss: 1.7654...  0.0556 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7117...  Training loss: 1.7990...  0.0559 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7118...  Training loss: 1.8454...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7119...  Training loss: 1.8325...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7120...  Training loss: 1.7962...  0.0535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7121...  Training loss: 1.8574...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7122...  Training loss: 1.8835...  0.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7123...  Training loss: 1.7425...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7124...  Training loss: 1.8292...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7125...  Training loss: 1.7869...  0.0526 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7126...  Training loss: 1.7873...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7127...  Training loss: 1.8121...  0.0524 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7128...  Training loss: 1.7768...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7129...  Training loss: 1.7966...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7130...  Training loss: 1.7687...  0.0559 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7131...  Training loss: 1.7598...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7132...  Training loss: 1.7541...  0.0574 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7133...  Training loss: 1.7613...  0.0573 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7134...  Training loss: 1.7452...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7135...  Training loss: 1.7989...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7136...  Training loss: 1.8267...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7137...  Training loss: 1.7640...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7138...  Training loss: 1.7180...  0.0522 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7139...  Training loss: 1.7670...  0.0536 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7140...  Training loss: 1.8521...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7141...  Training loss: 1.7841...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7142...  Training loss: 1.7563...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7143...  Training loss: 1.8041...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7144...  Training loss: 1.7909...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7145...  Training loss: 1.7555...  0.0558 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7146...  Training loss: 1.7679...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7147...  Training loss: 1.7691...  0.0566 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7148...  Training loss: 1.7584...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7149...  Training loss: 1.7969...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7150...  Training loss: 1.7859...  0.0564 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7151...  Training loss: 1.7737...  0.0559 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7152...  Training loss: 1.7798...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7153...  Training loss: 1.7722...  0.0559 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7154...  Training loss: 1.7783...  0.0583 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7155...  Training loss: 1.7839...  0.0575 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7156...  Training loss: 1.7670...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7157...  Training loss: 1.7636...  0.0568 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7158...  Training loss: 1.7716...  0.0570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7159...  Training loss: 1.7563...  0.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7160...  Training loss: 1.7694...  0.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7161...  Training loss: 1.7999...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7162...  Training loss: 1.7852...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7163...  Training loss: 1.7604...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7164...  Training loss: 1.7541...  0.0536 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7165...  Training loss: 1.8111...  0.0578 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7166...  Training loss: 1.8097...  0.0582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7167...  Training loss: 1.8136...  0.0556 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7168...  Training loss: 1.7971...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7169...  Training loss: 1.7757...  0.0541 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7170...  Training loss: 1.7818...  0.0557 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7171...  Training loss: 1.8332...  0.0558 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7172...  Training loss: 1.7919...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7173...  Training loss: 1.8139...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7174...  Training loss: 1.7677...  0.0579 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7175...  Training loss: 1.7737...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7176...  Training loss: 1.8348...  0.0565 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7177...  Training loss: 1.8773...  0.0557 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7178...  Training loss: 1.8283...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7179...  Training loss: 1.8136...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7180...  Training loss: 1.7959...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7181...  Training loss: 1.8421...  0.0619 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7182...  Training loss: 1.7629...  0.0541 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7183...  Training loss: 1.7597...  0.0572 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7184...  Training loss: 1.7489...  0.0539 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7185...  Training loss: 1.7877...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7186...  Training loss: 1.7951...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7187...  Training loss: 1.7805...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7188...  Training loss: 1.8228...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7189...  Training loss: 1.7802...  0.0550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7190...  Training loss: 1.7858...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7191...  Training loss: 1.7991...  0.0561 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7192...  Training loss: 1.8708...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7193...  Training loss: 1.8083...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7194...  Training loss: 1.7893...  0.0569 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7195...  Training loss: 1.7931...  0.0615 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7196...  Training loss: 1.7883...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7197...  Training loss: 1.7517...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7198...  Training loss: 1.8616...  0.0570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7199...  Training loss: 1.8124...  0.0520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7200...  Training loss: 1.7909...  0.0555 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7201...  Training loss: 1.7267...  0.0562 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7202...  Training loss: 1.8379...  0.0552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7203...  Training loss: 1.7712...  0.0590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7204...  Training loss: 1.8028...  0.0536 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 7205...  Training loss: 1.8048...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7206...  Training loss: 1.7114...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7207...  Training loss: 1.7254...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7208...  Training loss: 1.8096...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7209...  Training loss: 1.7483...  0.0556 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7210...  Training loss: 1.7839...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7211...  Training loss: 1.8101...  0.0586 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7212...  Training loss: 1.7466...  0.0543 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7213...  Training loss: 1.7789...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7214...  Training loss: 1.8126...  0.0577 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7215...  Training loss: 1.7496...  0.0535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7216...  Training loss: 1.8125...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7217...  Training loss: 1.7794...  0.0526 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7218...  Training loss: 1.8024...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7219...  Training loss: 1.7640...  0.0522 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7220...  Training loss: 1.8148...  0.0550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7221...  Training loss: 1.8405...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7222...  Training loss: 1.8058...  0.0536 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7223...  Training loss: 1.7499...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7224...  Training loss: 1.8404...  0.0587 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7225...  Training loss: 1.8438...  0.0558 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7226...  Training loss: 1.8346...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7227...  Training loss: 1.8521...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7228...  Training loss: 1.7951...  0.0581 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7229...  Training loss: 1.8439...  0.0556 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7230...  Training loss: 1.8316...  0.0569 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7231...  Training loss: 1.7569...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7232...  Training loss: 1.8316...  0.0576 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7233...  Training loss: 1.8174...  0.0557 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7234...  Training loss: 1.7695...  0.0557 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7235...  Training loss: 1.7659...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7236...  Training loss: 1.7356...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7237...  Training loss: 1.7776...  0.0535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7238...  Training loss: 1.7975...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7239...  Training loss: 1.8276...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7240...  Training loss: 1.7795...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7241...  Training loss: 1.7635...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7242...  Training loss: 1.7843...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7243...  Training loss: 1.8130...  0.0523 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7244...  Training loss: 1.7741...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7245...  Training loss: 1.8379...  0.0583 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7246...  Training loss: 1.8113...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7247...  Training loss: 1.7725...  0.0570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7248...  Training loss: 1.8075...  0.0543 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7249...  Training loss: 1.7733...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7250...  Training loss: 1.7407...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7251...  Training loss: 1.8014...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7252...  Training loss: 1.8395...  0.0525 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7253...  Training loss: 1.8062...  0.0587 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7254...  Training loss: 1.7750...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7255...  Training loss: 1.7519...  0.0565 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7256...  Training loss: 1.7836...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7257...  Training loss: 1.7410...  0.0552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7258...  Training loss: 1.7689...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7259...  Training loss: 1.7951...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7260...  Training loss: 1.7769...  0.0537 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7261...  Training loss: 1.7879...  0.0568 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7262...  Training loss: 1.7745...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7263...  Training loss: 1.8197...  0.0569 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7264...  Training loss: 1.7996...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7265...  Training loss: 1.6880...  0.0555 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7266...  Training loss: 1.7632...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7267...  Training loss: 1.7523...  0.0557 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7268...  Training loss: 1.7348...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7269...  Training loss: 1.7899...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7270...  Training loss: 1.8427...  0.0522 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7271...  Training loss: 1.8522...  0.0576 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7272...  Training loss: 1.8042...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7273...  Training loss: 1.7506...  0.0539 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7274...  Training loss: 1.7942...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7275...  Training loss: 1.7669...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7276...  Training loss: 1.7831...  0.0550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7277...  Training loss: 1.7876...  0.0542 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7278...  Training loss: 1.7830...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7279...  Training loss: 1.7836...  0.0559 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7280...  Training loss: 1.7844...  0.0537 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7281...  Training loss: 1.7500...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7282...  Training loss: 1.7980...  0.0543 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7283...  Training loss: 1.7457...  0.0524 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7284...  Training loss: 1.7862...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7285...  Training loss: 1.8106...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7286...  Training loss: 1.8117...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7287...  Training loss: 1.7713...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7288...  Training loss: 1.7979...  0.0537 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7289...  Training loss: 1.7941...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7290...  Training loss: 1.7893...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7291...  Training loss: 1.7801...  0.0573 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7292...  Training loss: 1.7858...  0.0565 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7293...  Training loss: 1.7615...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7294...  Training loss: 1.7297...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7295...  Training loss: 1.8272...  0.0569 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7296...  Training loss: 1.8414...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7297...  Training loss: 1.8075...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7298...  Training loss: 1.8765...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7299...  Training loss: 1.7594...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7300...  Training loss: 1.8341...  0.0561 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7301...  Training loss: 1.7777...  0.0525 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7302...  Training loss: 1.7698...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7303...  Training loss: 1.8254...  0.0524 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7304...  Training loss: 1.8136...  0.0555 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 7305...  Training loss: 1.8741...  0.0535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7306...  Training loss: 1.7760...  0.0539 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7307...  Training loss: 1.8233...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7308...  Training loss: 1.7867...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7309...  Training loss: 1.8147...  0.0572 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7310...  Training loss: 1.7754...  0.0550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7311...  Training loss: 1.7778...  0.0537 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7312...  Training loss: 1.8036...  0.0526 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7313...  Training loss: 1.7877...  0.0583 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7314...  Training loss: 1.7556...  0.0550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7315...  Training loss: 1.7597...  0.0558 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7316...  Training loss: 1.7816...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7317...  Training loss: 1.7775...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7318...  Training loss: 1.8580...  0.0582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7319...  Training loss: 1.7315...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7320...  Training loss: 1.7829...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7321...  Training loss: 1.7892...  0.0555 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7322...  Training loss: 1.8211...  0.0563 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7323...  Training loss: 1.8697...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7324...  Training loss: 1.8008...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7325...  Training loss: 1.7494...  0.0606 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7326...  Training loss: 1.7309...  0.0557 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7327...  Training loss: 1.8249...  0.0579 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7328...  Training loss: 1.7857...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7329...  Training loss: 1.8194...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7330...  Training loss: 1.8033...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7331...  Training loss: 1.8436...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7332...  Training loss: 1.8251...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7333...  Training loss: 1.8513...  0.0522 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7334...  Training loss: 1.8481...  0.0579 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7335...  Training loss: 1.7968...  0.0570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7336...  Training loss: 1.7761...  0.0559 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7337...  Training loss: 1.7984...  0.0535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7338...  Training loss: 1.7818...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7339...  Training loss: 1.7718...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7340...  Training loss: 1.7588...  0.0578 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7341...  Training loss: 1.7725...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7342...  Training loss: 1.7481...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7343...  Training loss: 1.7669...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7344...  Training loss: 1.7038...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7345...  Training loss: 1.8138...  0.0612 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7346...  Training loss: 1.8438...  0.0552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7347...  Training loss: 1.8142...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7348...  Training loss: 1.7938...  0.0568 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7349...  Training loss: 1.7814...  0.0552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7350...  Training loss: 1.7674...  0.0557 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7351...  Training loss: 1.7756...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7352...  Training loss: 1.7646...  0.0552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7353...  Training loss: 1.7891...  0.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7354...  Training loss: 1.7862...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7355...  Training loss: 1.7972...  0.0543 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7356...  Training loss: 1.8065...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7357...  Training loss: 1.7414...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7358...  Training loss: 1.7895...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7359...  Training loss: 1.7191...  0.0577 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7360...  Training loss: 1.8111...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7361...  Training loss: 1.7792...  0.0540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7362...  Training loss: 1.7666...  0.0535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7363...  Training loss: 1.7661...  0.0596 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7364...  Training loss: 1.7453...  0.0564 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7365...  Training loss: 1.8273...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7366...  Training loss: 1.7750...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7367...  Training loss: 1.7962...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7368...  Training loss: 1.8300...  0.0544 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7369...  Training loss: 1.8390...  0.0556 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7370...  Training loss: 1.7764...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7371...  Training loss: 1.8083...  0.0548 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7372...  Training loss: 1.7453...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7373...  Training loss: 1.7893...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7374...  Training loss: 1.7962...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7375...  Training loss: 1.7719...  0.0580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7376...  Training loss: 1.7812...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7377...  Training loss: 1.7927...  0.0583 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7378...  Training loss: 1.7545...  0.0543 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7379...  Training loss: 1.7989...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7380...  Training loss: 1.7841...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7381...  Training loss: 1.8348...  0.0670 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7382...  Training loss: 1.8179...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7383...  Training loss: 1.7830...  0.0579 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7384...  Training loss: 1.8697...  0.0571 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7385...  Training loss: 1.9004...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7386...  Training loss: 1.8402...  0.0555 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7387...  Training loss: 1.7707...  0.0540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7388...  Training loss: 1.8781...  0.0550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7389...  Training loss: 1.7699...  0.0539 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7390...  Training loss: 1.8231...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7391...  Training loss: 1.8108...  0.0591 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7392...  Training loss: 1.8587...  0.0536 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7393...  Training loss: 1.7825...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7394...  Training loss: 1.7606...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7395...  Training loss: 1.8098...  0.0555 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7396...  Training loss: 1.8157...  0.0540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7397...  Training loss: 1.7413...  0.0543 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7398...  Training loss: 1.7844...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7399...  Training loss: 1.8291...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7400...  Training loss: 1.8206...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7401...  Training loss: 1.8330...  0.0537 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7402...  Training loss: 1.7851...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7403...  Training loss: 1.8069...  0.0522 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7404...  Training loss: 1.8236...  0.0527 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 7405...  Training loss: 1.8045...  0.0523 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7406...  Training loss: 1.8067...  0.0536 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7407...  Training loss: 1.7619...  0.0576 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7408...  Training loss: 1.7933...  0.0572 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7409...  Training loss: 1.7897...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7410...  Training loss: 1.8375...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7411...  Training loss: 1.7671...  0.0581 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7412...  Training loss: 1.8369...  0.0582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7413...  Training loss: 1.7872...  0.0566 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7414...  Training loss: 1.7646...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7415...  Training loss: 1.7617...  0.0536 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7416...  Training loss: 1.7663...  0.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7417...  Training loss: 1.7395...  0.0569 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7418...  Training loss: 1.7965...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7419...  Training loss: 1.8088...  0.0532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7420...  Training loss: 1.8252...  0.0576 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7421...  Training loss: 1.7674...  0.0577 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7422...  Training loss: 1.7983...  0.0576 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7423...  Training loss: 1.7934...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7424...  Training loss: 1.7676...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7425...  Training loss: 1.7980...  0.0547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7426...  Training loss: 1.7974...  0.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7427...  Training loss: 1.7438...  0.0580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7428...  Training loss: 1.7531...  0.0574 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7429...  Training loss: 1.7727...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7430...  Training loss: 1.8395...  0.0602 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7431...  Training loss: 1.8488...  0.0562 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7432...  Training loss: 1.7978...  0.0569 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7433...  Training loss: 1.7484...  0.0583 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7434...  Training loss: 1.7888...  0.0534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7435...  Training loss: 1.7481...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7436...  Training loss: 1.8262...  0.0549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7437...  Training loss: 1.8449...  0.0612 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7438...  Training loss: 1.7615...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7439...  Training loss: 1.7374...  0.0545 sec/batch\n",
      "Epoch: 12/20...  Training Step: 7440...  Training loss: 1.7488...  0.0544 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7441...  Training loss: 1.8563...  0.0562 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7442...  Training loss: 1.8781...  0.0573 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7443...  Training loss: 1.8336...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7444...  Training loss: 1.7626...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7445...  Training loss: 1.7962...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7446...  Training loss: 1.8196...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7447...  Training loss: 1.7597...  0.0559 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7448...  Training loss: 1.7634...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7449...  Training loss: 1.7428...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7450...  Training loss: 1.7608...  0.0536 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7451...  Training loss: 1.7660...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7452...  Training loss: 1.7599...  0.0580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7453...  Training loss: 1.7781...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7454...  Training loss: 1.7480...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7455...  Training loss: 1.8195...  0.0570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7456...  Training loss: 1.8348...  0.0554 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7457...  Training loss: 1.7755...  0.0554 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7458...  Training loss: 1.7937...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7459...  Training loss: 1.7696...  0.0562 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7460...  Training loss: 1.7890...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7461...  Training loss: 1.8406...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7462...  Training loss: 1.7757...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7463...  Training loss: 1.7680...  0.0584 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7464...  Training loss: 1.8072...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7465...  Training loss: 1.7819...  0.0568 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7466...  Training loss: 1.7468...  0.0602 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7467...  Training loss: 1.7866...  0.0584 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7468...  Training loss: 1.8226...  0.0565 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7469...  Training loss: 1.7872...  0.0568 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7470...  Training loss: 1.7428...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7471...  Training loss: 1.7395...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7472...  Training loss: 1.8044...  0.0586 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7473...  Training loss: 1.7868...  0.0575 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7474...  Training loss: 1.7741...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7475...  Training loss: 1.7784...  0.0572 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7476...  Training loss: 1.7903...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7477...  Training loss: 1.7800...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7478...  Training loss: 1.8067...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7479...  Training loss: 1.7907...  0.0554 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7480...  Training loss: 1.7632...  0.0523 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7481...  Training loss: 1.7906...  0.0558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7482...  Training loss: 1.8092...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7483...  Training loss: 1.8054...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7484...  Training loss: 1.7926...  0.0563 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7485...  Training loss: 1.7785...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7486...  Training loss: 1.7676...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7487...  Training loss: 1.6746...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7488...  Training loss: 1.7793...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7489...  Training loss: 1.7336...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7490...  Training loss: 1.8129...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7491...  Training loss: 1.7561...  0.0585 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7492...  Training loss: 1.7504...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7493...  Training loss: 1.7669...  0.0580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7494...  Training loss: 1.7985...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7495...  Training loss: 1.7966...  0.0543 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7496...  Training loss: 1.7718...  0.0526 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7497...  Training loss: 1.7339...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7498...  Training loss: 1.7784...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7499...  Training loss: 1.7665...  0.0621 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7500...  Training loss: 1.8369...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7501...  Training loss: 1.7726...  0.0581 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7502...  Training loss: 1.7609...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7503...  Training loss: 1.8160...  0.0564 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7504...  Training loss: 1.7713...  0.0531 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 7505...  Training loss: 1.7435...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7506...  Training loss: 1.7120...  0.0588 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7507...  Training loss: 1.7562...  0.0587 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7508...  Training loss: 1.7515...  0.0562 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7509...  Training loss: 1.7955...  0.0555 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7510...  Training loss: 1.7807...  0.0569 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7511...  Training loss: 1.8251...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7512...  Training loss: 1.7922...  0.0579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7513...  Training loss: 1.7089...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7514...  Training loss: 1.7493...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7515...  Training loss: 1.8325...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7516...  Training loss: 1.8036...  0.0524 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7517...  Training loss: 1.7718...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7518...  Training loss: 1.7586...  0.0574 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7519...  Training loss: 1.7827...  0.0616 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7520...  Training loss: 1.8036...  0.0572 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7521...  Training loss: 1.7239...  0.0581 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7522...  Training loss: 1.7909...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7523...  Training loss: 1.7530...  0.0578 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7524...  Training loss: 1.7554...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7525...  Training loss: 1.7514...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7526...  Training loss: 1.8218...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7527...  Training loss: 1.7502...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7528...  Training loss: 1.8271...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7529...  Training loss: 1.7657...  0.0540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7530...  Training loss: 1.7983...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7531...  Training loss: 1.7451...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7532...  Training loss: 1.8334...  0.0560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7533...  Training loss: 1.7923...  0.0555 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7534...  Training loss: 1.7679...  0.0558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7535...  Training loss: 1.7622...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7536...  Training loss: 1.8005...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7537...  Training loss: 1.8225...  0.0592 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7538...  Training loss: 1.7175...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7539...  Training loss: 1.8224...  0.0555 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7540...  Training loss: 1.7561...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7541...  Training loss: 1.7620...  0.0541 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7542...  Training loss: 1.7442...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7543...  Training loss: 1.8022...  0.0554 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7544...  Training loss: 1.8334...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7545...  Training loss: 1.7893...  0.0586 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7546...  Training loss: 1.7619...  0.0573 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7547...  Training loss: 1.8098...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7548...  Training loss: 1.7720...  0.0524 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7549...  Training loss: 1.7788...  0.0558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7550...  Training loss: 1.7467...  0.0584 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7551...  Training loss: 1.7447...  0.0534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7552...  Training loss: 1.7646...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7553...  Training loss: 1.7546...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7554...  Training loss: 1.7709...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7555...  Training loss: 1.8078...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7556...  Training loss: 1.8173...  0.0587 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7557...  Training loss: 1.7369...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7558...  Training loss: 1.8102...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7559...  Training loss: 1.7772...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7560...  Training loss: 1.7734...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7561...  Training loss: 1.7547...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7562...  Training loss: 1.7301...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7563...  Training loss: 1.7576...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7564...  Training loss: 1.8055...  0.0526 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7565...  Training loss: 1.8007...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7566...  Training loss: 1.8235...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7567...  Training loss: 1.8366...  0.0558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7568...  Training loss: 1.7422...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7569...  Training loss: 1.7692...  0.0579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7570...  Training loss: 1.8229...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7571...  Training loss: 1.7808...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7572...  Training loss: 1.8181...  0.0583 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7573...  Training loss: 1.8333...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7574...  Training loss: 1.7793...  0.0537 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7575...  Training loss: 1.7613...  0.0538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7576...  Training loss: 1.7611...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7577...  Training loss: 1.7877...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7578...  Training loss: 1.7717...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7579...  Training loss: 1.8236...  0.0522 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7580...  Training loss: 1.7982...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7581...  Training loss: 1.8318...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7582...  Training loss: 1.7077...  0.0540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7583...  Training loss: 1.7843...  0.0534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7584...  Training loss: 1.7601...  0.0544 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7585...  Training loss: 1.7331...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7586...  Training loss: 1.8035...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7587...  Training loss: 1.7860...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7588...  Training loss: 1.8113...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7589...  Training loss: 1.7818...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7590...  Training loss: 1.8280...  0.0523 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7591...  Training loss: 1.8080...  0.0593 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7592...  Training loss: 1.7710...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7593...  Training loss: 1.7705...  0.0538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7594...  Training loss: 1.8301...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7595...  Training loss: 1.7620...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7596...  Training loss: 1.8030...  0.0574 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7597...  Training loss: 1.7865...  0.0567 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7598...  Training loss: 1.8093...  0.0538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7599...  Training loss: 1.8010...  0.0578 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7600...  Training loss: 1.7461...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7601...  Training loss: 1.7441...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7602...  Training loss: 1.7352...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7603...  Training loss: 1.7833...  0.0567 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7604...  Training loss: 1.7777...  0.0614 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 7605...  Training loss: 1.7919...  0.0538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7606...  Training loss: 1.8021...  0.0555 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7607...  Training loss: 1.7856...  0.0583 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7608...  Training loss: 1.7846...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7609...  Training loss: 1.7778...  0.0536 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7610...  Training loss: 1.7440...  0.0573 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7611...  Training loss: 1.7656...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7612...  Training loss: 1.7877...  0.0526 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7613...  Training loss: 1.7784...  0.0565 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7614...  Training loss: 1.7528...  0.0536 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7615...  Training loss: 1.7420...  0.0574 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7616...  Training loss: 1.7816...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7617...  Training loss: 1.7937...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7618...  Training loss: 1.7309...  0.0537 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7619...  Training loss: 1.7279...  0.0577 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7620...  Training loss: 1.7566...  0.0566 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7621...  Training loss: 1.7440...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7622...  Training loss: 1.8087...  0.0566 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7623...  Training loss: 1.7979...  0.0564 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7624...  Training loss: 1.7633...  0.0571 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7625...  Training loss: 1.7293...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7626...  Training loss: 1.7754...  0.0544 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7627...  Training loss: 1.7802...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7628...  Training loss: 1.7430...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7629...  Training loss: 1.7716...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7630...  Training loss: 1.8510...  0.0536 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7631...  Training loss: 1.8015...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7632...  Training loss: 1.8414...  0.0554 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7633...  Training loss: 1.8127...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7634...  Training loss: 1.7556...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7635...  Training loss: 1.7579...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7636...  Training loss: 1.8593...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7637...  Training loss: 1.7714...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7638...  Training loss: 1.8433...  0.0534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7639...  Training loss: 1.7689...  0.0587 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7640...  Training loss: 1.7927...  0.0591 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7641...  Training loss: 1.7743...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7642...  Training loss: 1.7795...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7643...  Training loss: 1.7653...  0.0583 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7644...  Training loss: 1.7662...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7645...  Training loss: 1.7830...  0.0570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7646...  Training loss: 1.7444...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7647...  Training loss: 1.8018...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7648...  Training loss: 1.7906...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7649...  Training loss: 1.7685...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7650...  Training loss: 1.7456...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7651...  Training loss: 1.7879...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7652...  Training loss: 1.7720...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7653...  Training loss: 1.8168...  0.0595 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7654...  Training loss: 1.8021...  0.0534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7655...  Training loss: 1.7929...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7656...  Training loss: 1.8070...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7657...  Training loss: 1.8040...  0.0523 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7658...  Training loss: 1.7496...  0.0602 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7659...  Training loss: 1.8524...  0.0544 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7660...  Training loss: 1.8335...  0.0526 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7661...  Training loss: 1.7958...  0.0563 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7662...  Training loss: 1.8231...  0.0525 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7663...  Training loss: 1.8256...  0.0561 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7664...  Training loss: 1.7468...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7665...  Training loss: 1.7597...  0.0561 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7666...  Training loss: 1.8202...  0.0555 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7667...  Training loss: 1.8136...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7668...  Training loss: 1.7452...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7669...  Training loss: 1.8004...  0.0525 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7670...  Training loss: 1.7550...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7671...  Training loss: 1.8374...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7672...  Training loss: 1.7588...  0.0578 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7673...  Training loss: 1.7461...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7674...  Training loss: 1.7757...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7675...  Training loss: 1.7272...  0.0615 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7676...  Training loss: 1.8023...  0.0588 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7677...  Training loss: 1.7672...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7678...  Training loss: 1.7448...  0.0525 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7679...  Training loss: 1.7633...  0.0537 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7680...  Training loss: 1.8038...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7681...  Training loss: 1.7415...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7682...  Training loss: 1.7453...  0.0581 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7683...  Training loss: 1.7627...  0.0585 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7684...  Training loss: 1.7524...  0.0582 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7685...  Training loss: 1.7868...  0.0580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7686...  Training loss: 1.7649...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7687...  Training loss: 1.7914...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7688...  Training loss: 1.7991...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7689...  Training loss: 1.7491...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7690...  Training loss: 1.7650...  0.0559 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7691...  Training loss: 1.7881...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7692...  Training loss: 1.7445...  0.0581 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7693...  Training loss: 1.7957...  0.0583 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7694...  Training loss: 1.8046...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7695...  Training loss: 1.8496...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7696...  Training loss: 1.7899...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7697...  Training loss: 1.7781...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7698...  Training loss: 1.7731...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7699...  Training loss: 1.7856...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7700...  Training loss: 1.7622...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7701...  Training loss: 1.7921...  0.0541 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7702...  Training loss: 1.7695...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7703...  Training loss: 1.7922...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7704...  Training loss: 1.7833...  0.0545 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 7705...  Training loss: 1.7845...  0.0558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7706...  Training loss: 1.7209...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7707...  Training loss: 1.7649...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7708...  Training loss: 1.7982...  0.0570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7709...  Training loss: 1.7684...  0.0538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7710...  Training loss: 1.7319...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7711...  Training loss: 1.7392...  0.0562 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7712...  Training loss: 1.7773...  0.0572 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7713...  Training loss: 1.7643...  0.0539 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7714...  Training loss: 1.7557...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7715...  Training loss: 1.7992...  0.0537 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7716...  Training loss: 1.8127...  0.0573 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7717...  Training loss: 1.8719...  0.0563 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7718...  Training loss: 1.8088...  0.0555 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7719...  Training loss: 1.8276...  0.0580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7720...  Training loss: 1.8367...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7721...  Training loss: 1.7712...  0.0534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7722...  Training loss: 1.7246...  0.0539 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7723...  Training loss: 1.7495...  0.0524 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7724...  Training loss: 1.7884...  0.0578 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7725...  Training loss: 1.7738...  0.0544 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7726...  Training loss: 1.8054...  0.0565 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7727...  Training loss: 1.7576...  0.0526 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7728...  Training loss: 1.7787...  0.0572 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7729...  Training loss: 1.7890...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7730...  Training loss: 1.8327...  0.0597 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7731...  Training loss: 1.7615...  0.0536 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7732...  Training loss: 1.7870...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7733...  Training loss: 1.7682...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7734...  Training loss: 1.7939...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7735...  Training loss: 1.7716...  0.0539 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7736...  Training loss: 1.7323...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7737...  Training loss: 1.8070...  0.0541 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7738...  Training loss: 1.8075...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7739...  Training loss: 1.8004...  0.0544 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7740...  Training loss: 1.7787...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7741...  Training loss: 1.8215...  0.0562 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7742...  Training loss: 1.8696...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7743...  Training loss: 1.7511...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7744...  Training loss: 1.8186...  0.0588 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7745...  Training loss: 1.7545...  0.0523 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7746...  Training loss: 1.7700...  0.0561 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7747...  Training loss: 1.7915...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7748...  Training loss: 1.7652...  0.0578 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7749...  Training loss: 1.7688...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7750...  Training loss: 1.7717...  0.0539 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7751...  Training loss: 1.7405...  0.0561 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7752...  Training loss: 1.7472...  0.0526 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7753...  Training loss: 1.7384...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7754...  Training loss: 1.7293...  0.0578 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7755...  Training loss: 1.7850...  0.0566 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7756...  Training loss: 1.8058...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7757...  Training loss: 1.7537...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7758...  Training loss: 1.7263...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7759...  Training loss: 1.7430...  0.0536 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7760...  Training loss: 1.8171...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7761...  Training loss: 1.7501...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7762...  Training loss: 1.7291...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7763...  Training loss: 1.7810...  0.0525 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7764...  Training loss: 1.7648...  0.0566 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7765...  Training loss: 1.7528...  0.0524 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7766...  Training loss: 1.7651...  0.0583 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7767...  Training loss: 1.7232...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7768...  Training loss: 1.7335...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7769...  Training loss: 1.7896...  0.0534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7770...  Training loss: 1.7700...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7771...  Training loss: 1.7868...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7772...  Training loss: 1.7725...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7773...  Training loss: 1.7472...  0.0569 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7774...  Training loss: 1.7791...  0.0522 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7775...  Training loss: 1.7813...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7776...  Training loss: 1.7736...  0.0525 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7777...  Training loss: 1.7470...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7778...  Training loss: 1.7483...  0.0587 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7779...  Training loss: 1.7484...  0.0579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7780...  Training loss: 1.7622...  0.0536 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7781...  Training loss: 1.7836...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7782...  Training loss: 1.7564...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7783...  Training loss: 1.7334...  0.0544 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7784...  Training loss: 1.7547...  0.0554 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7785...  Training loss: 1.7629...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7786...  Training loss: 1.7702...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7787...  Training loss: 1.7929...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7788...  Training loss: 1.7863...  0.0554 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7789...  Training loss: 1.7584...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7790...  Training loss: 1.7441...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7791...  Training loss: 1.8305...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7792...  Training loss: 1.7862...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7793...  Training loss: 1.7834...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7794...  Training loss: 1.7582...  0.0526 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7795...  Training loss: 1.7436...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7796...  Training loss: 1.8439...  0.0585 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7797...  Training loss: 1.8750...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7798...  Training loss: 1.7930...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7799...  Training loss: 1.7925...  0.0526 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7800...  Training loss: 1.7881...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7801...  Training loss: 1.8018...  0.0559 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7802...  Training loss: 1.7609...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7803...  Training loss: 1.7314...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7804...  Training loss: 1.7302...  0.0523 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 7805...  Training loss: 1.7683...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7806...  Training loss: 1.7870...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7807...  Training loss: 1.7602...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7808...  Training loss: 1.8094...  0.0569 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7809...  Training loss: 1.7921...  0.0526 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7810...  Training loss: 1.7491...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7811...  Training loss: 1.7738...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7812...  Training loss: 1.8431...  0.0584 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7813...  Training loss: 1.7886...  0.0560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7814...  Training loss: 1.7788...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7815...  Training loss: 1.7717...  0.0588 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7816...  Training loss: 1.7610...  0.0569 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7817...  Training loss: 1.7485...  0.0593 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7818...  Training loss: 1.8573...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7819...  Training loss: 1.7928...  0.0558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7820...  Training loss: 1.7787...  0.0559 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7821...  Training loss: 1.6990...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7822...  Training loss: 1.8339...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7823...  Training loss: 1.7325...  0.0579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7824...  Training loss: 1.7725...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7825...  Training loss: 1.7893...  0.0583 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7826...  Training loss: 1.7035...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7827...  Training loss: 1.7094...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7828...  Training loss: 1.7814...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7829...  Training loss: 1.7360...  0.0558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7830...  Training loss: 1.7303...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7831...  Training loss: 1.7891...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7832...  Training loss: 1.7198...  0.0522 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7833...  Training loss: 1.7645...  0.0573 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7834...  Training loss: 1.7997...  0.0562 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7835...  Training loss: 1.7311...  0.0560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7836...  Training loss: 1.7858...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7837...  Training loss: 1.7461...  0.0581 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7838...  Training loss: 1.7853...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7839...  Training loss: 1.7202...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7840...  Training loss: 1.7926...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7841...  Training loss: 1.8157...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7842...  Training loss: 1.7861...  0.0534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7843...  Training loss: 1.7420...  0.0555 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7844...  Training loss: 1.8120...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7845...  Training loss: 1.8359...  0.0537 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7846...  Training loss: 1.7870...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7847...  Training loss: 1.8368...  0.0560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7848...  Training loss: 1.7754...  0.0589 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7849...  Training loss: 1.8319...  0.0574 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7850...  Training loss: 1.8105...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7851...  Training loss: 1.7533...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7852...  Training loss: 1.7965...  0.0559 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7853...  Training loss: 1.7918...  0.0544 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7854...  Training loss: 1.7826...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7855...  Training loss: 1.7358...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7856...  Training loss: 1.7160...  0.0534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7857...  Training loss: 1.7540...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7858...  Training loss: 1.7792...  0.0543 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7859...  Training loss: 1.8174...  0.0538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7860...  Training loss: 1.7686...  0.0577 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7861...  Training loss: 1.7435...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7862...  Training loss: 1.7818...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7863...  Training loss: 1.7897...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7864...  Training loss: 1.7384...  0.0537 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7865...  Training loss: 1.8057...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7866...  Training loss: 1.8020...  0.0543 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7867...  Training loss: 1.7361...  0.0554 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7868...  Training loss: 1.7961...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7869...  Training loss: 1.7651...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7870...  Training loss: 1.7284...  0.0579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7871...  Training loss: 1.7636...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7872...  Training loss: 1.8067...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7873...  Training loss: 1.7855...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7874...  Training loss: 1.7477...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7875...  Training loss: 1.7458...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7876...  Training loss: 1.7619...  0.0573 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7877...  Training loss: 1.7346...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7878...  Training loss: 1.7572...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7879...  Training loss: 1.7837...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7880...  Training loss: 1.7764...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7881...  Training loss: 1.7629...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7882...  Training loss: 1.7601...  0.0558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7883...  Training loss: 1.8256...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7884...  Training loss: 1.7639...  0.0542 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7885...  Training loss: 1.6925...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7886...  Training loss: 1.7702...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7887...  Training loss: 1.7319...  0.0563 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7888...  Training loss: 1.7184...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7889...  Training loss: 1.7720...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7890...  Training loss: 1.8307...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7891...  Training loss: 1.8263...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7892...  Training loss: 1.8152...  0.0543 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7893...  Training loss: 1.7338...  0.0538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7894...  Training loss: 1.7901...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7895...  Training loss: 1.7474...  0.0544 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7896...  Training loss: 1.7795...  0.0543 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7897...  Training loss: 1.7720...  0.0544 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7898...  Training loss: 1.7743...  0.0571 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7899...  Training loss: 1.7731...  0.0569 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7900...  Training loss: 1.7652...  0.0572 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7901...  Training loss: 1.7246...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7902...  Training loss: 1.7536...  0.0589 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7903...  Training loss: 1.7561...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7904...  Training loss: 1.7705...  0.0528 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 7905...  Training loss: 1.8192...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7906...  Training loss: 1.8017...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7907...  Training loss: 1.7563...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7908...  Training loss: 1.7619...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7909...  Training loss: 1.8002...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7910...  Training loss: 1.7530...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7911...  Training loss: 1.7838...  0.0558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7912...  Training loss: 1.7887...  0.0557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7913...  Training loss: 1.7478...  0.0574 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7914...  Training loss: 1.7347...  0.0536 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7915...  Training loss: 1.8320...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7916...  Training loss: 1.8162...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7917...  Training loss: 1.7966...  0.0555 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7918...  Training loss: 1.8408...  0.0555 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7919...  Training loss: 1.7480...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7920...  Training loss: 1.8496...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7921...  Training loss: 1.7609...  0.0544 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7922...  Training loss: 1.7557...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7923...  Training loss: 1.7968...  0.0538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7924...  Training loss: 1.8121...  0.0543 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7925...  Training loss: 1.8398...  0.0585 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7926...  Training loss: 1.7698...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7927...  Training loss: 1.7775...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7928...  Training loss: 1.7731...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7929...  Training loss: 1.8000...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7930...  Training loss: 1.7976...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7931...  Training loss: 1.7732...  0.0541 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7932...  Training loss: 1.7966...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7933...  Training loss: 1.7648...  0.0525 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7934...  Training loss: 1.7367...  0.0566 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7935...  Training loss: 1.7527...  0.0525 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7936...  Training loss: 1.7623...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7937...  Training loss: 1.7523...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7938...  Training loss: 1.8316...  0.0572 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7939...  Training loss: 1.7245...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7940...  Training loss: 1.7799...  0.0545 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7941...  Training loss: 1.7755...  0.0582 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7942...  Training loss: 1.7842...  0.0534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7943...  Training loss: 1.8524...  0.0541 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7944...  Training loss: 1.7933...  0.0538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7945...  Training loss: 1.7407...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7946...  Training loss: 1.7267...  0.0564 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7947...  Training loss: 1.8040...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7948...  Training loss: 1.7598...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7949...  Training loss: 1.8142...  0.0558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7950...  Training loss: 1.7827...  0.0526 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7951...  Training loss: 1.8233...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7952...  Training loss: 1.8035...  0.0584 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7953...  Training loss: 1.8261...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7954...  Training loss: 1.8169...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7955...  Training loss: 1.7973...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7956...  Training loss: 1.7739...  0.0582 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7957...  Training loss: 1.7648...  0.0560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7958...  Training loss: 1.7504...  0.0563 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7959...  Training loss: 1.7518...  0.0577 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7960...  Training loss: 1.7601...  0.0534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7961...  Training loss: 1.7699...  0.0634 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7962...  Training loss: 1.7317...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7963...  Training loss: 1.7740...  0.0538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7964...  Training loss: 1.7036...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7965...  Training loss: 1.8127...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7966...  Training loss: 1.8218...  0.0554 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7967...  Training loss: 1.7924...  0.0579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7968...  Training loss: 1.8171...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7969...  Training loss: 1.7760...  0.0554 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7970...  Training loss: 1.7365...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7971...  Training loss: 1.7666...  0.0579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7972...  Training loss: 1.7504...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7973...  Training loss: 1.7455...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7974...  Training loss: 1.7857...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7975...  Training loss: 1.7632...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7976...  Training loss: 1.7738...  0.0586 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7977...  Training loss: 1.7466...  0.0638 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7978...  Training loss: 1.7734...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7979...  Training loss: 1.7093...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7980...  Training loss: 1.7806...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7981...  Training loss: 1.7809...  0.0560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7982...  Training loss: 1.7574...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7983...  Training loss: 1.7541...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7984...  Training loss: 1.7553...  0.0583 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7985...  Training loss: 1.7988...  0.0524 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7986...  Training loss: 1.7901...  0.0571 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7987...  Training loss: 1.8004...  0.0566 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7988...  Training loss: 1.8289...  0.0570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7989...  Training loss: 1.8084...  0.0570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7990...  Training loss: 1.7551...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7991...  Training loss: 1.8004...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7992...  Training loss: 1.7229...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7993...  Training loss: 1.8013...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7994...  Training loss: 1.7724...  0.0558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7995...  Training loss: 1.7626...  0.0588 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7996...  Training loss: 1.7717...  0.0559 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7997...  Training loss: 1.7667...  0.0537 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7998...  Training loss: 1.7356...  0.0526 sec/batch\n",
      "Epoch: 13/20...  Training Step: 7999...  Training loss: 1.7627...  0.0558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8000...  Training loss: 1.7617...  0.0526 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8001...  Training loss: 1.7870...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8002...  Training loss: 1.7945...  0.0540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8003...  Training loss: 1.7707...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8004...  Training loss: 1.8490...  0.0572 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 8005...  Training loss: 1.8705...  0.0529 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8006...  Training loss: 1.8437...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8007...  Training loss: 1.7743...  0.0556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8008...  Training loss: 1.8396...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8009...  Training loss: 1.7670...  0.0575 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8010...  Training loss: 1.7866...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8011...  Training loss: 1.7980...  0.0547 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8012...  Training loss: 1.8189...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8013...  Training loss: 1.7641...  0.0562 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8014...  Training loss: 1.7577...  0.0555 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8015...  Training loss: 1.7785...  0.0570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8016...  Training loss: 1.8109...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8017...  Training loss: 1.7114...  0.0581 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8018...  Training loss: 1.7804...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8019...  Training loss: 1.8238...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8020...  Training loss: 1.7826...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8021...  Training loss: 1.8169...  0.0587 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8022...  Training loss: 1.7697...  0.0564 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8023...  Training loss: 1.7808...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8024...  Training loss: 1.8188...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8025...  Training loss: 1.8053...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8026...  Training loss: 1.7973...  0.0605 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8027...  Training loss: 1.7284...  0.0559 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8028...  Training loss: 1.8059...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8029...  Training loss: 1.7838...  0.0564 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8030...  Training loss: 1.8115...  0.0575 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8031...  Training loss: 1.7458...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8032...  Training loss: 1.8063...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8033...  Training loss: 1.7674...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8034...  Training loss: 1.7451...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8035...  Training loss: 1.7420...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8036...  Training loss: 1.7421...  0.0579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8037...  Training loss: 1.7165...  0.0549 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8038...  Training loss: 1.7787...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8039...  Training loss: 1.7853...  0.0538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8040...  Training loss: 1.8110...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8041...  Training loss: 1.7588...  0.0562 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8042...  Training loss: 1.7611...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8043...  Training loss: 1.7836...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8044...  Training loss: 1.7462...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8045...  Training loss: 1.7738...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8046...  Training loss: 1.7799...  0.0548 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8047...  Training loss: 1.7306...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8048...  Training loss: 1.7548...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8049...  Training loss: 1.7705...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8050...  Training loss: 1.8192...  0.0578 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8051...  Training loss: 1.8176...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8052...  Training loss: 1.7820...  0.0579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8053...  Training loss: 1.7343...  0.0589 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8054...  Training loss: 1.7741...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8055...  Training loss: 1.7291...  0.0521 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8056...  Training loss: 1.7984...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8057...  Training loss: 1.8326...  0.0551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8058...  Training loss: 1.7467...  0.0567 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8059...  Training loss: 1.7368...  0.0534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 8060...  Training loss: 1.7552...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8061...  Training loss: 1.8463...  0.0556 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8062...  Training loss: 1.8594...  0.0584 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8063...  Training loss: 1.8383...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8064...  Training loss: 1.7544...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8065...  Training loss: 1.7682...  0.0561 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8066...  Training loss: 1.8051...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8067...  Training loss: 1.7381...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8068...  Training loss: 1.7277...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8069...  Training loss: 1.7180...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8070...  Training loss: 1.7452...  0.0554 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8071...  Training loss: 1.7555...  0.0595 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8072...  Training loss: 1.7369...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8073...  Training loss: 1.7755...  0.0554 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8074...  Training loss: 1.7346...  0.0536 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8075...  Training loss: 1.7980...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8076...  Training loss: 1.8097...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8077...  Training loss: 1.8035...  0.0569 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8078...  Training loss: 1.7598...  0.0554 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8079...  Training loss: 1.7196...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8080...  Training loss: 1.7834...  0.0564 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8081...  Training loss: 1.8127...  0.0533 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8082...  Training loss: 1.7626...  0.0579 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8083...  Training loss: 1.7517...  0.0533 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8084...  Training loss: 1.7705...  0.0532 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8085...  Training loss: 1.7465...  0.0543 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8086...  Training loss: 1.7338...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8087...  Training loss: 1.7719...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8088...  Training loss: 1.7686...  0.0568 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8089...  Training loss: 1.7738...  0.0585 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8090...  Training loss: 1.7460...  0.0547 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8091...  Training loss: 1.7319...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8092...  Training loss: 1.7858...  0.0542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8093...  Training loss: 1.7800...  0.0575 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8094...  Training loss: 1.7541...  0.0542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8095...  Training loss: 1.7742...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8096...  Training loss: 1.7445...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8097...  Training loss: 1.7671...  0.0536 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8098...  Training loss: 1.7787...  0.0556 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8099...  Training loss: 1.7983...  0.0580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8100...  Training loss: 1.7440...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8101...  Training loss: 1.7865...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8102...  Training loss: 1.8095...  0.0555 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8103...  Training loss: 1.7724...  0.0580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8104...  Training loss: 1.7992...  0.0553 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 8105...  Training loss: 1.7538...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8106...  Training loss: 1.7502...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8107...  Training loss: 1.6469...  0.0578 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8108...  Training loss: 1.7613...  0.0573 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8109...  Training loss: 1.7268...  0.0566 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8110...  Training loss: 1.7880...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8111...  Training loss: 1.7607...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8112...  Training loss: 1.7364...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8113...  Training loss: 1.7599...  0.0595 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8114...  Training loss: 1.7937...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8115...  Training loss: 1.7789...  0.0565 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8116...  Training loss: 1.7593...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8117...  Training loss: 1.7203...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8118...  Training loss: 1.7657...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8119...  Training loss: 1.7615...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8120...  Training loss: 1.7957...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8121...  Training loss: 1.7572...  0.0535 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8122...  Training loss: 1.7395...  0.0586 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8123...  Training loss: 1.7902...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8124...  Training loss: 1.7605...  0.0537 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8125...  Training loss: 1.7251...  0.0541 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8126...  Training loss: 1.7119...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8127...  Training loss: 1.7497...  0.0573 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8128...  Training loss: 1.7294...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8129...  Training loss: 1.7697...  0.0572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8130...  Training loss: 1.7706...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8131...  Training loss: 1.8041...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8132...  Training loss: 1.7767...  0.0576 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8133...  Training loss: 1.6920...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8134...  Training loss: 1.7586...  0.0562 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8135...  Training loss: 1.8009...  0.0536 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8136...  Training loss: 1.8053...  0.0553 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8137...  Training loss: 1.7745...  0.0672 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8138...  Training loss: 1.7397...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8139...  Training loss: 1.7756...  0.0558 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8140...  Training loss: 1.8001...  0.0535 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8141...  Training loss: 1.7036...  0.0565 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8142...  Training loss: 1.7536...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8143...  Training loss: 1.7452...  0.0533 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8144...  Training loss: 1.7644...  0.0543 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8145...  Training loss: 1.7562...  0.0583 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8146...  Training loss: 1.8013...  0.0523 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8147...  Training loss: 1.7160...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8148...  Training loss: 1.8112...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8149...  Training loss: 1.7559...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8150...  Training loss: 1.7672...  0.0582 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8151...  Training loss: 1.7404...  0.0582 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8152...  Training loss: 1.8282...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8153...  Training loss: 1.7768...  0.0578 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8154...  Training loss: 1.7661...  0.0583 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8155...  Training loss: 1.7482...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8156...  Training loss: 1.8008...  0.0587 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8157...  Training loss: 1.8042...  0.0522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8158...  Training loss: 1.6980...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8159...  Training loss: 1.7927...  0.0583 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8160...  Training loss: 1.7281...  0.0522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8161...  Training loss: 1.7441...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8162...  Training loss: 1.7625...  0.0520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8163...  Training loss: 1.8049...  0.0543 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8164...  Training loss: 1.8218...  0.0522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8165...  Training loss: 1.7502...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8166...  Training loss: 1.7299...  0.0542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8167...  Training loss: 1.7870...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8168...  Training loss: 1.7669...  0.0542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8169...  Training loss: 1.7626...  0.0604 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8170...  Training loss: 1.7144...  0.0560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8171...  Training loss: 1.7203...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8172...  Training loss: 1.7355...  0.0541 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8173...  Training loss: 1.7555...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8174...  Training loss: 1.7575...  0.0576 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8175...  Training loss: 1.7803...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8176...  Training loss: 1.7825...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8177...  Training loss: 1.7294...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8178...  Training loss: 1.7987...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8179...  Training loss: 1.7498...  0.0521 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8180...  Training loss: 1.7521...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8181...  Training loss: 1.7360...  0.0586 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8182...  Training loss: 1.7120...  0.0539 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8183...  Training loss: 1.7506...  0.0577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8184...  Training loss: 1.7836...  0.0554 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8185...  Training loss: 1.7932...  0.0572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8186...  Training loss: 1.8099...  0.0520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8187...  Training loss: 1.8135...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8188...  Training loss: 1.7319...  0.0596 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8189...  Training loss: 1.7641...  0.0522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8190...  Training loss: 1.8218...  0.0577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8191...  Training loss: 1.7621...  0.0566 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8192...  Training loss: 1.8143...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8193...  Training loss: 1.7959...  0.0593 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8194...  Training loss: 1.7616...  0.0558 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8195...  Training loss: 1.7237...  0.0568 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8196...  Training loss: 1.7500...  0.0547 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8197...  Training loss: 1.7572...  0.0555 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8198...  Training loss: 1.7698...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8199...  Training loss: 1.8024...  0.0532 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8200...  Training loss: 1.7764...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8201...  Training loss: 1.8189...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8202...  Training loss: 1.6864...  0.0569 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8203...  Training loss: 1.7710...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8204...  Training loss: 1.7513...  0.0551 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 8205...  Training loss: 1.7113...  0.0618 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8206...  Training loss: 1.7953...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8207...  Training loss: 1.7846...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8208...  Training loss: 1.7939...  0.0535 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8209...  Training loss: 1.7735...  0.0590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8210...  Training loss: 1.7890...  0.0521 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8211...  Training loss: 1.7892...  0.0570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8212...  Training loss: 1.7465...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8213...  Training loss: 1.7504...  0.0582 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8214...  Training loss: 1.8038...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8215...  Training loss: 1.7510...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8216...  Training loss: 1.7655...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8217...  Training loss: 1.7723...  0.0539 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8218...  Training loss: 1.7544...  0.0603 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8219...  Training loss: 1.7867...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8220...  Training loss: 1.7280...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8221...  Training loss: 1.7219...  0.0554 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8222...  Training loss: 1.7436...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8223...  Training loss: 1.7879...  0.0569 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8224...  Training loss: 1.7822...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8225...  Training loss: 1.7922...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8226...  Training loss: 1.7814...  0.0582 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8227...  Training loss: 1.7706...  0.0539 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8228...  Training loss: 1.7608...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8229...  Training loss: 1.7512...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8230...  Training loss: 1.7398...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8231...  Training loss: 1.7615...  0.0543 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8232...  Training loss: 1.7757...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8233...  Training loss: 1.7521...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8234...  Training loss: 1.7516...  0.0543 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8235...  Training loss: 1.7374...  0.0558 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8236...  Training loss: 1.7717...  0.0616 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8237...  Training loss: 1.7667...  0.0522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8238...  Training loss: 1.7360...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8239...  Training loss: 1.7324...  0.0553 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8240...  Training loss: 1.7369...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8241...  Training loss: 1.7571...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8242...  Training loss: 1.8048...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8243...  Training loss: 1.7819...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8244...  Training loss: 1.7300...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8245...  Training loss: 1.7038...  0.0564 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8246...  Training loss: 1.7384...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8247...  Training loss: 1.7587...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8248...  Training loss: 1.7440...  0.0522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8249...  Training loss: 1.7504...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8250...  Training loss: 1.8159...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8251...  Training loss: 1.7861...  0.0563 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8252...  Training loss: 1.8000...  0.0577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8253...  Training loss: 1.7962...  0.0583 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8254...  Training loss: 1.7561...  0.0567 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8255...  Training loss: 1.7335...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8256...  Training loss: 1.8401...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8257...  Training loss: 1.7564...  0.0601 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8258...  Training loss: 1.8208...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8259...  Training loss: 1.7612...  0.0557 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8260...  Training loss: 1.7923...  0.0543 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8261...  Training loss: 1.7399...  0.0555 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8262...  Training loss: 1.7542...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8263...  Training loss: 1.7552...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8264...  Training loss: 1.7527...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8265...  Training loss: 1.7756...  0.0569 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8266...  Training loss: 1.7167...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8267...  Training loss: 1.7842...  0.0554 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8268...  Training loss: 1.7682...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8269...  Training loss: 1.7591...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8270...  Training loss: 1.7464...  0.0567 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8271...  Training loss: 1.7829...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8272...  Training loss: 1.7852...  0.0584 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8273...  Training loss: 1.8073...  0.0558 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8274...  Training loss: 1.8007...  0.0587 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8275...  Training loss: 1.7813...  0.0556 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8276...  Training loss: 1.8024...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8277...  Training loss: 1.7784...  0.0534 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8278...  Training loss: 1.7402...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8279...  Training loss: 1.8529...  0.0535 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8280...  Training loss: 1.8158...  0.0540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8281...  Training loss: 1.8032...  0.0542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8282...  Training loss: 1.7944...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8283...  Training loss: 1.8034...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8284...  Training loss: 1.7226...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8285...  Training loss: 1.7545...  0.0562 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8286...  Training loss: 1.8046...  0.0532 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8287...  Training loss: 1.7968...  0.0555 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8288...  Training loss: 1.7410...  0.0553 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8289...  Training loss: 1.7810...  0.0536 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8290...  Training loss: 1.7416...  0.0578 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8291...  Training loss: 1.8146...  0.0590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8292...  Training loss: 1.7209...  0.0574 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8293...  Training loss: 1.7399...  0.0557 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8294...  Training loss: 1.7472...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8295...  Training loss: 1.7243...  0.0578 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8296...  Training loss: 1.8054...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8297...  Training loss: 1.7710...  0.0533 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8298...  Training loss: 1.7220...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8299...  Training loss: 1.7489...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8300...  Training loss: 1.7915...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8301...  Training loss: 1.7439...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8302...  Training loss: 1.7449...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8303...  Training loss: 1.7337...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8304...  Training loss: 1.7397...  0.0525 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 8305...  Training loss: 1.7761...  0.0532 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8306...  Training loss: 1.7693...  0.0565 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8307...  Training loss: 1.7702...  0.0543 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8308...  Training loss: 1.7996...  0.0563 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8309...  Training loss: 1.7317...  0.0574 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8310...  Training loss: 1.7427...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8311...  Training loss: 1.7729...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8312...  Training loss: 1.7534...  0.0565 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8313...  Training loss: 1.7909...  0.0567 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8314...  Training loss: 1.7839...  0.0590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8315...  Training loss: 1.8134...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8316...  Training loss: 1.7638...  0.0574 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8317...  Training loss: 1.7827...  0.0574 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8318...  Training loss: 1.7489...  0.0522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8319...  Training loss: 1.7668...  0.0547 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8320...  Training loss: 1.7546...  0.0561 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8321...  Training loss: 1.7770...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8322...  Training loss: 1.7322...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8323...  Training loss: 1.7976...  0.0523 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8324...  Training loss: 1.7632...  0.0556 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8325...  Training loss: 1.7868...  0.0521 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8326...  Training loss: 1.7027...  0.0568 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8327...  Training loss: 1.7673...  0.0563 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8328...  Training loss: 1.7739...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8329...  Training loss: 1.7591...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8330...  Training loss: 1.7295...  0.0555 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8331...  Training loss: 1.7165...  0.0578 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8332...  Training loss: 1.7775...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8333...  Training loss: 1.7321...  0.0534 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8334...  Training loss: 1.7451...  0.0539 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8335...  Training loss: 1.7764...  0.0572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8336...  Training loss: 1.7797...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8337...  Training loss: 1.8451...  0.0591 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8338...  Training loss: 1.8075...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8339...  Training loss: 1.8146...  0.0542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8340...  Training loss: 1.8027...  0.0584 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8341...  Training loss: 1.7677...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8342...  Training loss: 1.6966...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8343...  Training loss: 1.7219...  0.0559 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8344...  Training loss: 1.7786...  0.0568 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8345...  Training loss: 1.7418...  0.0536 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8346...  Training loss: 1.7906...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8347...  Training loss: 1.7341...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8348...  Training loss: 1.7713...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8349...  Training loss: 1.7581...  0.0523 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8350...  Training loss: 1.8106...  0.0586 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8351...  Training loss: 1.7503...  0.0534 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8352...  Training loss: 1.7496...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8353...  Training loss: 1.7430...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8354...  Training loss: 1.7798...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8355...  Training loss: 1.7317...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8356...  Training loss: 1.7195...  0.0568 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8357...  Training loss: 1.7828...  0.0559 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8358...  Training loss: 1.8064...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8359...  Training loss: 1.7834...  0.0537 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8360...  Training loss: 1.7455...  0.0582 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8361...  Training loss: 1.7982...  0.0580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8362...  Training loss: 1.8422...  0.0597 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8363...  Training loss: 1.7151...  0.0534 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8364...  Training loss: 1.8014...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8365...  Training loss: 1.7495...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8366...  Training loss: 1.7533...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8367...  Training loss: 1.7770...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8368...  Training loss: 1.7511...  0.0547 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8369...  Training loss: 1.7393...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8370...  Training loss: 1.7380...  0.0578 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8371...  Training loss: 1.7291...  0.0554 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8372...  Training loss: 1.7210...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8373...  Training loss: 1.7338...  0.0532 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8374...  Training loss: 1.7314...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8375...  Training loss: 1.7629...  0.0557 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8376...  Training loss: 1.8047...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8377...  Training loss: 1.7562...  0.0558 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8378...  Training loss: 1.7074...  0.0522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8379...  Training loss: 1.7400...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8380...  Training loss: 1.8209...  0.0555 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8381...  Training loss: 1.7536...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8382...  Training loss: 1.6870...  0.0556 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8383...  Training loss: 1.7799...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8384...  Training loss: 1.7486...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8385...  Training loss: 1.7341...  0.0533 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8386...  Training loss: 1.7462...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8387...  Training loss: 1.7311...  0.0553 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8388...  Training loss: 1.7447...  0.0538 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8389...  Training loss: 1.7643...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8390...  Training loss: 1.7459...  0.0533 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8391...  Training loss: 1.7190...  0.0574 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8392...  Training loss: 1.7564...  0.0539 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8393...  Training loss: 1.7278...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8394...  Training loss: 1.7526...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8395...  Training loss: 1.7534...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8396...  Training loss: 1.7588...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8397...  Training loss: 1.7080...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8398...  Training loss: 1.7466...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8399...  Training loss: 1.7139...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8400...  Training loss: 1.7562...  0.0586 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8401...  Training loss: 1.7523...  0.0561 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8402...  Training loss: 1.7572...  0.0542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8403...  Training loss: 1.7242...  0.0534 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8404...  Training loss: 1.7397...  0.0543 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 8405...  Training loss: 1.7396...  0.0536 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8406...  Training loss: 1.7710...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8407...  Training loss: 1.7610...  0.0587 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8408...  Training loss: 1.7790...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8409...  Training loss: 1.7558...  0.0536 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8410...  Training loss: 1.7530...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8411...  Training loss: 1.8137...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8412...  Training loss: 1.7776...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8413...  Training loss: 1.7611...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8414...  Training loss: 1.7321...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8415...  Training loss: 1.7270...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8416...  Training loss: 1.8084...  0.0584 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8417...  Training loss: 1.8363...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8418...  Training loss: 1.7979...  0.0558 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8419...  Training loss: 1.7595...  0.0602 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8420...  Training loss: 1.7685...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8421...  Training loss: 1.7868...  0.0591 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8422...  Training loss: 1.7117...  0.0588 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8423...  Training loss: 1.7026...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8424...  Training loss: 1.7103...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8425...  Training loss: 1.7537...  0.0554 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8426...  Training loss: 1.7738...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8427...  Training loss: 1.7381...  0.0582 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8428...  Training loss: 1.7890...  0.0536 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8429...  Training loss: 1.7920...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8430...  Training loss: 1.7338...  0.0568 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8431...  Training loss: 1.7591...  0.0543 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8432...  Training loss: 1.8321...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8433...  Training loss: 1.7785...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8434...  Training loss: 1.7611...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8435...  Training loss: 1.7453...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8436...  Training loss: 1.7616...  0.0555 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8437...  Training loss: 1.7262...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8438...  Training loss: 1.8390...  0.0532 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8439...  Training loss: 1.7878...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8440...  Training loss: 1.7368...  0.0547 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8441...  Training loss: 1.6963...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8442...  Training loss: 1.8120...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8443...  Training loss: 1.7489...  0.0574 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8444...  Training loss: 1.7880...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8445...  Training loss: 1.7638...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8446...  Training loss: 1.6880...  0.0542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8447...  Training loss: 1.7065...  0.0576 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8448...  Training loss: 1.7665...  0.0541 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8449...  Training loss: 1.7090...  0.0588 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8450...  Training loss: 1.7337...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8451...  Training loss: 1.7816...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8452...  Training loss: 1.7263...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8453...  Training loss: 1.7630...  0.0578 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8454...  Training loss: 1.7833...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8455...  Training loss: 1.7268...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8456...  Training loss: 1.7808...  0.0533 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8457...  Training loss: 1.7362...  0.0587 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8458...  Training loss: 1.7728...  0.0577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8459...  Training loss: 1.7245...  0.0571 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8460...  Training loss: 1.7682...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8461...  Training loss: 1.7892...  0.0563 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8462...  Training loss: 1.7601...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8463...  Training loss: 1.7339...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8464...  Training loss: 1.7746...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8465...  Training loss: 1.7935...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8466...  Training loss: 1.7921...  0.0580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8467...  Training loss: 1.8061...  0.0542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8468...  Training loss: 1.7768...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8469...  Training loss: 1.8111...  0.0547 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8470...  Training loss: 1.8111...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8471...  Training loss: 1.7588...  0.0583 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8472...  Training loss: 1.7819...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8473...  Training loss: 1.7858...  0.0534 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8474...  Training loss: 1.7483...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8475...  Training loss: 1.7267...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8476...  Training loss: 1.6920...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8477...  Training loss: 1.7320...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8478...  Training loss: 1.7608...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8479...  Training loss: 1.7963...  0.0554 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8480...  Training loss: 1.7468...  0.0555 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8481...  Training loss: 1.7340...  0.0593 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8482...  Training loss: 1.7425...  0.0547 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8483...  Training loss: 1.7562...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8484...  Training loss: 1.7267...  0.0539 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8485...  Training loss: 1.7986...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8486...  Training loss: 1.7938...  0.0532 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8487...  Training loss: 1.7270...  0.0568 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8488...  Training loss: 1.7908...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8489...  Training loss: 1.7436...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8490...  Training loss: 1.7185...  0.0580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8491...  Training loss: 1.7499...  0.0564 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8492...  Training loss: 1.7834...  0.0534 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8493...  Training loss: 1.7697...  0.0585 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8494...  Training loss: 1.7279...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8495...  Training loss: 1.7176...  0.0556 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8496...  Training loss: 1.7302...  0.0523 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8497...  Training loss: 1.7327...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8498...  Training loss: 1.7479...  0.0575 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8499...  Training loss: 1.7652...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8500...  Training loss: 1.7662...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8501...  Training loss: 1.7419...  0.0569 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8502...  Training loss: 1.7643...  0.0574 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8503...  Training loss: 1.7909...  0.0583 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8504...  Training loss: 1.7541...  0.0528 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 8505...  Training loss: 1.6837...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8506...  Training loss: 1.7541...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8507...  Training loss: 1.7146...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8508...  Training loss: 1.7098...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8509...  Training loss: 1.7368...  0.0568 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8510...  Training loss: 1.8184...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8511...  Training loss: 1.7957...  0.0522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8512...  Training loss: 1.7793...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8513...  Training loss: 1.7262...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8514...  Training loss: 1.7677...  0.0578 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8515...  Training loss: 1.7198...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8516...  Training loss: 1.7533...  0.0577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8517...  Training loss: 1.7640...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8518...  Training loss: 1.7579...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8519...  Training loss: 1.7581...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8520...  Training loss: 1.7477...  0.0581 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8521...  Training loss: 1.7223...  0.0606 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8522...  Training loss: 1.7536...  0.0523 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8523...  Training loss: 1.7561...  0.0540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8524...  Training loss: 1.7610...  0.0575 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8525...  Training loss: 1.7664...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8526...  Training loss: 1.7665...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8527...  Training loss: 1.7370...  0.0543 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8528...  Training loss: 1.7455...  0.0620 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8529...  Training loss: 1.7774...  0.0520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8530...  Training loss: 1.7433...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8531...  Training loss: 1.7507...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8532...  Training loss: 1.7641...  0.0547 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8533...  Training loss: 1.7344...  0.0590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8534...  Training loss: 1.7277...  0.0521 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8535...  Training loss: 1.8075...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8536...  Training loss: 1.7952...  0.0583 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8537...  Training loss: 1.7699...  0.0584 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8538...  Training loss: 1.8295...  0.0577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8539...  Training loss: 1.7548...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8540...  Training loss: 1.8233...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8541...  Training loss: 1.7606...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8542...  Training loss: 1.7078...  0.0560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8543...  Training loss: 1.7739...  0.0543 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8544...  Training loss: 1.7952...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8545...  Training loss: 1.8442...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8546...  Training loss: 1.7665...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8547...  Training loss: 1.7762...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8548...  Training loss: 1.7676...  0.0582 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8549...  Training loss: 1.7806...  0.0598 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8550...  Training loss: 1.7661...  0.0539 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8551...  Training loss: 1.7520...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8552...  Training loss: 1.7678...  0.0590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8553...  Training loss: 1.7662...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8554...  Training loss: 1.7292...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8555...  Training loss: 1.7335...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8556...  Training loss: 1.7689...  0.0539 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8557...  Training loss: 1.7311...  0.0534 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8558...  Training loss: 1.8159...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8559...  Training loss: 1.6912...  0.0558 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8560...  Training loss: 1.7633...  0.0573 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8561...  Training loss: 1.7607...  0.0584 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8562...  Training loss: 1.7635...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8563...  Training loss: 1.8305...  0.0540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8564...  Training loss: 1.7658...  0.0541 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8565...  Training loss: 1.7370...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8566...  Training loss: 1.7159...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8567...  Training loss: 1.7887...  0.0581 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8568...  Training loss: 1.7406...  0.0560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8569...  Training loss: 1.8068...  0.0565 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8570...  Training loss: 1.7740...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8571...  Training loss: 1.8113...  0.0547 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8572...  Training loss: 1.7933...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8573...  Training loss: 1.8110...  0.0553 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8574...  Training loss: 1.7995...  0.0572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8575...  Training loss: 1.7844...  0.0567 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8576...  Training loss: 1.7417...  0.0583 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8577...  Training loss: 1.7574...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8578...  Training loss: 1.7346...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8579...  Training loss: 1.7503...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8580...  Training loss: 1.7618...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8581...  Training loss: 1.7343...  0.0577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8582...  Training loss: 1.7315...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8583...  Training loss: 1.7695...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8584...  Training loss: 1.6790...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8585...  Training loss: 1.7848...  0.0520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8586...  Training loss: 1.8206...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8587...  Training loss: 1.7964...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8588...  Training loss: 1.7736...  0.0552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8589...  Training loss: 1.7664...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8590...  Training loss: 1.7429...  0.0572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8591...  Training loss: 1.7321...  0.0576 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8592...  Training loss: 1.7433...  0.0608 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8593...  Training loss: 1.7637...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8594...  Training loss: 1.7436...  0.0568 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8595...  Training loss: 1.7653...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8596...  Training loss: 1.7599...  0.0547 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8597...  Training loss: 1.7291...  0.0557 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8598...  Training loss: 1.7546...  0.0541 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8599...  Training loss: 1.6873...  0.0572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8600...  Training loss: 1.7882...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8601...  Training loss: 1.7978...  0.0579 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8602...  Training loss: 1.7340...  0.0587 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8603...  Training loss: 1.7333...  0.0567 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8604...  Training loss: 1.7289...  0.0550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 8605...  Training loss: 1.7777...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8606...  Training loss: 1.7565...  0.0558 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8607...  Training loss: 1.7947...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8608...  Training loss: 1.7907...  0.0541 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8609...  Training loss: 1.7965...  0.0541 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8610...  Training loss: 1.7309...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8611...  Training loss: 1.7816...  0.0540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8612...  Training loss: 1.7077...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8613...  Training loss: 1.7719...  0.0585 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8614...  Training loss: 1.7262...  0.0584 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8615...  Training loss: 1.7633...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8616...  Training loss: 1.7525...  0.0563 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8617...  Training loss: 1.7584...  0.0564 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8618...  Training loss: 1.7236...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8619...  Training loss: 1.7539...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8620...  Training loss: 1.7315...  0.0528 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8621...  Training loss: 1.7850...  0.0542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8622...  Training loss: 1.7740...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8623...  Training loss: 1.7551...  0.0540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8624...  Training loss: 1.8373...  0.0574 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8625...  Training loss: 1.8636...  0.0577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8626...  Training loss: 1.8328...  0.0617 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8627...  Training loss: 1.7489...  0.0542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8628...  Training loss: 1.8162...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8629...  Training loss: 1.7442...  0.0582 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8630...  Training loss: 1.7712...  0.0568 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8631...  Training loss: 1.7817...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8632...  Training loss: 1.8041...  0.0572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8633...  Training loss: 1.7519...  0.0556 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8634...  Training loss: 1.7495...  0.0522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8635...  Training loss: 1.7733...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8636...  Training loss: 1.7795...  0.0522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8637...  Training loss: 1.7198...  0.0558 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8638...  Training loss: 1.7800...  0.0570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8639...  Training loss: 1.7937...  0.0547 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8640...  Training loss: 1.7920...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8641...  Training loss: 1.8001...  0.0582 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8642...  Training loss: 1.7600...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8643...  Training loss: 1.7691...  0.0537 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8644...  Training loss: 1.8085...  0.0577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8645...  Training loss: 1.7865...  0.0521 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8646...  Training loss: 1.7613...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8647...  Training loss: 1.7207...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8648...  Training loss: 1.7827...  0.0523 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8649...  Training loss: 1.7469...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8650...  Training loss: 1.8053...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8651...  Training loss: 1.7208...  0.0580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8652...  Training loss: 1.8004...  0.0547 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8653...  Training loss: 1.7600...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8654...  Training loss: 1.7497...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8655...  Training loss: 1.7573...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8656...  Training loss: 1.7357...  0.0563 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8657...  Training loss: 1.7051...  0.0524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8658...  Training loss: 1.7573...  0.0532 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8659...  Training loss: 1.7767...  0.0532 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8660...  Training loss: 1.7782...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8661...  Training loss: 1.7493...  0.0558 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8662...  Training loss: 1.7690...  0.0576 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8663...  Training loss: 1.7733...  0.0549 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8664...  Training loss: 1.7467...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8665...  Training loss: 1.7504...  0.0544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8666...  Training loss: 1.7560...  0.0527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8667...  Training loss: 1.7199...  0.0522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8668...  Training loss: 1.7141...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8669...  Training loss: 1.7595...  0.0568 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8670...  Training loss: 1.8269...  0.0521 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8671...  Training loss: 1.8194...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8672...  Training loss: 1.7743...  0.0573 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8673...  Training loss: 1.6968...  0.0536 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8674...  Training loss: 1.7595...  0.0546 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8675...  Training loss: 1.6916...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8676...  Training loss: 1.7773...  0.0575 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8677...  Training loss: 1.7895...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8678...  Training loss: 1.7194...  0.0526 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8679...  Training loss: 1.7138...  0.0534 sec/batch\n",
      "Epoch: 14/20...  Training Step: 8680...  Training loss: 1.7201...  0.0556 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8681...  Training loss: 1.8309...  0.0563 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8682...  Training loss: 1.8339...  0.0535 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8683...  Training loss: 1.8111...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8684...  Training loss: 1.7331...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8685...  Training loss: 1.7676...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8686...  Training loss: 1.7936...  0.0577 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8687...  Training loss: 1.7311...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8688...  Training loss: 1.7163...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8689...  Training loss: 1.7002...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8690...  Training loss: 1.7240...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8691...  Training loss: 1.7420...  0.0541 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8692...  Training loss: 1.7146...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8693...  Training loss: 1.7576...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8694...  Training loss: 1.7300...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8695...  Training loss: 1.7871...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8696...  Training loss: 1.7961...  0.0587 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8697...  Training loss: 1.7677...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8698...  Training loss: 1.7565...  0.0520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8699...  Training loss: 1.7178...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8700...  Training loss: 1.7706...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8701...  Training loss: 1.8269...  0.0560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8702...  Training loss: 1.7510...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8703...  Training loss: 1.7490...  0.0569 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8704...  Training loss: 1.7611...  0.0545 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 8705...  Training loss: 1.7472...  0.0564 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8706...  Training loss: 1.7265...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8707...  Training loss: 1.7479...  0.0592 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8708...  Training loss: 1.7645...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8709...  Training loss: 1.7599...  0.0581 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8710...  Training loss: 1.7066...  0.0522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8711...  Training loss: 1.7292...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8712...  Training loss: 1.7625...  0.0563 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8713...  Training loss: 1.7461...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8714...  Training loss: 1.7382...  0.0577 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8715...  Training loss: 1.7331...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8716...  Training loss: 1.7383...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8717...  Training loss: 1.7805...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8718...  Training loss: 1.7641...  0.0577 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8719...  Training loss: 1.7696...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8720...  Training loss: 1.7229...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8721...  Training loss: 1.7561...  0.0581 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8722...  Training loss: 1.7671...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8723...  Training loss: 1.7511...  0.0577 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8724...  Training loss: 1.7721...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8725...  Training loss: 1.7606...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8726...  Training loss: 1.7457...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8727...  Training loss: 1.6243...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8728...  Training loss: 1.7508...  0.0519 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8729...  Training loss: 1.7070...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8730...  Training loss: 1.7758...  0.0543 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8731...  Training loss: 1.7356...  0.0589 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8732...  Training loss: 1.7247...  0.0556 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8733...  Training loss: 1.7566...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8734...  Training loss: 1.7768...  0.0541 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8735...  Training loss: 1.7650...  0.0535 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8736...  Training loss: 1.7695...  0.0564 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8737...  Training loss: 1.7276...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8738...  Training loss: 1.7500...  0.0566 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8739...  Training loss: 1.7214...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8740...  Training loss: 1.7920...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8741...  Training loss: 1.7486...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8742...  Training loss: 1.7319...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8743...  Training loss: 1.7842...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8744...  Training loss: 1.7431...  0.0562 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8745...  Training loss: 1.7119...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8746...  Training loss: 1.6876...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8747...  Training loss: 1.7147...  0.0579 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8748...  Training loss: 1.7418...  0.0542 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8749...  Training loss: 1.7423...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8750...  Training loss: 1.7571...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8751...  Training loss: 1.7986...  0.0574 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8752...  Training loss: 1.7634...  0.0543 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8753...  Training loss: 1.6774...  0.0569 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8754...  Training loss: 1.7484...  0.0567 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8755...  Training loss: 1.7878...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8756...  Training loss: 1.7718...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8757...  Training loss: 1.7723...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8758...  Training loss: 1.7372...  0.0573 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8759...  Training loss: 1.7789...  0.0558 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8760...  Training loss: 1.7832...  0.0533 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8761...  Training loss: 1.6791...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8762...  Training loss: 1.7470...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8763...  Training loss: 1.7165...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8764...  Training loss: 1.7369...  0.0553 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8765...  Training loss: 1.7377...  0.0560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8766...  Training loss: 1.7888...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8767...  Training loss: 1.7079...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8768...  Training loss: 1.8015...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8769...  Training loss: 1.7466...  0.0554 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8770...  Training loss: 1.7551...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8771...  Training loss: 1.6916...  0.0551 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8772...  Training loss: 1.7953...  0.0614 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8773...  Training loss: 1.7413...  0.0531 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8774...  Training loss: 1.7602...  0.0569 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8775...  Training loss: 1.7372...  0.0573 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8776...  Training loss: 1.7755...  0.0564 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8777...  Training loss: 1.7837...  0.0560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8778...  Training loss: 1.6872...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8779...  Training loss: 1.7907...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8780...  Training loss: 1.7001...  0.0533 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8781...  Training loss: 1.7266...  0.0596 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8782...  Training loss: 1.7171...  0.0537 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8783...  Training loss: 1.7951...  0.0564 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8784...  Training loss: 1.8114...  0.0521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8785...  Training loss: 1.7645...  0.0594 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8786...  Training loss: 1.7169...  0.0571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8787...  Training loss: 1.7950...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8788...  Training loss: 1.7325...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8789...  Training loss: 1.7743...  0.0556 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8790...  Training loss: 1.7217...  0.0517 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8791...  Training loss: 1.7172...  0.0576 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8792...  Training loss: 1.7205...  0.0576 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8793...  Training loss: 1.7533...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8794...  Training loss: 1.7220...  0.0557 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8795...  Training loss: 1.7483...  0.0580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8796...  Training loss: 1.7899...  0.0535 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8797...  Training loss: 1.7174...  0.0521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8798...  Training loss: 1.7871...  0.0551 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8799...  Training loss: 1.7207...  0.0522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8800...  Training loss: 1.7556...  0.0584 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8801...  Training loss: 1.7237...  0.0536 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8802...  Training loss: 1.6971...  0.0576 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8803...  Training loss: 1.7506...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8804...  Training loss: 1.7550...  0.0526 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 8805...  Training loss: 1.7757...  0.0564 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8806...  Training loss: 1.7691...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8807...  Training loss: 1.8238...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8808...  Training loss: 1.7219...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8809...  Training loss: 1.7707...  0.0521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8810...  Training loss: 1.7860...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8811...  Training loss: 1.7561...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8812...  Training loss: 1.8025...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8813...  Training loss: 1.7853...  0.0556 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8814...  Training loss: 1.7686...  0.0522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8815...  Training loss: 1.7459...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8816...  Training loss: 1.7404...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8817...  Training loss: 1.7408...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8818...  Training loss: 1.7423...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8819...  Training loss: 1.7927...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8820...  Training loss: 1.7592...  0.0576 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8821...  Training loss: 1.8037...  0.0577 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8822...  Training loss: 1.6779...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8823...  Training loss: 1.7405...  0.0542 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8824...  Training loss: 1.7444...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8825...  Training loss: 1.7096...  0.0601 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8826...  Training loss: 1.7949...  0.0555 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8827...  Training loss: 1.7689...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8828...  Training loss: 1.7638...  0.0584 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8829...  Training loss: 1.7582...  0.0577 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8830...  Training loss: 1.7930...  0.0522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8831...  Training loss: 1.7524...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8832...  Training loss: 1.7201...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8833...  Training loss: 1.7379...  0.0531 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8834...  Training loss: 1.7979...  0.0531 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8835...  Training loss: 1.7439...  0.0586 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8836...  Training loss: 1.7668...  0.0521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8837...  Training loss: 1.7560...  0.0558 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8838...  Training loss: 1.7806...  0.0521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8839...  Training loss: 1.7740...  0.0565 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8840...  Training loss: 1.7164...  0.0555 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8841...  Training loss: 1.7271...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8842...  Training loss: 1.7287...  0.0558 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8843...  Training loss: 1.7769...  0.0573 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8844...  Training loss: 1.7447...  0.0577 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8845...  Training loss: 1.7930...  0.0568 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8846...  Training loss: 1.7511...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8847...  Training loss: 1.7646...  0.0580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8848...  Training loss: 1.7431...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8849...  Training loss: 1.7365...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8850...  Training loss: 1.7253...  0.0543 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8851...  Training loss: 1.7341...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8852...  Training loss: 1.7626...  0.0539 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8853...  Training loss: 1.7389...  0.0619 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8854...  Training loss: 1.7179...  0.0551 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8855...  Training loss: 1.7404...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8856...  Training loss: 1.7544...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8857...  Training loss: 1.7339...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8858...  Training loss: 1.7310...  0.0553 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8859...  Training loss: 1.7298...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8860...  Training loss: 1.7235...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8861...  Training loss: 1.7102...  0.0568 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8862...  Training loss: 1.7756...  0.0543 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8863...  Training loss: 1.7581...  0.0580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8864...  Training loss: 1.6993...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8865...  Training loss: 1.7062...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8866...  Training loss: 1.7468...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8867...  Training loss: 1.7413...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8868...  Training loss: 1.7294...  0.0543 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8869...  Training loss: 1.7455...  0.0555 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8870...  Training loss: 1.8082...  0.0579 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8871...  Training loss: 1.7570...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8872...  Training loss: 1.7972...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8873...  Training loss: 1.7695...  0.0566 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8874...  Training loss: 1.7230...  0.0574 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8875...  Training loss: 1.7345...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8876...  Training loss: 1.8076...  0.0565 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8877...  Training loss: 1.7505...  0.0559 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8878...  Training loss: 1.8290...  0.0584 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8879...  Training loss: 1.7371...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8880...  Training loss: 1.7717...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8881...  Training loss: 1.7346...  0.0567 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8882...  Training loss: 1.7228...  0.0567 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8883...  Training loss: 1.7436...  0.0567 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8884...  Training loss: 1.7165...  0.0561 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8885...  Training loss: 1.7520...  0.0592 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8886...  Training loss: 1.7186...  0.0541 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8887...  Training loss: 1.7959...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8888...  Training loss: 1.7559...  0.0575 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8889...  Training loss: 1.7520...  0.0539 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8890...  Training loss: 1.7189...  0.0521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8891...  Training loss: 1.7554...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8892...  Training loss: 1.7669...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8893...  Training loss: 1.7794...  0.0555 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8894...  Training loss: 1.7876...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8895...  Training loss: 1.7917...  0.0538 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8896...  Training loss: 1.7911...  0.0555 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8897...  Training loss: 1.7787...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8898...  Training loss: 1.7404...  0.0533 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8899...  Training loss: 1.8254...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8900...  Training loss: 1.7854...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8901...  Training loss: 1.7603...  0.0573 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8902...  Training loss: 1.7674...  0.0573 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8903...  Training loss: 1.7983...  0.0533 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8904...  Training loss: 1.7152...  0.0522 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 8905...  Training loss: 1.7487...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8906...  Training loss: 1.7866...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8907...  Training loss: 1.7771...  0.0551 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8908...  Training loss: 1.7264...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8909...  Training loss: 1.7790...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8910...  Training loss: 1.7385...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8911...  Training loss: 1.8008...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8912...  Training loss: 1.7225...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8913...  Training loss: 1.7148...  0.0542 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8914...  Training loss: 1.7409...  0.0565 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8915...  Training loss: 1.7222...  0.0522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8916...  Training loss: 1.7754...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8917...  Training loss: 1.7700...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8918...  Training loss: 1.7142...  0.0628 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8919...  Training loss: 1.7315...  0.0562 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8920...  Training loss: 1.7678...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8921...  Training loss: 1.7061...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8922...  Training loss: 1.6948...  0.0521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8923...  Training loss: 1.7126...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8924...  Training loss: 1.7346...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8925...  Training loss: 1.7507...  0.0617 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8926...  Training loss: 1.7406...  0.0564 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8927...  Training loss: 1.7708...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8928...  Training loss: 1.7660...  0.0521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8929...  Training loss: 1.7135...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8930...  Training loss: 1.7510...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8931...  Training loss: 1.7542...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8932...  Training loss: 1.7338...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8933...  Training loss: 1.7564...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8934...  Training loss: 1.7522...  0.0542 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8935...  Training loss: 1.7984...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8936...  Training loss: 1.7376...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8937...  Training loss: 1.7306...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8938...  Training loss: 1.7559...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8939...  Training loss: 1.7741...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8940...  Training loss: 1.7462...  0.0582 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8941...  Training loss: 1.7599...  0.0553 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8942...  Training loss: 1.7168...  0.0557 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8943...  Training loss: 1.7393...  0.0583 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8944...  Training loss: 1.7706...  0.0571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8945...  Training loss: 1.7626...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8946...  Training loss: 1.6952...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8947...  Training loss: 1.7466...  0.0581 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8948...  Training loss: 1.7711...  0.0543 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8949...  Training loss: 1.7491...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8950...  Training loss: 1.7115...  0.0541 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8951...  Training loss: 1.7194...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8952...  Training loss: 1.7581...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8953...  Training loss: 1.7225...  0.0568 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8954...  Training loss: 1.7347...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8955...  Training loss: 1.7690...  0.0597 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8956...  Training loss: 1.7906...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8957...  Training loss: 1.8379...  0.0584 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8958...  Training loss: 1.7572...  0.0538 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8959...  Training loss: 1.7982...  0.0575 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8960...  Training loss: 1.7927...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8961...  Training loss: 1.7676...  0.0553 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8962...  Training loss: 1.6897...  0.0562 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8963...  Training loss: 1.7064...  0.0573 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8964...  Training loss: 1.7696...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8965...  Training loss: 1.7214...  0.0576 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8966...  Training loss: 1.7617...  0.0572 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8967...  Training loss: 1.7349...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8968...  Training loss: 1.7626...  0.0551 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8969...  Training loss: 1.7643...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8970...  Training loss: 1.7976...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8971...  Training loss: 1.7529...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8972...  Training loss: 1.7639...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8973...  Training loss: 1.7483...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8974...  Training loss: 1.7634...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8975...  Training loss: 1.7264...  0.0522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8976...  Training loss: 1.7056...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8977...  Training loss: 1.7558...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8978...  Training loss: 1.7827...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8979...  Training loss: 1.7970...  0.0560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8980...  Training loss: 1.7324...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8981...  Training loss: 1.7924...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8982...  Training loss: 1.8302...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8983...  Training loss: 1.7242...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8984...  Training loss: 1.7740...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8985...  Training loss: 1.7165...  0.0553 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8986...  Training loss: 1.7542...  0.0555 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8987...  Training loss: 1.7422...  0.0573 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8988...  Training loss: 1.7517...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8989...  Training loss: 1.7423...  0.0572 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8990...  Training loss: 1.7302...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8991...  Training loss: 1.7225...  0.0533 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8992...  Training loss: 1.7029...  0.0535 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8993...  Training loss: 1.7293...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8994...  Training loss: 1.6841...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8995...  Training loss: 1.7595...  0.0520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8996...  Training loss: 1.7799...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8997...  Training loss: 1.7260...  0.0559 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8998...  Training loss: 1.6761...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 8999...  Training loss: 1.7130...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9000...  Training loss: 1.7824...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9001...  Training loss: 1.7566...  0.0566 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9002...  Training loss: 1.6914...  0.0562 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9003...  Training loss: 1.7666...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9004...  Training loss: 1.7499...  0.0547 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 9005...  Training loss: 1.7102...  0.0531 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9006...  Training loss: 1.7235...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9007...  Training loss: 1.7098...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9008...  Training loss: 1.7076...  0.0560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9009...  Training loss: 1.7486...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9010...  Training loss: 1.7357...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9011...  Training loss: 1.7098...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9012...  Training loss: 1.7367...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9013...  Training loss: 1.7297...  0.0575 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9014...  Training loss: 1.7369...  0.0582 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9015...  Training loss: 1.7421...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9016...  Training loss: 1.7422...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9017...  Training loss: 1.7088...  0.0520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9018...  Training loss: 1.7189...  0.0531 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9019...  Training loss: 1.7047...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9020...  Training loss: 1.7530...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9021...  Training loss: 1.7609...  0.0595 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9022...  Training loss: 1.7349...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9023...  Training loss: 1.6940...  0.0557 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9024...  Training loss: 1.7298...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9025...  Training loss: 1.7286...  0.0567 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9026...  Training loss: 1.7488...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9027...  Training loss: 1.7569...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9028...  Training loss: 1.7792...  0.0551 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9029...  Training loss: 1.7471...  0.0569 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9030...  Training loss: 1.7394...  0.0554 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9031...  Training loss: 1.7725...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9032...  Training loss: 1.7459...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9033...  Training loss: 1.7567...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9034...  Training loss: 1.7128...  0.0557 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9035...  Training loss: 1.7147...  0.0542 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9036...  Training loss: 1.8010...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9037...  Training loss: 1.8458...  0.0522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9038...  Training loss: 1.8141...  0.0565 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9039...  Training loss: 1.7593...  0.0568 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9040...  Training loss: 1.7625...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9041...  Training loss: 1.7744...  0.0556 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9042...  Training loss: 1.7078...  0.0535 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9043...  Training loss: 1.7187...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9044...  Training loss: 1.7060...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9045...  Training loss: 1.7420...  0.0538 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9046...  Training loss: 1.7510...  0.0559 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9047...  Training loss: 1.7558...  0.0556 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9048...  Training loss: 1.7820...  0.0531 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9049...  Training loss: 1.7565...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9050...  Training loss: 1.7402...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9051...  Training loss: 1.7575...  0.0580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9052...  Training loss: 1.8362...  0.0583 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9053...  Training loss: 1.7551...  0.0559 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9054...  Training loss: 1.7532...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9055...  Training loss: 1.7155...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9056...  Training loss: 1.7437...  0.0564 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9057...  Training loss: 1.7164...  0.0588 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9058...  Training loss: 1.8257...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9059...  Training loss: 1.7608...  0.0632 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9060...  Training loss: 1.7134...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9061...  Training loss: 1.6674...  0.0551 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9062...  Training loss: 1.7834...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9063...  Training loss: 1.7298...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9064...  Training loss: 1.7555...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9065...  Training loss: 1.7409...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9066...  Training loss: 1.6453...  0.0557 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9067...  Training loss: 1.6776...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9068...  Training loss: 1.7542...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9069...  Training loss: 1.7056...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9070...  Training loss: 1.7109...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9071...  Training loss: 1.7788...  0.0565 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9072...  Training loss: 1.7036...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9073...  Training loss: 1.7441...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9074...  Training loss: 1.7399...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9075...  Training loss: 1.7069...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9076...  Training loss: 1.7648...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9077...  Training loss: 1.6926...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9078...  Training loss: 1.7648...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9079...  Training loss: 1.7077...  0.0567 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9080...  Training loss: 1.7678...  0.0562 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9081...  Training loss: 1.7758...  0.0554 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9082...  Training loss: 1.7448...  0.0596 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9083...  Training loss: 1.7114...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9084...  Training loss: 1.7585...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9085...  Training loss: 1.7864...  0.0543 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9086...  Training loss: 1.7830...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9087...  Training loss: 1.7907...  0.0541 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9088...  Training loss: 1.7561...  0.0531 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9089...  Training loss: 1.8028...  0.0574 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9090...  Training loss: 1.7773...  0.0555 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9091...  Training loss: 1.7201...  0.0522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9092...  Training loss: 1.7607...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9093...  Training loss: 1.7744...  0.0541 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9094...  Training loss: 1.7560...  0.0588 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9095...  Training loss: 1.7274...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9096...  Training loss: 1.6934...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9097...  Training loss: 1.7520...  0.0586 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9098...  Training loss: 1.7585...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9099...  Training loss: 1.7724...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9100...  Training loss: 1.7268...  0.0607 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9101...  Training loss: 1.7137...  0.0531 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9102...  Training loss: 1.7330...  0.0575 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9103...  Training loss: 1.7721...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9104...  Training loss: 1.7154...  0.0532 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 9105...  Training loss: 1.7797...  0.0557 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9106...  Training loss: 1.7510...  0.0558 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9107...  Training loss: 1.7071...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9108...  Training loss: 1.7762...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9109...  Training loss: 1.7335...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9110...  Training loss: 1.6881...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9111...  Training loss: 1.7428...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9112...  Training loss: 1.7866...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9113...  Training loss: 1.7547...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9114...  Training loss: 1.7366...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9115...  Training loss: 1.7198...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9116...  Training loss: 1.7263...  0.0567 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9117...  Training loss: 1.7057...  0.0601 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9118...  Training loss: 1.7434...  0.0602 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9119...  Training loss: 1.7294...  0.0551 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9120...  Training loss: 1.7288...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9121...  Training loss: 1.7451...  0.0522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9122...  Training loss: 1.7378...  0.0585 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9123...  Training loss: 1.7977...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9124...  Training loss: 1.7548...  0.0534 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9125...  Training loss: 1.6519...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9126...  Training loss: 1.7239...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9127...  Training loss: 1.6902...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9128...  Training loss: 1.7013...  0.0551 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9129...  Training loss: 1.7609...  0.0538 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9130...  Training loss: 1.8093...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9131...  Training loss: 1.7777...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9132...  Training loss: 1.7707...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9133...  Training loss: 1.7062...  0.0584 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9134...  Training loss: 1.7438...  0.0522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9135...  Training loss: 1.7182...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9136...  Training loss: 1.7420...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9137...  Training loss: 1.7288...  0.0557 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9138...  Training loss: 1.7381...  0.0522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9139...  Training loss: 1.7323...  0.0563 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9140...  Training loss: 1.7435...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9141...  Training loss: 1.7069...  0.0569 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9142...  Training loss: 1.7534...  0.0533 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9143...  Training loss: 1.7213...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9144...  Training loss: 1.7547...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9145...  Training loss: 1.7705...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9146...  Training loss: 1.7572...  0.0566 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9147...  Training loss: 1.7264...  0.0557 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9148...  Training loss: 1.7448...  0.0569 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9149...  Training loss: 1.7541...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9150...  Training loss: 1.7292...  0.0569 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9151...  Training loss: 1.7475...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9152...  Training loss: 1.7560...  0.0576 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9153...  Training loss: 1.7311...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9154...  Training loss: 1.7068...  0.0587 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9155...  Training loss: 1.8044...  0.0580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9156...  Training loss: 1.7977...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9157...  Training loss: 1.7484...  0.0588 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9158...  Training loss: 1.8149...  0.0532 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9159...  Training loss: 1.7249...  0.0522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9160...  Training loss: 1.8238...  0.0566 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9161...  Training loss: 1.7520...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9162...  Training loss: 1.7100...  0.0576 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9163...  Training loss: 1.7684...  0.0573 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9164...  Training loss: 1.7700...  0.0560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9165...  Training loss: 1.8433...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9166...  Training loss: 1.7304...  0.0564 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9167...  Training loss: 1.7564...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9168...  Training loss: 1.7591...  0.0603 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9169...  Training loss: 1.7534...  0.0575 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9170...  Training loss: 1.7450...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9171...  Training loss: 1.7360...  0.0551 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9172...  Training loss: 1.7435...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9173...  Training loss: 1.7313...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9174...  Training loss: 1.7257...  0.0521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9175...  Training loss: 1.7161...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9176...  Training loss: 1.7295...  0.0585 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9177...  Training loss: 1.7211...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9178...  Training loss: 1.7821...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9179...  Training loss: 1.6806...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9180...  Training loss: 1.7549...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9181...  Training loss: 1.7550...  0.0584 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9182...  Training loss: 1.7633...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9183...  Training loss: 1.8238...  0.0540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9184...  Training loss: 1.7651...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9185...  Training loss: 1.7138...  0.0541 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9186...  Training loss: 1.7266...  0.0533 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9187...  Training loss: 1.7778...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9188...  Training loss: 1.7337...  0.0524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9189...  Training loss: 1.7744...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9190...  Training loss: 1.7648...  0.0611 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9191...  Training loss: 1.7975...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9192...  Training loss: 1.7883...  0.0554 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9193...  Training loss: 1.7867...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9194...  Training loss: 1.8070...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9195...  Training loss: 1.7479...  0.0579 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9196...  Training loss: 1.7388...  0.0571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9197...  Training loss: 1.7400...  0.0559 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9198...  Training loss: 1.7304...  0.0581 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9199...  Training loss: 1.7291...  0.0539 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9200...  Training loss: 1.7289...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9201...  Training loss: 1.7391...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9202...  Training loss: 1.7100...  0.0556 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9203...  Training loss: 1.7283...  0.0569 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9204...  Training loss: 1.6846...  0.0580 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 9205...  Training loss: 1.7783...  0.0559 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9206...  Training loss: 1.7940...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9207...  Training loss: 1.7675...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9208...  Training loss: 1.7744...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9209...  Training loss: 1.7393...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9210...  Training loss: 1.7081...  0.0541 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9211...  Training loss: 1.7272...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9212...  Training loss: 1.7344...  0.0521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9213...  Training loss: 1.7429...  0.0554 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9214...  Training loss: 1.7304...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9215...  Training loss: 1.7504...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9216...  Training loss: 1.7399...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9217...  Training loss: 1.7216...  0.0569 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9218...  Training loss: 1.7596...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9219...  Training loss: 1.6548...  0.0551 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9220...  Training loss: 1.7716...  0.0571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9221...  Training loss: 1.7523...  0.0566 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9222...  Training loss: 1.7253...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9223...  Training loss: 1.7188...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9224...  Training loss: 1.7156...  0.0574 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9225...  Training loss: 1.7775...  0.0576 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9226...  Training loss: 1.7509...  0.0531 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9227...  Training loss: 1.7938...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9228...  Training loss: 1.8139...  0.0523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9229...  Training loss: 1.7840...  0.0543 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9230...  Training loss: 1.7185...  0.0571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9231...  Training loss: 1.7796...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9232...  Training loss: 1.7031...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9233...  Training loss: 1.7480...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9234...  Training loss: 1.7606...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9235...  Training loss: 1.7387...  0.0579 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9236...  Training loss: 1.7449...  0.0568 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9237...  Training loss: 1.7489...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9238...  Training loss: 1.7199...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9239...  Training loss: 1.7391...  0.0566 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9240...  Training loss: 1.7276...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9241...  Training loss: 1.7784...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9242...  Training loss: 1.7887...  0.0532 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9243...  Training loss: 1.7497...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9244...  Training loss: 1.8101...  0.0537 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9245...  Training loss: 1.8349...  0.0542 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9246...  Training loss: 1.8126...  0.0532 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9247...  Training loss: 1.7374...  0.0535 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9248...  Training loss: 1.8104...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9249...  Training loss: 1.7441...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9250...  Training loss: 1.7663...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9251...  Training loss: 1.7796...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9252...  Training loss: 1.8142...  0.0542 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9253...  Training loss: 1.7368...  0.0529 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9254...  Training loss: 1.7305...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9255...  Training loss: 1.7574...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9256...  Training loss: 1.7681...  0.0519 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9257...  Training loss: 1.7090...  0.0557 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9258...  Training loss: 1.7617...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9259...  Training loss: 1.7846...  0.0584 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9260...  Training loss: 1.7605...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9261...  Training loss: 1.7856...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9262...  Training loss: 1.7411...  0.0558 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9263...  Training loss: 1.7691...  0.0564 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9264...  Training loss: 1.7998...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9265...  Training loss: 1.7700...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9266...  Training loss: 1.7605...  0.0551 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9267...  Training loss: 1.7226...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9268...  Training loss: 1.7634...  0.0544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9269...  Training loss: 1.7254...  0.0555 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9270...  Training loss: 1.7784...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9271...  Training loss: 1.7166...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9272...  Training loss: 1.7848...  0.0543 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9273...  Training loss: 1.7576...  0.0553 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9274...  Training loss: 1.7163...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9275...  Training loss: 1.7291...  0.0594 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9276...  Training loss: 1.7224...  0.0565 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9277...  Training loss: 1.7017...  0.0527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9278...  Training loss: 1.7483...  0.0532 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9279...  Training loss: 1.7621...  0.0575 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9280...  Training loss: 1.7727...  0.0579 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9281...  Training loss: 1.7315...  0.0563 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9282...  Training loss: 1.7326...  0.0556 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9283...  Training loss: 1.7528...  0.0548 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9284...  Training loss: 1.7315...  0.0570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9285...  Training loss: 1.7438...  0.0594 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9286...  Training loss: 1.7469...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9287...  Training loss: 1.6935...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9288...  Training loss: 1.7128...  0.0593 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9289...  Training loss: 1.7581...  0.0582 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9290...  Training loss: 1.7909...  0.0521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9291...  Training loss: 1.8235...  0.0566 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9292...  Training loss: 1.7618...  0.0546 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9293...  Training loss: 1.7132...  0.0561 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9294...  Training loss: 1.7614...  0.0525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9295...  Training loss: 1.7027...  0.0576 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9296...  Training loss: 1.7695...  0.0572 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9297...  Training loss: 1.7974...  0.0528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9298...  Training loss: 1.7042...  0.0547 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9299...  Training loss: 1.7024...  0.0540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 9300...  Training loss: 1.7107...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9301...  Training loss: 1.8256...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9302...  Training loss: 1.8130...  0.0583 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9303...  Training loss: 1.7889...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9304...  Training loss: 1.7027...  0.0551 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 9305...  Training loss: 1.7394...  0.0564 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9306...  Training loss: 1.7668...  0.0564 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9307...  Training loss: 1.7074...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9308...  Training loss: 1.6909...  0.0571 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9309...  Training loss: 1.6933...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9310...  Training loss: 1.7136...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9311...  Training loss: 1.7253...  0.0573 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9312...  Training loss: 1.7020...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9313...  Training loss: 1.7400...  0.0576 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9314...  Training loss: 1.7065...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9315...  Training loss: 1.7732...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9316...  Training loss: 1.7807...  0.0577 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9317...  Training loss: 1.7507...  0.0521 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9318...  Training loss: 1.7431...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9319...  Training loss: 1.7199...  0.0554 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9320...  Training loss: 1.7721...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9321...  Training loss: 1.8008...  0.0537 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9322...  Training loss: 1.7520...  0.0543 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9323...  Training loss: 1.7290...  0.0578 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9324...  Training loss: 1.7587...  0.0572 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9325...  Training loss: 1.7373...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9326...  Training loss: 1.7105...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9327...  Training loss: 1.7171...  0.0553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9328...  Training loss: 1.7549...  0.0520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9329...  Training loss: 1.7541...  0.0563 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9330...  Training loss: 1.7042...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9331...  Training loss: 1.7273...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9332...  Training loss: 1.7509...  0.0588 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9333...  Training loss: 1.7385...  0.0541 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9334...  Training loss: 1.7336...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9335...  Training loss: 1.7093...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9336...  Training loss: 1.7293...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9337...  Training loss: 1.7412...  0.0554 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9338...  Training loss: 1.7567...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9339...  Training loss: 1.7589...  0.0520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9340...  Training loss: 1.7218...  0.0557 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9341...  Training loss: 1.7490...  0.0575 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9342...  Training loss: 1.7493...  0.0543 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9343...  Training loss: 1.7599...  0.0569 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9344...  Training loss: 1.7757...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9345...  Training loss: 1.7068...  0.0575 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9346...  Training loss: 1.7048...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9347...  Training loss: 1.6261...  0.0597 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9348...  Training loss: 1.7397...  0.0525 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9349...  Training loss: 1.7033...  0.0590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9350...  Training loss: 1.7734...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9351...  Training loss: 1.7211...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9352...  Training loss: 1.7321...  0.0569 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9353...  Training loss: 1.7454...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9354...  Training loss: 1.7382...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9355...  Training loss: 1.7412...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9356...  Training loss: 1.7485...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9357...  Training loss: 1.7116...  0.0520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9358...  Training loss: 1.7340...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9359...  Training loss: 1.7352...  0.0582 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9360...  Training loss: 1.7679...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9361...  Training loss: 1.7427...  0.0558 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9362...  Training loss: 1.7284...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9363...  Training loss: 1.7925...  0.0542 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9364...  Training loss: 1.7435...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9365...  Training loss: 1.7039...  0.0561 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9366...  Training loss: 1.6798...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9367...  Training loss: 1.7240...  0.0542 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9368...  Training loss: 1.7116...  0.0571 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9369...  Training loss: 1.7383...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9370...  Training loss: 1.7607...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9371...  Training loss: 1.7699...  0.0562 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9372...  Training loss: 1.7660...  0.0572 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9373...  Training loss: 1.6789...  0.0525 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9374...  Training loss: 1.7191...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9375...  Training loss: 1.7824...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9376...  Training loss: 1.7629...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9377...  Training loss: 1.7516...  0.0587 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9378...  Training loss: 1.7084...  0.0571 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9379...  Training loss: 1.7503...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9380...  Training loss: 1.7647...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9381...  Training loss: 1.6770...  0.0557 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9382...  Training loss: 1.7269...  0.0572 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9383...  Training loss: 1.6895...  0.0608 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9384...  Training loss: 1.7247...  0.0522 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9385...  Training loss: 1.7116...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9386...  Training loss: 1.7756...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9387...  Training loss: 1.7038...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9388...  Training loss: 1.7913...  0.0545 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9389...  Training loss: 1.7295...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9390...  Training loss: 1.7371...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9391...  Training loss: 1.6913...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9392...  Training loss: 1.8135...  0.0559 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9393...  Training loss: 1.7297...  0.0558 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9394...  Training loss: 1.7435...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9395...  Training loss: 1.7243...  0.0521 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9396...  Training loss: 1.7799...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9397...  Training loss: 1.7815...  0.0581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9398...  Training loss: 1.6767...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9399...  Training loss: 1.7779...  0.0568 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9400...  Training loss: 1.7033...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9401...  Training loss: 1.6966...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9402...  Training loss: 1.6963...  0.0573 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9403...  Training loss: 1.7800...  0.0581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9404...  Training loss: 1.7910...  0.0530 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 9405...  Training loss: 1.7367...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9406...  Training loss: 1.7097...  0.0523 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9407...  Training loss: 1.7741...  0.0555 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9408...  Training loss: 1.7331...  0.0542 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9409...  Training loss: 1.7473...  0.0560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9410...  Training loss: 1.6948...  0.0586 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9411...  Training loss: 1.6941...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9412...  Training loss: 1.7285...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9413...  Training loss: 1.7312...  0.0583 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9414...  Training loss: 1.7103...  0.0609 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9415...  Training loss: 1.7277...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9416...  Training loss: 1.7984...  0.0534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9417...  Training loss: 1.6907...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9418...  Training loss: 1.7817...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9419...  Training loss: 1.7102...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9420...  Training loss: 1.7112...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9421...  Training loss: 1.7098...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9422...  Training loss: 1.6856...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9423...  Training loss: 1.7170...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9424...  Training loss: 1.7767...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9425...  Training loss: 1.7588...  0.0566 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9426...  Training loss: 1.7704...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9427...  Training loss: 1.7976...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9428...  Training loss: 1.6872...  0.0525 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9429...  Training loss: 1.7552...  0.0574 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9430...  Training loss: 1.7850...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9431...  Training loss: 1.7364...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9432...  Training loss: 1.8012...  0.0521 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9433...  Training loss: 1.7750...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9434...  Training loss: 1.7327...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9435...  Training loss: 1.7310...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9436...  Training loss: 1.7232...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9437...  Training loss: 1.7321...  0.0589 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9438...  Training loss: 1.7383...  0.0582 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9439...  Training loss: 1.7831...  0.0579 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9440...  Training loss: 1.7327...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9441...  Training loss: 1.7799...  0.0590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9442...  Training loss: 1.6676...  0.0587 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9443...  Training loss: 1.7402...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9444...  Training loss: 1.7183...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9445...  Training loss: 1.6835...  0.0555 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9446...  Training loss: 1.7765...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9447...  Training loss: 1.7636...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9448...  Training loss: 1.7597...  0.0525 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9449...  Training loss: 1.7453...  0.0553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9450...  Training loss: 1.7820...  0.0587 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9451...  Training loss: 1.7604...  0.0610 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9452...  Training loss: 1.7126...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9453...  Training loss: 1.7544...  0.0523 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9454...  Training loss: 1.7824...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9455...  Training loss: 1.7254...  0.0538 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9456...  Training loss: 1.7483...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9457...  Training loss: 1.7439...  0.0582 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9458...  Training loss: 1.7665...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9459...  Training loss: 1.7369...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9460...  Training loss: 1.7056...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9461...  Training loss: 1.6882...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9462...  Training loss: 1.7144...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9463...  Training loss: 1.7564...  0.0560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9464...  Training loss: 1.7214...  0.0539 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9465...  Training loss: 1.7423...  0.0571 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9466...  Training loss: 1.7640...  0.0556 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9467...  Training loss: 1.7498...  0.0573 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9468...  Training loss: 1.7351...  0.0543 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9469...  Training loss: 1.7358...  0.0536 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9470...  Training loss: 1.6992...  0.0579 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9471...  Training loss: 1.7231...  0.0583 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9472...  Training loss: 1.7532...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9473...  Training loss: 1.7085...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9474...  Training loss: 1.7171...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9475...  Training loss: 1.6979...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9476...  Training loss: 1.7533...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9477...  Training loss: 1.7377...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9478...  Training loss: 1.7060...  0.0594 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9479...  Training loss: 1.7027...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9480...  Training loss: 1.7070...  0.0605 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9481...  Training loss: 1.6953...  0.0564 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9482...  Training loss: 1.7718...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9483...  Training loss: 1.7486...  0.0577 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9484...  Training loss: 1.7002...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9485...  Training loss: 1.6932...  0.0566 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9486...  Training loss: 1.7379...  0.0559 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9487...  Training loss: 1.7546...  0.0560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9488...  Training loss: 1.7250...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9489...  Training loss: 1.7372...  0.0540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9490...  Training loss: 1.7870...  0.0562 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9491...  Training loss: 1.7426...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9492...  Training loss: 1.7760...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9493...  Training loss: 1.7726...  0.0539 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9494...  Training loss: 1.7382...  0.0572 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9495...  Training loss: 1.7317...  0.0525 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9496...  Training loss: 1.7979...  0.0534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9497...  Training loss: 1.7539...  0.0523 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9498...  Training loss: 1.8161...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9499...  Training loss: 1.7339...  0.0569 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9500...  Training loss: 1.7611...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9501...  Training loss: 1.7152...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9502...  Training loss: 1.7378...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9503...  Training loss: 1.7240...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9504...  Training loss: 1.7203...  0.0524 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 9505...  Training loss: 1.7496...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9506...  Training loss: 1.7192...  0.0520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9507...  Training loss: 1.7719...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9508...  Training loss: 1.7348...  0.0539 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9509...  Training loss: 1.7270...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9510...  Training loss: 1.7210...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9511...  Training loss: 1.7283...  0.0562 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9512...  Training loss: 1.7747...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9513...  Training loss: 1.7522...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9514...  Training loss: 1.7625...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9515...  Training loss: 1.7533...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9516...  Training loss: 1.7642...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9517...  Training loss: 1.7676...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9518...  Training loss: 1.7282...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9519...  Training loss: 1.8014...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9520...  Training loss: 1.7854...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9521...  Training loss: 1.7749...  0.0580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9522...  Training loss: 1.7751...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9523...  Training loss: 1.7749...  0.0613 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9524...  Training loss: 1.7050...  0.0558 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9525...  Training loss: 1.7135...  0.0560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9526...  Training loss: 1.7694...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9527...  Training loss: 1.7679...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9528...  Training loss: 1.7161...  0.0540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9529...  Training loss: 1.7651...  0.0558 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9530...  Training loss: 1.7308...  0.0555 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9531...  Training loss: 1.8016...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9532...  Training loss: 1.6830...  0.0553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9533...  Training loss: 1.7001...  0.0521 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9534...  Training loss: 1.7412...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9535...  Training loss: 1.6958...  0.0581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9536...  Training loss: 1.7790...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9537...  Training loss: 1.7275...  0.0581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9538...  Training loss: 1.6996...  0.0535 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9539...  Training loss: 1.7057...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9540...  Training loss: 1.7590...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9541...  Training loss: 1.6929...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9542...  Training loss: 1.6824...  0.0567 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9543...  Training loss: 1.6991...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9544...  Training loss: 1.7355...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9545...  Training loss: 1.7322...  0.0587 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9546...  Training loss: 1.7286...  0.0557 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9547...  Training loss: 1.7628...  0.0543 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9548...  Training loss: 1.7454...  0.0569 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9549...  Training loss: 1.7034...  0.0558 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9550...  Training loss: 1.7414...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9551...  Training loss: 1.7608...  0.0545 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9552...  Training loss: 1.6957...  0.0536 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9553...  Training loss: 1.7623...  0.0561 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9554...  Training loss: 1.7655...  0.0560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9555...  Training loss: 1.7927...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9556...  Training loss: 1.7376...  0.0553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9557...  Training loss: 1.7511...  0.0539 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9558...  Training loss: 1.7325...  0.0578 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9559...  Training loss: 1.7395...  0.0582 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9560...  Training loss: 1.7225...  0.0540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9561...  Training loss: 1.7487...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9562...  Training loss: 1.6910...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9563...  Training loss: 1.7484...  0.0537 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9564...  Training loss: 1.7512...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9565...  Training loss: 1.7552...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9566...  Training loss: 1.6931...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9567...  Training loss: 1.7104...  0.0567 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9568...  Training loss: 1.7505...  0.0534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9569...  Training loss: 1.7211...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9570...  Training loss: 1.6992...  0.0570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9571...  Training loss: 1.6849...  0.0522 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9572...  Training loss: 1.7490...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9573...  Training loss: 1.7173...  0.0557 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9574...  Training loss: 1.7240...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9575...  Training loss: 1.7679...  0.0555 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9576...  Training loss: 1.7629...  0.0558 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9577...  Training loss: 1.8369...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9578...  Training loss: 1.7657...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9579...  Training loss: 1.7965...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9580...  Training loss: 1.7690...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9581...  Training loss: 1.7622...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9582...  Training loss: 1.6893...  0.0578 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9583...  Training loss: 1.6828...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9584...  Training loss: 1.7538...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9585...  Training loss: 1.7325...  0.0586 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9586...  Training loss: 1.7692...  0.0545 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9587...  Training loss: 1.7378...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9588...  Training loss: 1.7677...  0.0534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9589...  Training loss: 1.7560...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9590...  Training loss: 1.7669...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9591...  Training loss: 1.7010...  0.0523 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9592...  Training loss: 1.7424...  0.0537 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9593...  Training loss: 1.6984...  0.0555 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9594...  Training loss: 1.7686...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9595...  Training loss: 1.7289...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9596...  Training loss: 1.6906...  0.0570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9597...  Training loss: 1.7599...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9598...  Training loss: 1.7745...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9599...  Training loss: 1.7795...  0.0545 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9600...  Training loss: 1.7179...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9601...  Training loss: 1.7754...  0.0545 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9602...  Training loss: 1.8071...  0.0557 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9603...  Training loss: 1.7009...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9604...  Training loss: 1.7747...  0.0545 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 9605...  Training loss: 1.7124...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9606...  Training loss: 1.7376...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9607...  Training loss: 1.7606...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9608...  Training loss: 1.7261...  0.0565 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9609...  Training loss: 1.7215...  0.0562 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9610...  Training loss: 1.7167...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9611...  Training loss: 1.7046...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9612...  Training loss: 1.6830...  0.0534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9613...  Training loss: 1.7139...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9614...  Training loss: 1.6861...  0.0535 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9615...  Training loss: 1.7321...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9616...  Training loss: 1.7685...  0.0535 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9617...  Training loss: 1.7071...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9618...  Training loss: 1.6686...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9619...  Training loss: 1.7070...  0.0583 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9620...  Training loss: 1.7659...  0.0543 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9621...  Training loss: 1.7234...  0.0562 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9622...  Training loss: 1.6881...  0.0564 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9623...  Training loss: 1.7413...  0.0554 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9624...  Training loss: 1.7394...  0.0586 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9625...  Training loss: 1.7232...  0.0559 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9626...  Training loss: 1.7127...  0.0584 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9627...  Training loss: 1.6993...  0.0555 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9628...  Training loss: 1.7116...  0.0554 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9629...  Training loss: 1.7340...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9630...  Training loss: 1.7290...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9631...  Training loss: 1.7062...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9632...  Training loss: 1.7374...  0.0554 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9633...  Training loss: 1.7206...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9634...  Training loss: 1.7232...  0.0582 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9635...  Training loss: 1.7245...  0.0554 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9636...  Training loss: 1.7493...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9637...  Training loss: 1.6984...  0.0563 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9638...  Training loss: 1.7202...  0.0581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9639...  Training loss: 1.6951...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9640...  Training loss: 1.7303...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9641...  Training loss: 1.7498...  0.0536 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9642...  Training loss: 1.7302...  0.0556 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9643...  Training loss: 1.6843...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9644...  Training loss: 1.7238...  0.0541 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9645...  Training loss: 1.7426...  0.0534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9646...  Training loss: 1.7336...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9647...  Training loss: 1.7521...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9648...  Training loss: 1.7329...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9649...  Training loss: 1.7112...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9650...  Training loss: 1.7046...  0.0576 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9651...  Training loss: 1.7622...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9652...  Training loss: 1.7516...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9653...  Training loss: 1.7502...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9654...  Training loss: 1.7178...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9655...  Training loss: 1.7265...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9656...  Training loss: 1.7731...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9657...  Training loss: 1.8361...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9658...  Training loss: 1.7684...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9659...  Training loss: 1.7387...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9660...  Training loss: 1.7259...  0.0535 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9661...  Training loss: 1.7622...  0.0555 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9662...  Training loss: 1.7007...  0.0589 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9663...  Training loss: 1.6963...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9664...  Training loss: 1.6982...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9665...  Training loss: 1.7321...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9666...  Training loss: 1.7407...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9667...  Training loss: 1.7341...  0.0537 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9668...  Training loss: 1.7583...  0.0538 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9669...  Training loss: 1.7713...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9670...  Training loss: 1.7098...  0.0525 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9671...  Training loss: 1.7495...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9672...  Training loss: 1.8078...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9673...  Training loss: 1.7471...  0.0578 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9674...  Training loss: 1.7318...  0.0587 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9675...  Training loss: 1.7352...  0.0538 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9676...  Training loss: 1.7063...  0.0573 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9677...  Training loss: 1.7061...  0.0576 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9678...  Training loss: 1.8004...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9679...  Training loss: 1.7428...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9680...  Training loss: 1.7255...  0.0591 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9681...  Training loss: 1.6701...  0.0586 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9682...  Training loss: 1.7846...  0.0572 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9683...  Training loss: 1.7098...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9684...  Training loss: 1.7032...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9685...  Training loss: 1.7311...  0.0535 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9686...  Training loss: 1.6418...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9687...  Training loss: 1.6714...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9688...  Training loss: 1.7307...  0.0553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9689...  Training loss: 1.6963...  0.0539 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9690...  Training loss: 1.7039...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9691...  Training loss: 1.7570...  0.0557 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9692...  Training loss: 1.6918...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9693...  Training loss: 1.7427...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9694...  Training loss: 1.7567...  0.0542 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9695...  Training loss: 1.6978...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9696...  Training loss: 1.7566...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9697...  Training loss: 1.7247...  0.0575 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9698...  Training loss: 1.7465...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9699...  Training loss: 1.7114...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9700...  Training loss: 1.7634...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9701...  Training loss: 1.7742...  0.0556 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9702...  Training loss: 1.7384...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9703...  Training loss: 1.7198...  0.0586 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9704...  Training loss: 1.7550...  0.0554 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 9705...  Training loss: 1.7974...  0.0536 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9706...  Training loss: 1.7598...  0.0554 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9707...  Training loss: 1.7891...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9708...  Training loss: 1.7519...  0.0545 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9709...  Training loss: 1.7837...  0.0525 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9710...  Training loss: 1.7761...  0.0553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9711...  Training loss: 1.7021...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9712...  Training loss: 1.7731...  0.0545 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9713...  Training loss: 1.7722...  0.0576 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9714...  Training loss: 1.7273...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9715...  Training loss: 1.7093...  0.0587 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9716...  Training loss: 1.7126...  0.0586 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9717...  Training loss: 1.7257...  0.0542 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9718...  Training loss: 1.7384...  0.0523 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9719...  Training loss: 1.7804...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9720...  Training loss: 1.7264...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9721...  Training loss: 1.7064...  0.0555 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9722...  Training loss: 1.7310...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9723...  Training loss: 1.7511...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9724...  Training loss: 1.6796...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9725...  Training loss: 1.7583...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9726...  Training loss: 1.7620...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9727...  Training loss: 1.7021...  0.0523 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9728...  Training loss: 1.7568...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9729...  Training loss: 1.7058...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9730...  Training loss: 1.6809...  0.0572 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9731...  Training loss: 1.7150...  0.0575 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9732...  Training loss: 1.7931...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9733...  Training loss: 1.7477...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9734...  Training loss: 1.7149...  0.0556 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9735...  Training loss: 1.6993...  0.0580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9736...  Training loss: 1.7254...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9737...  Training loss: 1.6874...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9738...  Training loss: 1.7355...  0.0563 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9739...  Training loss: 1.7389...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9740...  Training loss: 1.7112...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9741...  Training loss: 1.7218...  0.0541 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9742...  Training loss: 1.7364...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9743...  Training loss: 1.7658...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9744...  Training loss: 1.7464...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9745...  Training loss: 1.6423...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9746...  Training loss: 1.7328...  0.0556 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9747...  Training loss: 1.6993...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9748...  Training loss: 1.6891...  0.0560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9749...  Training loss: 1.7256...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9750...  Training loss: 1.7683...  0.0534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9751...  Training loss: 1.7921...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9752...  Training loss: 1.7605...  0.0564 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9753...  Training loss: 1.6833...  0.0535 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9754...  Training loss: 1.7381...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9755...  Training loss: 1.7256...  0.0536 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9756...  Training loss: 1.7286...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9757...  Training loss: 1.7046...  0.0539 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9758...  Training loss: 1.7367...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9759...  Training loss: 1.7190...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9760...  Training loss: 1.7288...  0.0572 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9761...  Training loss: 1.7007...  0.0631 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9762...  Training loss: 1.7365...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9763...  Training loss: 1.7086...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9764...  Training loss: 1.7351...  0.0582 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9765...  Training loss: 1.7659...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9766...  Training loss: 1.7560...  0.0580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9767...  Training loss: 1.7193...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9768...  Training loss: 1.7323...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9769...  Training loss: 1.7413...  0.0545 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9770...  Training loss: 1.7126...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9771...  Training loss: 1.7367...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9772...  Training loss: 1.7463...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9773...  Training loss: 1.7190...  0.0604 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9774...  Training loss: 1.6846...  0.0541 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9775...  Training loss: 1.7968...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9776...  Training loss: 1.7851...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9777...  Training loss: 1.7312...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9778...  Training loss: 1.8137...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9779...  Training loss: 1.7213...  0.0554 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9780...  Training loss: 1.7891...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9781...  Training loss: 1.7355...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9782...  Training loss: 1.6987...  0.0582 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9783...  Training loss: 1.7483...  0.0577 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9784...  Training loss: 1.7580...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9785...  Training loss: 1.8229...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9786...  Training loss: 1.7195...  0.0534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9787...  Training loss: 1.7656...  0.0557 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9788...  Training loss: 1.7558...  0.0603 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9789...  Training loss: 1.7491...  0.0563 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9790...  Training loss: 1.7435...  0.0522 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9791...  Training loss: 1.7293...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9792...  Training loss: 1.7573...  0.0542 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9793...  Training loss: 1.7372...  0.0543 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9794...  Training loss: 1.7200...  0.0577 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9795...  Training loss: 1.6957...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9796...  Training loss: 1.7339...  0.0525 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9797...  Training loss: 1.7144...  0.0605 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9798...  Training loss: 1.7967...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9799...  Training loss: 1.6779...  0.0543 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9800...  Training loss: 1.7421...  0.0539 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9801...  Training loss: 1.7404...  0.0534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9802...  Training loss: 1.7431...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9803...  Training loss: 1.8133...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9804...  Training loss: 1.7463...  0.0576 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 9805...  Training loss: 1.6798...  0.0670 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9806...  Training loss: 1.7023...  0.0537 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9807...  Training loss: 1.7703...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9808...  Training loss: 1.7184...  0.0578 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9809...  Training loss: 1.7722...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9810...  Training loss: 1.7431...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9811...  Training loss: 1.7913...  0.0570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9812...  Training loss: 1.7652...  0.0537 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9813...  Training loss: 1.7719...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9814...  Training loss: 1.7549...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9815...  Training loss: 1.7380...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9816...  Training loss: 1.7210...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9817...  Training loss: 1.7242...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9818...  Training loss: 1.7061...  0.0538 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9819...  Training loss: 1.7255...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9820...  Training loss: 1.7261...  0.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9821...  Training loss: 1.7260...  0.0553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9822...  Training loss: 1.6870...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9823...  Training loss: 1.7264...  0.0571 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9824...  Training loss: 1.6648...  0.0619 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9825...  Training loss: 1.7668...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9826...  Training loss: 1.7756...  0.0553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9827...  Training loss: 1.7877...  0.0547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9828...  Training loss: 1.7507...  0.0562 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9829...  Training loss: 1.7284...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9830...  Training loss: 1.7229...  0.0536 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9831...  Training loss: 1.7018...  0.0527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9832...  Training loss: 1.7229...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9833...  Training loss: 1.7166...  0.0537 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9834...  Training loss: 1.7407...  0.0541 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9835...  Training loss: 1.7527...  0.0544 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9836...  Training loss: 1.7356...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9837...  Training loss: 1.6991...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9838...  Training loss: 1.7215...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9839...  Training loss: 1.6632...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9840...  Training loss: 1.7415...  0.0521 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9841...  Training loss: 1.7535...  0.0572 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9842...  Training loss: 1.7142...  0.0523 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9843...  Training loss: 1.6949...  0.0536 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9844...  Training loss: 1.7005...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9845...  Training loss: 1.7518...  0.0553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9846...  Training loss: 1.7410...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9847...  Training loss: 1.7912...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9848...  Training loss: 1.7882...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9849...  Training loss: 1.7530...  0.0534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9850...  Training loss: 1.7272...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9851...  Training loss: 1.7612...  0.0526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9852...  Training loss: 1.6804...  0.0580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9853...  Training loss: 1.7394...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9854...  Training loss: 1.7402...  0.0536 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9855...  Training loss: 1.7175...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9856...  Training loss: 1.7266...  0.0594 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9857...  Training loss: 1.7480...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9858...  Training loss: 1.6893...  0.0578 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9859...  Training loss: 1.7342...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9860...  Training loss: 1.7218...  0.0564 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9861...  Training loss: 1.7805...  0.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9862...  Training loss: 1.7586...  0.0553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9863...  Training loss: 1.7253...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9864...  Training loss: 1.8136...  0.0572 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9865...  Training loss: 1.8223...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9866...  Training loss: 1.8162...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9867...  Training loss: 1.7253...  0.0581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9868...  Training loss: 1.7916...  0.0538 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9869...  Training loss: 1.7139...  0.0553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9870...  Training loss: 1.7533...  0.0535 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9871...  Training loss: 1.7527...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9872...  Training loss: 1.7956...  0.0567 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9873...  Training loss: 1.7486...  0.0539 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9874...  Training loss: 1.7122...  0.0590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9875...  Training loss: 1.7479...  0.0535 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9876...  Training loss: 1.7499...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9877...  Training loss: 1.7041...  0.0587 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9878...  Training loss: 1.7476...  0.0577 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9879...  Training loss: 1.7583...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9880...  Training loss: 1.7589...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9881...  Training loss: 1.7791...  0.0528 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9882...  Training loss: 1.7371...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9883...  Training loss: 1.7291...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9884...  Training loss: 1.7822...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9885...  Training loss: 1.7623...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9886...  Training loss: 1.7448...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9887...  Training loss: 1.7084...  0.0561 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9888...  Training loss: 1.7524...  0.0535 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9889...  Training loss: 1.7446...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9890...  Training loss: 1.7580...  0.0539 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9891...  Training loss: 1.6991...  0.0579 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9892...  Training loss: 1.7860...  0.0595 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9893...  Training loss: 1.7174...  0.0553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9894...  Training loss: 1.7131...  0.0575 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9895...  Training loss: 1.7183...  0.0537 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9896...  Training loss: 1.7004...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9897...  Training loss: 1.6781...  0.0579 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9898...  Training loss: 1.7324...  0.0545 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9899...  Training loss: 1.7373...  0.0580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9900...  Training loss: 1.7530...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9901...  Training loss: 1.7281...  0.0524 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9902...  Training loss: 1.7270...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9903...  Training loss: 1.7490...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9904...  Training loss: 1.7062...  0.0530 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 9905...  Training loss: 1.7328...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9906...  Training loss: 1.7387...  0.0534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9907...  Training loss: 1.6880...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9908...  Training loss: 1.6937...  0.0556 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9909...  Training loss: 1.7456...  0.0595 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9910...  Training loss: 1.7827...  0.0529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9911...  Training loss: 1.7938...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9912...  Training loss: 1.7373...  0.0573 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9913...  Training loss: 1.7007...  0.0556 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9914...  Training loss: 1.7547...  0.0581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9915...  Training loss: 1.6924...  0.0560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9916...  Training loss: 1.7662...  0.0590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9917...  Training loss: 1.7825...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9918...  Training loss: 1.7062...  0.0538 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9919...  Training loss: 1.6839...  0.0549 sec/batch\n",
      "Epoch: 16/20...  Training Step: 9920...  Training loss: 1.6819...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9921...  Training loss: 1.8217...  0.0531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9922...  Training loss: 1.8034...  0.0576 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9923...  Training loss: 1.7813...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9924...  Training loss: 1.7047...  0.0538 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9925...  Training loss: 1.7312...  0.0536 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9926...  Training loss: 1.7596...  0.0544 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9927...  Training loss: 1.7068...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9928...  Training loss: 1.6935...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9929...  Training loss: 1.6806...  0.0560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9930...  Training loss: 1.6857...  0.0543 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9931...  Training loss: 1.7154...  0.0585 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9932...  Training loss: 1.7093...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9933...  Training loss: 1.7211...  0.0561 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9934...  Training loss: 1.6969...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9935...  Training loss: 1.7535...  0.0543 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9936...  Training loss: 1.7702...  0.0576 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9937...  Training loss: 1.7384...  0.0579 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9938...  Training loss: 1.7639...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9939...  Training loss: 1.7115...  0.0584 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9940...  Training loss: 1.7439...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9941...  Training loss: 1.7821...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9942...  Training loss: 1.7276...  0.0545 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9943...  Training loss: 1.7106...  0.0539 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9944...  Training loss: 1.7523...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9945...  Training loss: 1.6862...  0.0614 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9946...  Training loss: 1.7015...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9947...  Training loss: 1.7212...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9948...  Training loss: 1.7617...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9949...  Training loss: 1.7325...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9950...  Training loss: 1.7160...  0.0574 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9951...  Training loss: 1.7214...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9952...  Training loss: 1.7285...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9953...  Training loss: 1.7103...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9954...  Training loss: 1.7187...  0.0539 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9955...  Training loss: 1.7202...  0.0564 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9956...  Training loss: 1.7102...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9957...  Training loss: 1.7553...  0.0594 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9958...  Training loss: 1.7241...  0.0539 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9959...  Training loss: 1.7403...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9960...  Training loss: 1.7149...  0.0539 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9961...  Training loss: 1.7379...  0.0564 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9962...  Training loss: 1.7556...  0.0571 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9963...  Training loss: 1.7344...  0.0570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9964...  Training loss: 1.7513...  0.0524 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9965...  Training loss: 1.7214...  0.0540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9966...  Training loss: 1.7225...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9967...  Training loss: 1.5985...  0.0567 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9968...  Training loss: 1.7217...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9969...  Training loss: 1.6706...  0.0531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9970...  Training loss: 1.7242...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9971...  Training loss: 1.7074...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9972...  Training loss: 1.7014...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9973...  Training loss: 1.7438...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9974...  Training loss: 1.7426...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9975...  Training loss: 1.7407...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9976...  Training loss: 1.7219...  0.0536 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9977...  Training loss: 1.6998...  0.0524 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9978...  Training loss: 1.7190...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9979...  Training loss: 1.7079...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9980...  Training loss: 1.7602...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9981...  Training loss: 1.7325...  0.0561 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9982...  Training loss: 1.6976...  0.0531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9983...  Training loss: 1.7623...  0.0573 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9984...  Training loss: 1.7286...  0.0531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9985...  Training loss: 1.6825...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9986...  Training loss: 1.6746...  0.0540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9987...  Training loss: 1.7004...  0.0589 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9988...  Training loss: 1.6964...  0.0539 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9989...  Training loss: 1.7153...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9990...  Training loss: 1.7343...  0.0555 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9991...  Training loss: 1.7934...  0.0567 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9992...  Training loss: 1.7450...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9993...  Training loss: 1.6700...  0.0587 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9994...  Training loss: 1.7148...  0.0538 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9995...  Training loss: 1.7629...  0.0575 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9996...  Training loss: 1.7660...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9997...  Training loss: 1.7319...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9998...  Training loss: 1.7070...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 9999...  Training loss: 1.7299...  0.0592 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10000...  Training loss: 1.7475...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10001...  Training loss: 1.6701...  0.0582 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10002...  Training loss: 1.7320...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10003...  Training loss: 1.6907...  0.0538 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10004...  Training loss: 1.7118...  0.0534 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 10005...  Training loss: 1.7016...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10006...  Training loss: 1.7677...  0.0521 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10007...  Training loss: 1.6908...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10008...  Training loss: 1.7776...  0.0573 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10009...  Training loss: 1.7222...  0.0573 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10010...  Training loss: 1.7410...  0.0576 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10011...  Training loss: 1.7178...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10012...  Training loss: 1.7785...  0.0562 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10013...  Training loss: 1.7498...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10014...  Training loss: 1.7291...  0.0562 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10015...  Training loss: 1.7036...  0.0527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10016...  Training loss: 1.7633...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10017...  Training loss: 1.7536...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10018...  Training loss: 1.6747...  0.0587 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10019...  Training loss: 1.7584...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10020...  Training loss: 1.6949...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10021...  Training loss: 1.6852...  0.0558 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10022...  Training loss: 1.6952...  0.0527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10023...  Training loss: 1.7543...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10024...  Training loss: 1.8011...  0.0545 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10025...  Training loss: 1.7313...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10026...  Training loss: 1.6904...  0.0536 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10027...  Training loss: 1.7584...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10028...  Training loss: 1.7130...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10029...  Training loss: 1.7186...  0.0562 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10030...  Training loss: 1.6736...  0.0592 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10031...  Training loss: 1.6894...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10032...  Training loss: 1.7118...  0.0593 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10033...  Training loss: 1.6981...  0.0530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10034...  Training loss: 1.6997...  0.0577 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10035...  Training loss: 1.7617...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10036...  Training loss: 1.7525...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10037...  Training loss: 1.6649...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10038...  Training loss: 1.7597...  0.0555 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10039...  Training loss: 1.7072...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10040...  Training loss: 1.7103...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10041...  Training loss: 1.7013...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10042...  Training loss: 1.6813...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10043...  Training loss: 1.7260...  0.0571 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10044...  Training loss: 1.7432...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10045...  Training loss: 1.7498...  0.0567 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10046...  Training loss: 1.7557...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10047...  Training loss: 1.7641...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10048...  Training loss: 1.6858...  0.0555 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10049...  Training loss: 1.7180...  0.0587 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10050...  Training loss: 1.7766...  0.0585 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10051...  Training loss: 1.7138...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10052...  Training loss: 1.7622...  0.0577 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10053...  Training loss: 1.7625...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10054...  Training loss: 1.7394...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10055...  Training loss: 1.7172...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10056...  Training loss: 1.7262...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10057...  Training loss: 1.7433...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10058...  Training loss: 1.7265...  0.0560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10059...  Training loss: 1.7753...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10060...  Training loss: 1.7517...  0.0568 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10061...  Training loss: 1.7905...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10062...  Training loss: 1.6440...  0.0563 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10063...  Training loss: 1.7623...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10064...  Training loss: 1.7069...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10065...  Training loss: 1.6737...  0.0575 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10066...  Training loss: 1.7633...  0.0561 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10067...  Training loss: 1.7337...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10068...  Training loss: 1.7292...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10069...  Training loss: 1.7597...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10070...  Training loss: 1.7652...  0.0558 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10071...  Training loss: 1.7415...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10072...  Training loss: 1.7002...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10073...  Training loss: 1.7329...  0.0574 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10074...  Training loss: 1.7620...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10075...  Training loss: 1.7180...  0.0544 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10076...  Training loss: 1.7477...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10077...  Training loss: 1.7451...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10078...  Training loss: 1.7584...  0.0575 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10079...  Training loss: 1.7201...  0.0555 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10080...  Training loss: 1.6946...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10081...  Training loss: 1.7231...  0.0561 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10082...  Training loss: 1.7035...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10083...  Training loss: 1.7534...  0.0577 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10084...  Training loss: 1.7237...  0.0561 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10085...  Training loss: 1.7520...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10086...  Training loss: 1.7435...  0.0574 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10087...  Training loss: 1.7387...  0.0555 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10088...  Training loss: 1.7229...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10089...  Training loss: 1.7197...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10090...  Training loss: 1.6945...  0.0567 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10091...  Training loss: 1.7221...  0.0581 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10092...  Training loss: 1.7483...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10093...  Training loss: 1.7250...  0.0530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10094...  Training loss: 1.7119...  0.0574 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10095...  Training loss: 1.6974...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10096...  Training loss: 1.7245...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10097...  Training loss: 1.7346...  0.0578 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10098...  Training loss: 1.7098...  0.0524 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10099...  Training loss: 1.7061...  0.0587 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10100...  Training loss: 1.6949...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10101...  Training loss: 1.7082...  0.0591 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10102...  Training loss: 1.7547...  0.0589 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10103...  Training loss: 1.7385...  0.0583 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10104...  Training loss: 1.6978...  0.0572 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 10105...  Training loss: 1.6876...  0.0585 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10106...  Training loss: 1.7311...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10107...  Training loss: 1.7190...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10108...  Training loss: 1.7209...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10109...  Training loss: 1.7274...  0.0578 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10110...  Training loss: 1.7899...  0.0537 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10111...  Training loss: 1.7448...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10112...  Training loss: 1.7718...  0.0545 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10113...  Training loss: 1.7441...  0.0598 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10114...  Training loss: 1.7055...  0.0531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10115...  Training loss: 1.7159...  0.0565 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10116...  Training loss: 1.7848...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10117...  Training loss: 1.7394...  0.0573 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10118...  Training loss: 1.8056...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10119...  Training loss: 1.7007...  0.0531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10120...  Training loss: 1.7489...  0.0537 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10121...  Training loss: 1.6939...  0.0583 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10122...  Training loss: 1.7180...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10123...  Training loss: 1.7284...  0.0588 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10124...  Training loss: 1.6994...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10125...  Training loss: 1.7272...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10126...  Training loss: 1.6998...  0.0566 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10127...  Training loss: 1.7537...  0.0566 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10128...  Training loss: 1.7319...  0.0545 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10129...  Training loss: 1.7237...  0.0570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10130...  Training loss: 1.7071...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10131...  Training loss: 1.7423...  0.0582 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10132...  Training loss: 1.7458...  0.0527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10133...  Training loss: 1.7320...  0.0590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10134...  Training loss: 1.7579...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10135...  Training loss: 1.7626...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10136...  Training loss: 1.7594...  0.0576 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10137...  Training loss: 1.7340...  0.0539 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10138...  Training loss: 1.7178...  0.0570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10139...  Training loss: 1.7934...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10140...  Training loss: 1.7665...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10141...  Training loss: 1.7490...  0.0558 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10142...  Training loss: 1.7634...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10143...  Training loss: 1.7766...  0.0530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10144...  Training loss: 1.6801...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10145...  Training loss: 1.7182...  0.0539 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10146...  Training loss: 1.7520...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10147...  Training loss: 1.7714...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10148...  Training loss: 1.7203...  0.0566 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10149...  Training loss: 1.7518...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10150...  Training loss: 1.6991...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10151...  Training loss: 1.7955...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10152...  Training loss: 1.6993...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10153...  Training loss: 1.7093...  0.0583 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10154...  Training loss: 1.6870...  0.0569 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10155...  Training loss: 1.6935...  0.0598 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10156...  Training loss: 1.7632...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10157...  Training loss: 1.7202...  0.0530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10158...  Training loss: 1.6821...  0.0538 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10159...  Training loss: 1.7139...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10160...  Training loss: 1.7502...  0.0564 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10161...  Training loss: 1.6940...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10162...  Training loss: 1.6788...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10163...  Training loss: 1.6906...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10164...  Training loss: 1.7247...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10165...  Training loss: 1.7180...  0.0543 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10166...  Training loss: 1.7291...  0.0530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10167...  Training loss: 1.7283...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10168...  Training loss: 1.7308...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10169...  Training loss: 1.6928...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10170...  Training loss: 1.6992...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10171...  Training loss: 1.7160...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10172...  Training loss: 1.6857...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10173...  Training loss: 1.7469...  0.0577 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10174...  Training loss: 1.7560...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10175...  Training loss: 1.8030...  0.0563 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10176...  Training loss: 1.7135...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10177...  Training loss: 1.7184...  0.0597 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10178...  Training loss: 1.7137...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10179...  Training loss: 1.7264...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10180...  Training loss: 1.7203...  0.0560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10181...  Training loss: 1.7463...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10182...  Training loss: 1.7076...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10183...  Training loss: 1.7348...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10184...  Training loss: 1.7151...  0.0559 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10185...  Training loss: 1.7429...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10186...  Training loss: 1.6754...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10187...  Training loss: 1.7138...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10188...  Training loss: 1.7354...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10189...  Training loss: 1.7207...  0.0580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10190...  Training loss: 1.6947...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10191...  Training loss: 1.6857...  0.0598 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10192...  Training loss: 1.7224...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10193...  Training loss: 1.6973...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10194...  Training loss: 1.7018...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10195...  Training loss: 1.7648...  0.0541 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10196...  Training loss: 1.7615...  0.0545 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10197...  Training loss: 1.8220...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10198...  Training loss: 1.7674...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10199...  Training loss: 1.7880...  0.0569 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10200...  Training loss: 1.7605...  0.0569 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10201...  Training loss: 1.7563...  0.0539 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10202...  Training loss: 1.6724...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10203...  Training loss: 1.6880...  0.0567 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10204...  Training loss: 1.7378...  0.0536 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 10205...  Training loss: 1.7120...  0.0570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10206...  Training loss: 1.7415...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10207...  Training loss: 1.7120...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10208...  Training loss: 1.7471...  0.0544 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10209...  Training loss: 1.7266...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10210...  Training loss: 1.7672...  0.0576 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10211...  Training loss: 1.7136...  0.0581 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10212...  Training loss: 1.7030...  0.0543 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10213...  Training loss: 1.6861...  0.0569 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10214...  Training loss: 1.7417...  0.0524 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10215...  Training loss: 1.7024...  0.0585 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10216...  Training loss: 1.6571...  0.0559 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10217...  Training loss: 1.7513...  0.0555 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10218...  Training loss: 1.7975...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10219...  Training loss: 1.7492...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10220...  Training loss: 1.7125...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10221...  Training loss: 1.7585...  0.0527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10222...  Training loss: 1.8006...  0.0559 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10223...  Training loss: 1.6761...  0.0597 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10224...  Training loss: 1.7574...  0.0536 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10225...  Training loss: 1.7203...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10226...  Training loss: 1.7101...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10227...  Training loss: 1.7162...  0.0560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10228...  Training loss: 1.7288...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10229...  Training loss: 1.7094...  0.0578 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10230...  Training loss: 1.7057...  0.0568 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10231...  Training loss: 1.6853...  0.0544 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10232...  Training loss: 1.6734...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10233...  Training loss: 1.7139...  0.0527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10234...  Training loss: 1.6668...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10235...  Training loss: 1.7432...  0.0574 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10236...  Training loss: 1.7588...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10237...  Training loss: 1.6892...  0.0564 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10238...  Training loss: 1.6636...  0.0541 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10239...  Training loss: 1.6902...  0.0576 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10240...  Training loss: 1.7672...  0.0582 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10241...  Training loss: 1.7382...  0.0560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10242...  Training loss: 1.6690...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10243...  Training loss: 1.7270...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10244...  Training loss: 1.7256...  0.0523 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10245...  Training loss: 1.6976...  0.0563 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10246...  Training loss: 1.7076...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10247...  Training loss: 1.6708...  0.0531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10248...  Training loss: 1.6769...  0.0543 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10249...  Training loss: 1.7255...  0.0531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10250...  Training loss: 1.6986...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10251...  Training loss: 1.7000...  0.0527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10252...  Training loss: 1.7089...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10253...  Training loss: 1.7064...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10254...  Training loss: 1.7106...  0.0536 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10255...  Training loss: 1.7228...  0.0576 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10256...  Training loss: 1.7115...  0.0567 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10257...  Training loss: 1.6956...  0.0561 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10258...  Training loss: 1.7036...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10259...  Training loss: 1.6985...  0.0560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10260...  Training loss: 1.7004...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10261...  Training loss: 1.7437...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10262...  Training loss: 1.7297...  0.0563 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10263...  Training loss: 1.6895...  0.0538 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10264...  Training loss: 1.6970...  0.0531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10265...  Training loss: 1.7320...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10266...  Training loss: 1.7233...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10267...  Training loss: 1.7263...  0.0583 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10268...  Training loss: 1.7383...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10269...  Training loss: 1.7194...  0.0530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10270...  Training loss: 1.7104...  0.0568 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10271...  Training loss: 1.7502...  0.0576 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10272...  Training loss: 1.7470...  0.0540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10273...  Training loss: 1.7330...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10274...  Training loss: 1.7079...  0.0581 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10275...  Training loss: 1.7047...  0.0543 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10276...  Training loss: 1.7721...  0.0585 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10277...  Training loss: 1.8243...  0.0608 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10278...  Training loss: 1.7757...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10279...  Training loss: 1.7338...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10280...  Training loss: 1.7267...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10281...  Training loss: 1.7687...  0.0530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10282...  Training loss: 1.7063...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10283...  Training loss: 1.6878...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10284...  Training loss: 1.6804...  0.0523 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10285...  Training loss: 1.7107...  0.0536 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10286...  Training loss: 1.7375...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10287...  Training loss: 1.7063...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10288...  Training loss: 1.7689...  0.0582 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10289...  Training loss: 1.7611...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10290...  Training loss: 1.7442...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10291...  Training loss: 1.7210...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10292...  Training loss: 1.7998...  0.0537 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10293...  Training loss: 1.7361...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10294...  Training loss: 1.7366...  0.0617 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10295...  Training loss: 1.7041...  0.0527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10296...  Training loss: 1.7255...  0.0545 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10297...  Training loss: 1.7026...  0.0537 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10298...  Training loss: 1.7882...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10299...  Training loss: 1.7408...  0.0579 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10300...  Training loss: 1.7189...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10301...  Training loss: 1.6717...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10302...  Training loss: 1.7780...  0.0572 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10303...  Training loss: 1.7135...  0.0603 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10304...  Training loss: 1.7218...  0.0572 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 10305...  Training loss: 1.7528...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10306...  Training loss: 1.6434...  0.0539 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10307...  Training loss: 1.6493...  0.0524 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10308...  Training loss: 1.7317...  0.0527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10309...  Training loss: 1.6822...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10310...  Training loss: 1.7070...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10311...  Training loss: 1.7558...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10312...  Training loss: 1.6693...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10313...  Training loss: 1.7061...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10314...  Training loss: 1.7506...  0.0541 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10315...  Training loss: 1.6871...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10316...  Training loss: 1.7492...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10317...  Training loss: 1.6988...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10318...  Training loss: 1.7134...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10319...  Training loss: 1.6853...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10320...  Training loss: 1.7309...  0.0524 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10321...  Training loss: 1.7849...  0.0555 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10322...  Training loss: 1.7408...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10323...  Training loss: 1.6966...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10324...  Training loss: 1.7530...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10325...  Training loss: 1.7841...  0.0538 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10326...  Training loss: 1.7709...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10327...  Training loss: 1.7557...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10328...  Training loss: 1.7284...  0.0536 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10329...  Training loss: 1.7859...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10330...  Training loss: 1.7792...  0.0589 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10331...  Training loss: 1.7015...  0.0588 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10332...  Training loss: 1.7401...  0.0527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10333...  Training loss: 1.7603...  0.0543 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10334...  Training loss: 1.7281...  0.0527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10335...  Training loss: 1.7098...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10336...  Training loss: 1.6823...  0.0569 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10337...  Training loss: 1.7104...  0.0537 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10338...  Training loss: 1.7358...  0.0540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10339...  Training loss: 1.7723...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10340...  Training loss: 1.7235...  0.0531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10341...  Training loss: 1.6996...  0.0561 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10342...  Training loss: 1.7161...  0.0587 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10343...  Training loss: 1.7376...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10344...  Training loss: 1.6832...  0.0540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10345...  Training loss: 1.7365...  0.0566 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10346...  Training loss: 1.7529...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10347...  Training loss: 1.7013...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10348...  Training loss: 1.7701...  0.0539 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10349...  Training loss: 1.7031...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10350...  Training loss: 1.6721...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10351...  Training loss: 1.7107...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10352...  Training loss: 1.7570...  0.0541 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10353...  Training loss: 1.7387...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10354...  Training loss: 1.7124...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10355...  Training loss: 1.6848...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10356...  Training loss: 1.7185...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10357...  Training loss: 1.6802...  0.0567 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10358...  Training loss: 1.7197...  0.0543 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10359...  Training loss: 1.7234...  0.0581 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10360...  Training loss: 1.7165...  0.0570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10361...  Training loss: 1.7054...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10362...  Training loss: 1.7174...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10363...  Training loss: 1.7526...  0.0531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10364...  Training loss: 1.7280...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10365...  Training loss: 1.6205...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10366...  Training loss: 1.7152...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10367...  Training loss: 1.6905...  0.0582 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10368...  Training loss: 1.6828...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10369...  Training loss: 1.7126...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10370...  Training loss: 1.7710...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10371...  Training loss: 1.7578...  0.0523 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10372...  Training loss: 1.7541...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10373...  Training loss: 1.6814...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10374...  Training loss: 1.7255...  0.0578 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10375...  Training loss: 1.6918...  0.0560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10376...  Training loss: 1.7175...  0.0601 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10377...  Training loss: 1.7234...  0.0540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10378...  Training loss: 1.7147...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10379...  Training loss: 1.7077...  0.0563 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10380...  Training loss: 1.7242...  0.0583 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10381...  Training loss: 1.6752...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10382...  Training loss: 1.7298...  0.0570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10383...  Training loss: 1.6751...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10384...  Training loss: 1.7226...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10385...  Training loss: 1.7529...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10386...  Training loss: 1.7423...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10387...  Training loss: 1.7022...  0.0576 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10388...  Training loss: 1.7290...  0.0541 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10389...  Training loss: 1.7298...  0.0559 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10390...  Training loss: 1.7219...  0.0593 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10391...  Training loss: 1.7289...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10392...  Training loss: 1.7589...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10393...  Training loss: 1.7108...  0.0562 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10394...  Training loss: 1.6749...  0.0568 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10395...  Training loss: 1.7905...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10396...  Training loss: 1.7567...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10397...  Training loss: 1.7143...  0.0571 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10398...  Training loss: 1.7926...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10399...  Training loss: 1.7014...  0.0578 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10400...  Training loss: 1.7964...  0.0542 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10401...  Training loss: 1.7336...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10402...  Training loss: 1.6963...  0.0545 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10403...  Training loss: 1.7343...  0.0543 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10404...  Training loss: 1.7439...  0.0532 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 10405...  Training loss: 1.8127...  0.0527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10406...  Training loss: 1.7137...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10407...  Training loss: 1.7266...  0.0609 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10408...  Training loss: 1.7369...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10409...  Training loss: 1.7517...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10410...  Training loss: 1.7221...  0.0536 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10411...  Training loss: 1.7148...  0.0566 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10412...  Training loss: 1.7330...  0.0569 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10413...  Training loss: 1.7441...  0.0578 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10414...  Training loss: 1.7111...  0.0581 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10415...  Training loss: 1.6931...  0.0565 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10416...  Training loss: 1.7181...  0.0589 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10417...  Training loss: 1.7212...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10418...  Training loss: 1.7672...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10419...  Training loss: 1.6645...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10420...  Training loss: 1.7378...  0.0540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10421...  Training loss: 1.7275...  0.0558 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10422...  Training loss: 1.7448...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10423...  Training loss: 1.8072...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10424...  Training loss: 1.7245...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10425...  Training loss: 1.6956...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10426...  Training loss: 1.6704...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10427...  Training loss: 1.7676...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10428...  Training loss: 1.7130...  0.0579 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10429...  Training loss: 1.7421...  0.0530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10430...  Training loss: 1.7524...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10431...  Training loss: 1.7791...  0.0536 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10432...  Training loss: 1.7640...  0.0530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10433...  Training loss: 1.7615...  0.0605 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10434...  Training loss: 1.7841...  0.0544 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10435...  Training loss: 1.7347...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10436...  Training loss: 1.7245...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10437...  Training loss: 1.7061...  0.0566 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10438...  Training loss: 1.7056...  0.0558 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10439...  Training loss: 1.7100...  0.0541 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10440...  Training loss: 1.7180...  0.0536 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10441...  Training loss: 1.7152...  0.0540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10442...  Training loss: 1.6736...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10443...  Training loss: 1.7058...  0.0593 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10444...  Training loss: 1.6655...  0.0540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10445...  Training loss: 1.7413...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10446...  Training loss: 1.7752...  0.0544 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10447...  Training loss: 1.7530...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10448...  Training loss: 1.7423...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10449...  Training loss: 1.7248...  0.0582 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10450...  Training loss: 1.7011...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10451...  Training loss: 1.6968...  0.0581 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10452...  Training loss: 1.7196...  0.0547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10453...  Training loss: 1.7108...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10454...  Training loss: 1.7130...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10455...  Training loss: 1.7556...  0.0559 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10456...  Training loss: 1.7254...  0.0563 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10457...  Training loss: 1.6892...  0.0563 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10458...  Training loss: 1.7099...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10459...  Training loss: 1.6340...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10460...  Training loss: 1.7256...  0.0538 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10461...  Training loss: 1.7285...  0.0560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10462...  Training loss: 1.6967...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10463...  Training loss: 1.7043...  0.0586 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10464...  Training loss: 1.6918...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10465...  Training loss: 1.7638...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10466...  Training loss: 1.7282...  0.0590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10467...  Training loss: 1.7466...  0.0586 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10468...  Training loss: 1.7726...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10469...  Training loss: 1.7500...  0.0558 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10470...  Training loss: 1.7013...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10471...  Training loss: 1.7493...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10472...  Training loss: 1.6713...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10473...  Training loss: 1.7307...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10474...  Training loss: 1.7321...  0.0563 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10475...  Training loss: 1.7038...  0.0552 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10476...  Training loss: 1.7330...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10477...  Training loss: 1.7127...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10478...  Training loss: 1.6624...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10479...  Training loss: 1.7224...  0.0595 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10480...  Training loss: 1.7032...  0.0581 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10481...  Training loss: 1.7812...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10482...  Training loss: 1.7416...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10483...  Training loss: 1.7249...  0.0579 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10484...  Training loss: 1.7829...  0.0558 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10485...  Training loss: 1.8207...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10486...  Training loss: 1.7875...  0.0528 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10487...  Training loss: 1.7471...  0.0537 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10488...  Training loss: 1.7867...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10489...  Training loss: 1.7107...  0.0591 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10490...  Training loss: 1.7409...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10491...  Training loss: 1.7446...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10492...  Training loss: 1.7684...  0.0587 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10493...  Training loss: 1.7481...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10494...  Training loss: 1.7134...  0.0555 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10495...  Training loss: 1.7283...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10496...  Training loss: 1.7451...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10497...  Training loss: 1.6927...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10498...  Training loss: 1.7627...  0.0527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10499...  Training loss: 1.7660...  0.0553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10500...  Training loss: 1.7503...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10501...  Training loss: 1.7844...  0.0562 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10502...  Training loss: 1.7503...  0.0541 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10503...  Training loss: 1.7259...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10504...  Training loss: 1.7773...  0.0527 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 10505...  Training loss: 1.7587...  0.0544 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10506...  Training loss: 1.7235...  0.0551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10507...  Training loss: 1.6964...  0.0595 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10508...  Training loss: 1.7290...  0.0589 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10509...  Training loss: 1.7104...  0.0585 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10510...  Training loss: 1.7624...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10511...  Training loss: 1.6898...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10512...  Training loss: 1.7805...  0.0589 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10513...  Training loss: 1.7302...  0.0529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10514...  Training loss: 1.6689...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10515...  Training loss: 1.7038...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10516...  Training loss: 1.6926...  0.0581 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10517...  Training loss: 1.6858...  0.0568 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10518...  Training loss: 1.7299...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10519...  Training loss: 1.7440...  0.0532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10520...  Training loss: 1.7565...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10521...  Training loss: 1.7105...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10522...  Training loss: 1.7276...  0.0533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10523...  Training loss: 1.7202...  0.0569 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10524...  Training loss: 1.6985...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10525...  Training loss: 1.7237...  0.0565 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10526...  Training loss: 1.7201...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10527...  Training loss: 1.6728...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10528...  Training loss: 1.7001...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10529...  Training loss: 1.7181...  0.0570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10530...  Training loss: 1.7783...  0.0568 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10531...  Training loss: 1.7796...  0.0531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10532...  Training loss: 1.7371...  0.0537 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10533...  Training loss: 1.6673...  0.0535 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10534...  Training loss: 1.7375...  0.0580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10535...  Training loss: 1.6808...  0.0577 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10536...  Training loss: 1.7435...  0.0580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10537...  Training loss: 1.7642...  0.0538 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10538...  Training loss: 1.6926...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10539...  Training loss: 1.6802...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 10540...  Training loss: 1.6849...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10541...  Training loss: 1.8147...  0.0572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10542...  Training loss: 1.7914...  0.0523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10543...  Training loss: 1.7674...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10544...  Training loss: 1.7088...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10545...  Training loss: 1.7372...  0.0557 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10546...  Training loss: 1.7487...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10547...  Training loss: 1.6874...  0.0534 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10548...  Training loss: 1.6916...  0.0544 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10549...  Training loss: 1.6610...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10550...  Training loss: 1.7013...  0.0554 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10551...  Training loss: 1.7100...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10552...  Training loss: 1.6651...  0.0617 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10553...  Training loss: 1.7246...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10554...  Training loss: 1.6884...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10555...  Training loss: 1.7389...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10556...  Training loss: 1.7608...  0.0543 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10557...  Training loss: 1.7504...  0.0592 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10558...  Training loss: 1.7170...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10559...  Training loss: 1.6945...  0.0535 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10560...  Training loss: 1.7449...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10561...  Training loss: 1.7863...  0.0564 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10562...  Training loss: 1.7231...  0.0554 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10563...  Training loss: 1.7027...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10564...  Training loss: 1.7216...  0.0575 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10565...  Training loss: 1.7116...  0.0559 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10566...  Training loss: 1.6562...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10567...  Training loss: 1.7021...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10568...  Training loss: 1.7149...  0.0578 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10569...  Training loss: 1.7322...  0.0537 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10570...  Training loss: 1.6921...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10571...  Training loss: 1.6868...  0.0565 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10572...  Training loss: 1.7524...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10573...  Training loss: 1.7230...  0.0533 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10574...  Training loss: 1.7129...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10575...  Training loss: 1.6870...  0.0536 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10576...  Training loss: 1.6966...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10577...  Training loss: 1.7236...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10578...  Training loss: 1.7440...  0.0522 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10579...  Training loss: 1.7482...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10580...  Training loss: 1.6990...  0.0577 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10581...  Training loss: 1.7215...  0.0533 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10582...  Training loss: 1.7517...  0.0583 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10583...  Training loss: 1.7305...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10584...  Training loss: 1.7432...  0.0571 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10585...  Training loss: 1.6847...  0.0534 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10586...  Training loss: 1.7017...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10587...  Training loss: 1.5982...  0.0581 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10588...  Training loss: 1.7170...  0.0603 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10589...  Training loss: 1.6582...  0.0578 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10590...  Training loss: 1.7269...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10591...  Training loss: 1.6763...  0.0567 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10592...  Training loss: 1.6865...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10593...  Training loss: 1.7124...  0.0536 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10594...  Training loss: 1.7260...  0.0557 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10595...  Training loss: 1.7436...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10596...  Training loss: 1.7170...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10597...  Training loss: 1.6949...  0.0574 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10598...  Training loss: 1.6992...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10599...  Training loss: 1.7012...  0.0544 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10600...  Training loss: 1.7530...  0.0554 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10601...  Training loss: 1.7206...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10602...  Training loss: 1.6910...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10603...  Training loss: 1.7461...  0.0591 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10604...  Training loss: 1.7083...  0.0538 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 10605...  Training loss: 1.6706...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10606...  Training loss: 1.6594...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10607...  Training loss: 1.6952...  0.0618 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10608...  Training loss: 1.7120...  0.0581 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10609...  Training loss: 1.7071...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10610...  Training loss: 1.7314...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10611...  Training loss: 1.7858...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10612...  Training loss: 1.7418...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10613...  Training loss: 1.6439...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10614...  Training loss: 1.7075...  0.0559 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10615...  Training loss: 1.7519...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10616...  Training loss: 1.7478...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10617...  Training loss: 1.7221...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10618...  Training loss: 1.6905...  0.0545 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10619...  Training loss: 1.7219...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10620...  Training loss: 1.7576...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10621...  Training loss: 1.6663...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10622...  Training loss: 1.7027...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10623...  Training loss: 1.6923...  0.0543 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10624...  Training loss: 1.6906...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10625...  Training loss: 1.6894...  0.0564 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10626...  Training loss: 1.7638...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10627...  Training loss: 1.6745...  0.0521 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10628...  Training loss: 1.7465...  0.0560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10629...  Training loss: 1.6958...  0.0589 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10630...  Training loss: 1.7131...  0.0523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10631...  Training loss: 1.6972...  0.0555 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10632...  Training loss: 1.7703...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10633...  Training loss: 1.7048...  0.0543 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10634...  Training loss: 1.7216...  0.0583 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10635...  Training loss: 1.7088...  0.0534 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10636...  Training loss: 1.7580...  0.0578 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10637...  Training loss: 1.7397...  0.0555 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10638...  Training loss: 1.6657...  0.0534 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10639...  Training loss: 1.7452...  0.0570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10640...  Training loss: 1.7092...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10641...  Training loss: 1.6888...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10642...  Training loss: 1.6646...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10643...  Training loss: 1.7259...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10644...  Training loss: 1.7650...  0.0561 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10645...  Training loss: 1.7266...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10646...  Training loss: 1.6838...  0.0542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10647...  Training loss: 1.7453...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10648...  Training loss: 1.7321...  0.0543 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10649...  Training loss: 1.7137...  0.0544 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10650...  Training loss: 1.6754...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10651...  Training loss: 1.6562...  0.0569 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10652...  Training loss: 1.6809...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10653...  Training loss: 1.7067...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10654...  Training loss: 1.6849...  0.0543 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10655...  Training loss: 1.7323...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10656...  Training loss: 1.7433...  0.0542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10657...  Training loss: 1.6892...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10658...  Training loss: 1.7560...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10659...  Training loss: 1.7029...  0.0521 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10660...  Training loss: 1.7112...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10661...  Training loss: 1.6776...  0.0534 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10662...  Training loss: 1.6720...  0.0565 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10663...  Training loss: 1.7134...  0.0578 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10664...  Training loss: 1.7401...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10665...  Training loss: 1.7337...  0.0591 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10666...  Training loss: 1.7489...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10667...  Training loss: 1.7643...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10668...  Training loss: 1.6822...  0.0578 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10669...  Training loss: 1.7059...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10670...  Training loss: 1.7716...  0.0557 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10671...  Training loss: 1.7211...  0.0565 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10672...  Training loss: 1.7657...  0.0545 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10673...  Training loss: 1.7585...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10674...  Training loss: 1.7224...  0.0519 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10675...  Training loss: 1.7006...  0.0585 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10676...  Training loss: 1.6920...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10677...  Training loss: 1.7137...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10678...  Training loss: 1.7186...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10679...  Training loss: 1.7572...  0.0556 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10680...  Training loss: 1.7400...  0.0591 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10681...  Training loss: 1.7772...  0.0572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10682...  Training loss: 1.6454...  0.0555 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10683...  Training loss: 1.7469...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10684...  Training loss: 1.6978...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10685...  Training loss: 1.6623...  0.0561 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10686...  Training loss: 1.7555...  0.0523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10687...  Training loss: 1.7457...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10688...  Training loss: 1.7484...  0.0572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10689...  Training loss: 1.7294...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10690...  Training loss: 1.7425...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10691...  Training loss: 1.7457...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10692...  Training loss: 1.7166...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10693...  Training loss: 1.7141...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10694...  Training loss: 1.7849...  0.0521 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10695...  Training loss: 1.7014...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10696...  Training loss: 1.6977...  0.0540 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10697...  Training loss: 1.7271...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10698...  Training loss: 1.7370...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10699...  Training loss: 1.7245...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10700...  Training loss: 1.6941...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10701...  Training loss: 1.6776...  0.0556 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10702...  Training loss: 1.6959...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10703...  Training loss: 1.7324...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10704...  Training loss: 1.7371...  0.0531 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 10705...  Training loss: 1.7358...  0.0520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10706...  Training loss: 1.7202...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10707...  Training loss: 1.7131...  0.0570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10708...  Training loss: 1.7156...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10709...  Training loss: 1.7144...  0.0583 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10710...  Training loss: 1.7042...  0.0522 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10711...  Training loss: 1.6984...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10712...  Training loss: 1.7298...  0.0523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10713...  Training loss: 1.6901...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10714...  Training loss: 1.6951...  0.0584 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10715...  Training loss: 1.6742...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10716...  Training loss: 1.7102...  0.0620 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10717...  Training loss: 1.7050...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10718...  Training loss: 1.6860...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10719...  Training loss: 1.6844...  0.0523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10720...  Training loss: 1.6902...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10721...  Training loss: 1.6797...  0.0543 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10722...  Training loss: 1.7691...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10723...  Training loss: 1.7585...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10724...  Training loss: 1.6753...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10725...  Training loss: 1.6722...  0.0586 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10726...  Training loss: 1.7155...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10727...  Training loss: 1.7242...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10728...  Training loss: 1.6940...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10729...  Training loss: 1.7209...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10730...  Training loss: 1.7831...  0.0537 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10731...  Training loss: 1.7144...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10732...  Training loss: 1.7639...  0.0544 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10733...  Training loss: 1.7260...  0.0536 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10734...  Training loss: 1.7057...  0.0587 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10735...  Training loss: 1.6877...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10736...  Training loss: 1.7686...  0.0521 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10737...  Training loss: 1.7466...  0.0544 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10738...  Training loss: 1.7914...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10739...  Training loss: 1.7042...  0.0555 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10740...  Training loss: 1.7461...  0.0587 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10741...  Training loss: 1.6815...  0.0567 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10742...  Training loss: 1.7101...  0.0556 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10743...  Training loss: 1.6847...  0.0561 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10744...  Training loss: 1.6808...  0.0561 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10745...  Training loss: 1.7181...  0.0557 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10746...  Training loss: 1.7049...  0.0566 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10747...  Training loss: 1.7425...  0.0566 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10748...  Training loss: 1.7271...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10749...  Training loss: 1.7060...  0.0571 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10750...  Training loss: 1.6989...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10751...  Training loss: 1.7158...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10752...  Training loss: 1.7278...  0.0520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10753...  Training loss: 1.7360...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10754...  Training loss: 1.7480...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10755...  Training loss: 1.7511...  0.0576 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10756...  Training loss: 1.7380...  0.0574 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10757...  Training loss: 1.7465...  0.0567 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10758...  Training loss: 1.7062...  0.0541 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10759...  Training loss: 1.8046...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10760...  Training loss: 1.7520...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10761...  Training loss: 1.7645...  0.0518 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10762...  Training loss: 1.7548...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10763...  Training loss: 1.7502...  0.0574 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10764...  Training loss: 1.6806...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10765...  Training loss: 1.7205...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10766...  Training loss: 1.7516...  0.0542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10767...  Training loss: 1.7677...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10768...  Training loss: 1.6982...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10769...  Training loss: 1.7266...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10770...  Training loss: 1.7001...  0.0521 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10771...  Training loss: 1.7713...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10772...  Training loss: 1.6895...  0.0522 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10773...  Training loss: 1.6809...  0.0577 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10774...  Training loss: 1.7005...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10775...  Training loss: 1.6778...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10776...  Training loss: 1.7672...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10777...  Training loss: 1.7153...  0.0523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10778...  Training loss: 1.6889...  0.0541 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10779...  Training loss: 1.6844...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10780...  Training loss: 1.7318...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10781...  Training loss: 1.6816...  0.0545 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10782...  Training loss: 1.6910...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10783...  Training loss: 1.6798...  0.0576 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10784...  Training loss: 1.7155...  0.0583 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10785...  Training loss: 1.7254...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10786...  Training loss: 1.6925...  0.0522 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10787...  Training loss: 1.7326...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10788...  Training loss: 1.7451...  0.0544 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10789...  Training loss: 1.6794...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10790...  Training loss: 1.6886...  0.0586 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10791...  Training loss: 1.7083...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10792...  Training loss: 1.6890...  0.0523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10793...  Training loss: 1.7274...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10794...  Training loss: 1.7258...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10795...  Training loss: 1.7726...  0.0541 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10796...  Training loss: 1.7067...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10797...  Training loss: 1.7192...  0.0562 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10798...  Training loss: 1.7043...  0.0587 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10799...  Training loss: 1.7220...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10800...  Training loss: 1.7252...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10801...  Training loss: 1.7257...  0.0572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10802...  Training loss: 1.6764...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10803...  Training loss: 1.7284...  0.0557 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10804...  Training loss: 1.7298...  0.0535 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 10805...  Training loss: 1.7280...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10806...  Training loss: 1.6680...  0.0576 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10807...  Training loss: 1.6972...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10808...  Training loss: 1.7202...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10809...  Training loss: 1.7004...  0.0558 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10810...  Training loss: 1.6618...  0.0555 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10811...  Training loss: 1.6847...  0.0522 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10812...  Training loss: 1.7270...  0.0573 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10813...  Training loss: 1.6990...  0.0572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10814...  Training loss: 1.6806...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10815...  Training loss: 1.7465...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10816...  Training loss: 1.7532...  0.0542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10817...  Training loss: 1.8046...  0.0558 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10818...  Training loss: 1.7501...  0.0561 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10819...  Training loss: 1.7654...  0.0591 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10820...  Training loss: 1.7539...  0.0521 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10821...  Training loss: 1.7377...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10822...  Training loss: 1.6599...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10823...  Training loss: 1.6693...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10824...  Training loss: 1.7531...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10825...  Training loss: 1.7042...  0.0539 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10826...  Training loss: 1.7446...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10827...  Training loss: 1.7050...  0.0581 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10828...  Training loss: 1.7362...  0.0579 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10829...  Training loss: 1.7168...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10830...  Training loss: 1.7545...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10831...  Training loss: 1.7127...  0.0568 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10832...  Training loss: 1.7147...  0.0545 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10833...  Training loss: 1.6947...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10834...  Training loss: 1.7593...  0.0607 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10835...  Training loss: 1.7069...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10836...  Training loss: 1.6516...  0.0522 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10837...  Training loss: 1.7224...  0.0586 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10838...  Training loss: 1.7757...  0.0574 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10839...  Training loss: 1.7346...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10840...  Training loss: 1.6984...  0.0588 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10841...  Training loss: 1.7480...  0.0581 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10842...  Training loss: 1.7862...  0.0583 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10843...  Training loss: 1.6693...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10844...  Training loss: 1.7515...  0.0565 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10845...  Training loss: 1.6947...  0.0540 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10846...  Training loss: 1.7093...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10847...  Training loss: 1.7173...  0.0542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10848...  Training loss: 1.7140...  0.0544 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10849...  Training loss: 1.7047...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10850...  Training loss: 1.7035...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10851...  Training loss: 1.6815...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10852...  Training loss: 1.6795...  0.0570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10853...  Training loss: 1.6826...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10854...  Training loss: 1.6715...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10855...  Training loss: 1.7331...  0.0566 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10856...  Training loss: 1.7494...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10857...  Training loss: 1.6896...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10858...  Training loss: 1.6516...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10859...  Training loss: 1.6680...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10860...  Training loss: 1.7552...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10861...  Training loss: 1.7107...  0.0566 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10862...  Training loss: 1.6685...  0.0621 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10863...  Training loss: 1.7331...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10864...  Training loss: 1.7112...  0.0575 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10865...  Training loss: 1.7008...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10866...  Training loss: 1.7015...  0.0545 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10867...  Training loss: 1.6923...  0.0540 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10868...  Training loss: 1.6890...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10869...  Training loss: 1.7181...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10870...  Training loss: 1.7018...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10871...  Training loss: 1.6703...  0.0559 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10872...  Training loss: 1.7033...  0.0585 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10873...  Training loss: 1.6909...  0.0569 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10874...  Training loss: 1.6848...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10875...  Training loss: 1.6998...  0.0545 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10876...  Training loss: 1.7222...  0.0556 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10877...  Training loss: 1.6791...  0.0522 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10878...  Training loss: 1.6856...  0.0558 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10879...  Training loss: 1.6746...  0.0583 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10880...  Training loss: 1.6942...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10881...  Training loss: 1.7230...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10882...  Training loss: 1.6852...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10883...  Training loss: 1.6825...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10884...  Training loss: 1.6714...  0.0569 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10885...  Training loss: 1.7031...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10886...  Training loss: 1.7100...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10887...  Training loss: 1.7220...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10888...  Training loss: 1.7355...  0.0523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10889...  Training loss: 1.6935...  0.0534 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10890...  Training loss: 1.7064...  0.0554 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10891...  Training loss: 1.7632...  0.0589 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10892...  Training loss: 1.7166...  0.0577 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10893...  Training loss: 1.7249...  0.0554 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10894...  Training loss: 1.6995...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10895...  Training loss: 1.6821...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10896...  Training loss: 1.7668...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10897...  Training loss: 1.8063...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10898...  Training loss: 1.7499...  0.0565 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10899...  Training loss: 1.7164...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10900...  Training loss: 1.7187...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10901...  Training loss: 1.7423...  0.0521 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10902...  Training loss: 1.6999...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10903...  Training loss: 1.6729...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10904...  Training loss: 1.6727...  0.0570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 10905...  Training loss: 1.7006...  0.0556 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10906...  Training loss: 1.7369...  0.0572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10907...  Training loss: 1.7029...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10908...  Training loss: 1.7235...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10909...  Training loss: 1.7350...  0.0571 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10910...  Training loss: 1.7165...  0.0521 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10911...  Training loss: 1.6905...  0.0569 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10912...  Training loss: 1.7798...  0.0594 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10913...  Training loss: 1.7153...  0.0597 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10914...  Training loss: 1.7158...  0.0543 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10915...  Training loss: 1.6984...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10916...  Training loss: 1.7055...  0.0533 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10917...  Training loss: 1.6734...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10918...  Training loss: 1.7826...  0.0534 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10919...  Training loss: 1.7474...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10920...  Training loss: 1.6926...  0.0533 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10921...  Training loss: 1.6415...  0.0581 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10922...  Training loss: 1.7730...  0.0541 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10923...  Training loss: 1.6992...  0.0567 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10924...  Training loss: 1.7046...  0.0523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10925...  Training loss: 1.7246...  0.0558 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10926...  Training loss: 1.6334...  0.0542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10927...  Training loss: 1.6660...  0.0593 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10928...  Training loss: 1.7353...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10929...  Training loss: 1.6900...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10930...  Training loss: 1.6619...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10931...  Training loss: 1.7532...  0.0522 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10932...  Training loss: 1.6607...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10933...  Training loss: 1.6973...  0.0538 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10934...  Training loss: 1.7389...  0.0564 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10935...  Training loss: 1.6894...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10936...  Training loss: 1.7203...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10937...  Training loss: 1.6733...  0.0541 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10938...  Training loss: 1.7370...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10939...  Training loss: 1.6878...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10940...  Training loss: 1.7456...  0.0574 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10941...  Training loss: 1.7568...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10942...  Training loss: 1.7183...  0.0616 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10943...  Training loss: 1.6746...  0.0534 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10944...  Training loss: 1.7239...  0.0543 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10945...  Training loss: 1.7600...  0.0581 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10946...  Training loss: 1.7530...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10947...  Training loss: 1.7609...  0.0544 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10948...  Training loss: 1.7250...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10949...  Training loss: 1.7772...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10950...  Training loss: 1.7476...  0.0542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10951...  Training loss: 1.7063...  0.0542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10952...  Training loss: 1.7341...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10953...  Training loss: 1.7377...  0.0541 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10954...  Training loss: 1.7165...  0.0578 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10955...  Training loss: 1.6871...  0.0522 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10956...  Training loss: 1.6748...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10957...  Training loss: 1.6912...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10958...  Training loss: 1.7247...  0.0576 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10959...  Training loss: 1.7726...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10960...  Training loss: 1.7084...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10961...  Training loss: 1.6957...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10962...  Training loss: 1.6964...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10963...  Training loss: 1.7139...  0.0584 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10964...  Training loss: 1.6965...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10965...  Training loss: 1.7516...  0.0579 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10966...  Training loss: 1.7448...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10967...  Training loss: 1.6742...  0.0542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10968...  Training loss: 1.7413...  0.0545 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10969...  Training loss: 1.7012...  0.0584 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10970...  Training loss: 1.6696...  0.0579 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10971...  Training loss: 1.7014...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10972...  Training loss: 1.7607...  0.0535 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10973...  Training loss: 1.7266...  0.0560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10974...  Training loss: 1.7025...  0.0541 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10975...  Training loss: 1.6691...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10976...  Training loss: 1.6946...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10977...  Training loss: 1.6894...  0.0542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10978...  Training loss: 1.7062...  0.0535 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10979...  Training loss: 1.6882...  0.0593 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10980...  Training loss: 1.7041...  0.0562 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10981...  Training loss: 1.7094...  0.0607 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10982...  Training loss: 1.7118...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10983...  Training loss: 1.7584...  0.0544 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10984...  Training loss: 1.7043...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10985...  Training loss: 1.6315...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10986...  Training loss: 1.7144...  0.0557 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10987...  Training loss: 1.6745...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10988...  Training loss: 1.6480...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10989...  Training loss: 1.7090...  0.0600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10990...  Training loss: 1.7517...  0.0537 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10991...  Training loss: 1.7530...  0.0545 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10992...  Training loss: 1.7353...  0.0582 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10993...  Training loss: 1.6677...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10994...  Training loss: 1.7283...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10995...  Training loss: 1.6798...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10996...  Training loss: 1.6996...  0.0523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10997...  Training loss: 1.7182...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10998...  Training loss: 1.7183...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 10999...  Training loss: 1.6994...  0.0567 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11000...  Training loss: 1.7092...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11001...  Training loss: 1.6728...  0.0587 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11002...  Training loss: 1.7128...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11003...  Training loss: 1.6951...  0.0564 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11004...  Training loss: 1.7026...  0.0527 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 11005...  Training loss: 1.7479...  0.0595 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11006...  Training loss: 1.7453...  0.0542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11007...  Training loss: 1.6910...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11008...  Training loss: 1.7133...  0.0523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11009...  Training loss: 1.7010...  0.0572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11010...  Training loss: 1.6823...  0.0565 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11011...  Training loss: 1.7001...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11012...  Training loss: 1.7270...  0.0563 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11013...  Training loss: 1.7185...  0.0577 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11014...  Training loss: 1.6766...  0.0533 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11015...  Training loss: 1.7623...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11016...  Training loss: 1.7598...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11017...  Training loss: 1.7273...  0.0558 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11018...  Training loss: 1.7864...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11019...  Training loss: 1.6898...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11020...  Training loss: 1.7749...  0.0533 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11021...  Training loss: 1.7060...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11022...  Training loss: 1.6935...  0.0571 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11023...  Training loss: 1.7210...  0.0570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11024...  Training loss: 1.7170...  0.0537 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11025...  Training loss: 1.8067...  0.0554 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11026...  Training loss: 1.6960...  0.0539 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11027...  Training loss: 1.7313...  0.0579 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11028...  Training loss: 1.7262...  0.0520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11029...  Training loss: 1.7251...  0.0584 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11030...  Training loss: 1.7080...  0.0585 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11031...  Training loss: 1.7172...  0.0557 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11032...  Training loss: 1.7290...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11033...  Training loss: 1.7198...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11034...  Training loss: 1.7065...  0.0556 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11035...  Training loss: 1.6681...  0.0575 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11036...  Training loss: 1.6986...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11037...  Training loss: 1.6959...  0.0540 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11038...  Training loss: 1.7854...  0.0582 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11039...  Training loss: 1.6535...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11040...  Training loss: 1.7138...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11041...  Training loss: 1.7132...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11042...  Training loss: 1.7223...  0.0572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11043...  Training loss: 1.8028...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11044...  Training loss: 1.7120...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11045...  Training loss: 1.6810...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11046...  Training loss: 1.6547...  0.0556 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11047...  Training loss: 1.7361...  0.0555 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11048...  Training loss: 1.7018...  0.0600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11049...  Training loss: 1.7592...  0.0556 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11050...  Training loss: 1.7320...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11051...  Training loss: 1.7447...  0.0565 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11052...  Training loss: 1.7448...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11053...  Training loss: 1.7701...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11054...  Training loss: 1.7626...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11055...  Training loss: 1.7298...  0.0548 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11056...  Training loss: 1.7054...  0.0563 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11057...  Training loss: 1.7116...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11058...  Training loss: 1.6883...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11059...  Training loss: 1.6854...  0.0578 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11060...  Training loss: 1.7086...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11061...  Training loss: 1.6966...  0.0576 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11062...  Training loss: 1.6715...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11063...  Training loss: 1.7268...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11064...  Training loss: 1.6386...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11065...  Training loss: 1.7379...  0.0562 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11066...  Training loss: 1.7518...  0.0574 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11067...  Training loss: 1.7412...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11068...  Training loss: 1.7248...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11069...  Training loss: 1.7183...  0.0560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11070...  Training loss: 1.6872...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11071...  Training loss: 1.6858...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11072...  Training loss: 1.7158...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11073...  Training loss: 1.7001...  0.0540 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11074...  Training loss: 1.7062...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11075...  Training loss: 1.7234...  0.0541 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11076...  Training loss: 1.7177...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11077...  Training loss: 1.6845...  0.0522 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11078...  Training loss: 1.7062...  0.0544 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11079...  Training loss: 1.6434...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11080...  Training loss: 1.7292...  0.0554 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11081...  Training loss: 1.7498...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11082...  Training loss: 1.6847...  0.0557 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11083...  Training loss: 1.7007...  0.0554 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11084...  Training loss: 1.6774...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11085...  Training loss: 1.7380...  0.0608 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11086...  Training loss: 1.7077...  0.0545 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11087...  Training loss: 1.7511...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11088...  Training loss: 1.7755...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11089...  Training loss: 1.7594...  0.0578 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11090...  Training loss: 1.6629...  0.0555 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11091...  Training loss: 1.7359...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11092...  Training loss: 1.6690...  0.0544 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11093...  Training loss: 1.7294...  0.0555 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11094...  Training loss: 1.6917...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11095...  Training loss: 1.6869...  0.0520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11096...  Training loss: 1.7174...  0.0574 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11097...  Training loss: 1.7145...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11098...  Training loss: 1.6626...  0.0537 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11099...  Training loss: 1.7237...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11100...  Training loss: 1.6983...  0.0545 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11101...  Training loss: 1.7414...  0.0572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11102...  Training loss: 1.7378...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11103...  Training loss: 1.6974...  0.0574 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11104...  Training loss: 1.7753...  0.0525 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 11105...  Training loss: 1.7999...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11106...  Training loss: 1.7678...  0.0585 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11107...  Training loss: 1.7264...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11108...  Training loss: 1.7784...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11109...  Training loss: 1.7116...  0.0560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11110...  Training loss: 1.7326...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11111...  Training loss: 1.7408...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11112...  Training loss: 1.7696...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11113...  Training loss: 1.7032...  0.0545 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11114...  Training loss: 1.7204...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11115...  Training loss: 1.7376...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11116...  Training loss: 1.7464...  0.0544 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11117...  Training loss: 1.6533...  0.0582 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11118...  Training loss: 1.7061...  0.0554 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11119...  Training loss: 1.7732...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11120...  Training loss: 1.7486...  0.0575 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11121...  Training loss: 1.7624...  0.0560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11122...  Training loss: 1.7283...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11123...  Training loss: 1.7208...  0.0579 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11124...  Training loss: 1.7477...  0.0542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11125...  Training loss: 1.7537...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11126...  Training loss: 1.7247...  0.0579 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11127...  Training loss: 1.6619...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11128...  Training loss: 1.7349...  0.0565 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11129...  Training loss: 1.7022...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11130...  Training loss: 1.7606...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11131...  Training loss: 1.6791...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11132...  Training loss: 1.7472...  0.0578 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11133...  Training loss: 1.7179...  0.0547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11134...  Training loss: 1.7192...  0.0541 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11135...  Training loss: 1.7104...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11136...  Training loss: 1.6756...  0.0583 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11137...  Training loss: 1.6544...  0.0551 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11138...  Training loss: 1.7162...  0.0556 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11139...  Training loss: 1.7421...  0.0534 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11140...  Training loss: 1.7302...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11141...  Training loss: 1.6925...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11142...  Training loss: 1.7147...  0.0523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11143...  Training loss: 1.7163...  0.0573 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11144...  Training loss: 1.6747...  0.0567 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11145...  Training loss: 1.7277...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11146...  Training loss: 1.7176...  0.0563 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11147...  Training loss: 1.6609...  0.0540 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11148...  Training loss: 1.6787...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11149...  Training loss: 1.7115...  0.0524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11150...  Training loss: 1.7570...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11151...  Training loss: 1.7684...  0.0540 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11152...  Training loss: 1.7257...  0.0549 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11153...  Training loss: 1.6746...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11154...  Training loss: 1.7163...  0.0581 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11155...  Training loss: 1.6579...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11156...  Training loss: 1.7254...  0.0603 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11157...  Training loss: 1.7661...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11158...  Training loss: 1.7038...  0.0525 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11159...  Training loss: 1.6732...  0.0529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 11160...  Training loss: 1.6655...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11161...  Training loss: 1.7863...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11162...  Training loss: 1.7836...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11163...  Training loss: 1.7613...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11164...  Training loss: 1.6923...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11165...  Training loss: 1.7062...  0.0576 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11166...  Training loss: 1.7467...  0.0576 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11167...  Training loss: 1.6815...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11168...  Training loss: 1.6708...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11169...  Training loss: 1.6423...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11170...  Training loss: 1.6692...  0.0533 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11171...  Training loss: 1.6934...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11172...  Training loss: 1.6707...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11173...  Training loss: 1.7175...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11174...  Training loss: 1.6852...  0.0592 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11175...  Training loss: 1.7595...  0.0615 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11176...  Training loss: 1.7569...  0.0555 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11177...  Training loss: 1.7306...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11178...  Training loss: 1.7073...  0.0589 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11179...  Training loss: 1.6859...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11180...  Training loss: 1.7268...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11181...  Training loss: 1.7626...  0.0557 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11182...  Training loss: 1.7152...  0.0614 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11183...  Training loss: 1.6991...  0.0535 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11184...  Training loss: 1.7246...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11185...  Training loss: 1.7061...  0.0559 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11186...  Training loss: 1.6618...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11187...  Training loss: 1.7159...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11188...  Training loss: 1.7220...  0.0693 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11189...  Training loss: 1.7264...  0.0539 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11190...  Training loss: 1.6689...  0.0524 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11191...  Training loss: 1.6797...  0.0636 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11192...  Training loss: 1.7258...  0.0606 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11193...  Training loss: 1.7069...  0.0589 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11194...  Training loss: 1.6838...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11195...  Training loss: 1.7092...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11196...  Training loss: 1.7162...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11197...  Training loss: 1.7135...  0.0581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11198...  Training loss: 1.7148...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11199...  Training loss: 1.7305...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11200...  Training loss: 1.6888...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11201...  Training loss: 1.7147...  0.0537 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11202...  Training loss: 1.7214...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11203...  Training loss: 1.7105...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11204...  Training loss: 1.7293...  0.0536 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 11205...  Training loss: 1.6926...  0.0591 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11206...  Training loss: 1.6981...  0.0538 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11207...  Training loss: 1.5846...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11208...  Training loss: 1.6924...  0.0543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11209...  Training loss: 1.6621...  0.0524 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11210...  Training loss: 1.7202...  0.0576 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11211...  Training loss: 1.6920...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11212...  Training loss: 1.6715...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11213...  Training loss: 1.6975...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11214...  Training loss: 1.7190...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11215...  Training loss: 1.7276...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11216...  Training loss: 1.7313...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11217...  Training loss: 1.6755...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11218...  Training loss: 1.7062...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11219...  Training loss: 1.6971...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11220...  Training loss: 1.7712...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11221...  Training loss: 1.6940...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11222...  Training loss: 1.6771...  0.0521 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11223...  Training loss: 1.7228...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11224...  Training loss: 1.7127...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11225...  Training loss: 1.6635...  0.0536 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11226...  Training loss: 1.6274...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11227...  Training loss: 1.6948...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11228...  Training loss: 1.6809...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11229...  Training loss: 1.6955...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11230...  Training loss: 1.7121...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11231...  Training loss: 1.7610...  0.0581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11232...  Training loss: 1.7132...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11233...  Training loss: 1.6618...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11234...  Training loss: 1.6832...  0.0556 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11235...  Training loss: 1.7528...  0.0538 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11236...  Training loss: 1.7301...  0.0555 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11237...  Training loss: 1.7128...  0.0573 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11238...  Training loss: 1.6989...  0.0524 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11239...  Training loss: 1.7264...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11240...  Training loss: 1.7278...  0.0573 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11241...  Training loss: 1.6492...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11242...  Training loss: 1.7108...  0.0573 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11243...  Training loss: 1.6686...  0.0584 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11244...  Training loss: 1.6730...  0.0519 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11245...  Training loss: 1.6760...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11246...  Training loss: 1.7433...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11247...  Training loss: 1.6726...  0.0558 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11248...  Training loss: 1.7646...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11249...  Training loss: 1.7105...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11250...  Training loss: 1.7196...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11251...  Training loss: 1.6639...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11252...  Training loss: 1.7444...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11253...  Training loss: 1.7165...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11254...  Training loss: 1.7130...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11255...  Training loss: 1.6970...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11256...  Training loss: 1.7528...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11257...  Training loss: 1.7220...  0.0580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11258...  Training loss: 1.6375...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11259...  Training loss: 1.7391...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11260...  Training loss: 1.6738...  0.0580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11261...  Training loss: 1.6879...  0.0593 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11262...  Training loss: 1.6732...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11263...  Training loss: 1.7543...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11264...  Training loss: 1.7462...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11265...  Training loss: 1.7250...  0.0557 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11266...  Training loss: 1.6579...  0.0568 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11267...  Training loss: 1.7373...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11268...  Training loss: 1.6991...  0.0566 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11269...  Training loss: 1.7133...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11270...  Training loss: 1.6365...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11271...  Training loss: 1.6717...  0.0559 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11272...  Training loss: 1.7081...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11273...  Training loss: 1.6948...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11274...  Training loss: 1.6833...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11275...  Training loss: 1.7503...  0.0520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11276...  Training loss: 1.7460...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11277...  Training loss: 1.6725...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11278...  Training loss: 1.7128...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11279...  Training loss: 1.6919...  0.0533 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11280...  Training loss: 1.6862...  0.0575 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11281...  Training loss: 1.6684...  0.0539 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11282...  Training loss: 1.6519...  0.0543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11283...  Training loss: 1.6908...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11284...  Training loss: 1.7287...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11285...  Training loss: 1.7374...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11286...  Training loss: 1.7423...  0.0584 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11287...  Training loss: 1.7623...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11288...  Training loss: 1.6803...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11289...  Training loss: 1.6995...  0.0574 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11290...  Training loss: 1.7674...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11291...  Training loss: 1.7116...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11292...  Training loss: 1.7633...  0.0573 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11293...  Training loss: 1.7356...  0.0614 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11294...  Training loss: 1.7023...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11295...  Training loss: 1.6836...  0.0540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11296...  Training loss: 1.7093...  0.0576 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11297...  Training loss: 1.6982...  0.0580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11298...  Training loss: 1.7102...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11299...  Training loss: 1.7597...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11300...  Training loss: 1.7152...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11301...  Training loss: 1.7442...  0.0592 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11302...  Training loss: 1.6340...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11303...  Training loss: 1.7040...  0.0577 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11304...  Training loss: 1.6810...  0.0528 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 11305...  Training loss: 1.6593...  0.0522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11306...  Training loss: 1.7233...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11307...  Training loss: 1.7419...  0.0559 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11308...  Training loss: 1.6856...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11309...  Training loss: 1.7360...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11310...  Training loss: 1.7373...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11311...  Training loss: 1.7303...  0.0568 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11312...  Training loss: 1.6911...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11313...  Training loss: 1.7152...  0.0583 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11314...  Training loss: 1.7475...  0.0586 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11315...  Training loss: 1.7144...  0.0556 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11316...  Training loss: 1.7179...  0.0540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11317...  Training loss: 1.7340...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11318...  Training loss: 1.7223...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11319...  Training loss: 1.7036...  0.0580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11320...  Training loss: 1.6622...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11321...  Training loss: 1.6724...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11322...  Training loss: 1.6884...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11323...  Training loss: 1.7467...  0.0568 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11324...  Training loss: 1.7183...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11325...  Training loss: 1.7200...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11326...  Training loss: 1.7147...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11327...  Training loss: 1.7246...  0.0564 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11328...  Training loss: 1.6997...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11329...  Training loss: 1.6971...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11330...  Training loss: 1.6771...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11331...  Training loss: 1.6911...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11332...  Training loss: 1.7039...  0.0543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11333...  Training loss: 1.7001...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11334...  Training loss: 1.6782...  0.0574 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11335...  Training loss: 1.7071...  0.0533 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11336...  Training loss: 1.6920...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11337...  Training loss: 1.7169...  0.0540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11338...  Training loss: 1.6848...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11339...  Training loss: 1.7098...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11340...  Training loss: 1.6748...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11341...  Training loss: 1.6810...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11342...  Training loss: 1.7250...  0.0543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11343...  Training loss: 1.7343...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11344...  Training loss: 1.6871...  0.0583 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11345...  Training loss: 1.6759...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11346...  Training loss: 1.6932...  0.0575 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11347...  Training loss: 1.7160...  0.0555 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11348...  Training loss: 1.6968...  0.0554 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11349...  Training loss: 1.6943...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11350...  Training loss: 1.7598...  0.0541 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11351...  Training loss: 1.7197...  0.0583 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11352...  Training loss: 1.7543...  0.0564 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11353...  Training loss: 1.7155...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11354...  Training loss: 1.7028...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11355...  Training loss: 1.6979...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11356...  Training loss: 1.7809...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11357...  Training loss: 1.7060...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11358...  Training loss: 1.7659...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11359...  Training loss: 1.6972...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11360...  Training loss: 1.7322...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11361...  Training loss: 1.6735...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11362...  Training loss: 1.7194...  0.0533 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11363...  Training loss: 1.6962...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11364...  Training loss: 1.6977...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11365...  Training loss: 1.6966...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11366...  Training loss: 1.7024...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11367...  Training loss: 1.7343...  0.0588 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11368...  Training loss: 1.7250...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11369...  Training loss: 1.6750...  0.0558 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11370...  Training loss: 1.6836...  0.0556 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11371...  Training loss: 1.7079...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11372...  Training loss: 1.7284...  0.0579 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11373...  Training loss: 1.7270...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11374...  Training loss: 1.7305...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11375...  Training loss: 1.7429...  0.0575 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11376...  Training loss: 1.7336...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11377...  Training loss: 1.7315...  0.0540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11378...  Training loss: 1.6920...  0.0522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11379...  Training loss: 1.7712...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11380...  Training loss: 1.7709...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11381...  Training loss: 1.7287...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11382...  Training loss: 1.7457...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11383...  Training loss: 1.7697...  0.0538 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11384...  Training loss: 1.6674...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11385...  Training loss: 1.6827...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11386...  Training loss: 1.7505...  0.0579 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11387...  Training loss: 1.7508...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11388...  Training loss: 1.6821...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11389...  Training loss: 1.7288...  0.0571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11390...  Training loss: 1.6929...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11391...  Training loss: 1.7784...  0.0580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11392...  Training loss: 1.6830...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11393...  Training loss: 1.6687...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11394...  Training loss: 1.7098...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11395...  Training loss: 1.6827...  0.0585 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11396...  Training loss: 1.7331...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11397...  Training loss: 1.6959...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11398...  Training loss: 1.6657...  0.0519 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11399...  Training loss: 1.6928...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11400...  Training loss: 1.6947...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11401...  Training loss: 1.6626...  0.0558 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11402...  Training loss: 1.6789...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11403...  Training loss: 1.6730...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11404...  Training loss: 1.7074...  0.0550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 11405...  Training loss: 1.7026...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11406...  Training loss: 1.7090...  0.0521 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11407...  Training loss: 1.7323...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11408...  Training loss: 1.7253...  0.0524 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11409...  Training loss: 1.6566...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11410...  Training loss: 1.6850...  0.0522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11411...  Training loss: 1.7119...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11412...  Training loss: 1.6754...  0.0522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11413...  Training loss: 1.7180...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11414...  Training loss: 1.7077...  0.0573 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11415...  Training loss: 1.7474...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11416...  Training loss: 1.6854...  0.0584 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11417...  Training loss: 1.7141...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11418...  Training loss: 1.7173...  0.0534 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11419...  Training loss: 1.7088...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11420...  Training loss: 1.6948...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11421...  Training loss: 1.7106...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11422...  Training loss: 1.6713...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11423...  Training loss: 1.7041...  0.0522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11424...  Training loss: 1.7089...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11425...  Training loss: 1.7302...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11426...  Training loss: 1.6524...  0.0524 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11427...  Training loss: 1.7106...  0.0559 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11428...  Training loss: 1.7047...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11429...  Training loss: 1.6970...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11430...  Training loss: 1.6638...  0.0578 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11431...  Training loss: 1.6777...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11432...  Training loss: 1.6991...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11433...  Training loss: 1.6806...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11434...  Training loss: 1.6842...  0.0520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11435...  Training loss: 1.7294...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11436...  Training loss: 1.7273...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11437...  Training loss: 1.8056...  0.0573 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11438...  Training loss: 1.7264...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11439...  Training loss: 1.7638...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11440...  Training loss: 1.7572...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11441...  Training loss: 1.7278...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11442...  Training loss: 1.6615...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11443...  Training loss: 1.6840...  0.0519 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11444...  Training loss: 1.7342...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11445...  Training loss: 1.7046...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11446...  Training loss: 1.7316...  0.0587 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11447...  Training loss: 1.7063...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11448...  Training loss: 1.7226...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11449...  Training loss: 1.6936...  0.0543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11450...  Training loss: 1.7561...  0.0567 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11451...  Training loss: 1.7009...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11452...  Training loss: 1.6945...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11453...  Training loss: 1.6758...  0.0567 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11454...  Training loss: 1.7242...  0.0580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11455...  Training loss: 1.6932...  0.0561 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11456...  Training loss: 1.6560...  0.0581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11457...  Training loss: 1.7216...  0.0543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11458...  Training loss: 1.7566...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11459...  Training loss: 1.7254...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11460...  Training loss: 1.6739...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11461...  Training loss: 1.7547...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11462...  Training loss: 1.7825...  0.0560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11463...  Training loss: 1.6657...  0.0522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11464...  Training loss: 1.7510...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11465...  Training loss: 1.6902...  0.0583 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11466...  Training loss: 1.6948...  0.0537 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11467...  Training loss: 1.7242...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11468...  Training loss: 1.7127...  0.0578 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11469...  Training loss: 1.6796...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11470...  Training loss: 1.6823...  0.0522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11471...  Training loss: 1.6806...  0.0533 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11472...  Training loss: 1.6659...  0.0570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11473...  Training loss: 1.6873...  0.0565 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11474...  Training loss: 1.6474...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11475...  Training loss: 1.7060...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11476...  Training loss: 1.7354...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11477...  Training loss: 1.6775...  0.0543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11478...  Training loss: 1.6448...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11479...  Training loss: 1.6918...  0.0561 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11480...  Training loss: 1.7547...  0.0582 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11481...  Training loss: 1.6877...  0.0580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11482...  Training loss: 1.6383...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11483...  Training loss: 1.6985...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11484...  Training loss: 1.6890...  0.0520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11485...  Training loss: 1.6958...  0.0534 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11486...  Training loss: 1.6996...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11487...  Training loss: 1.6605...  0.0533 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11488...  Training loss: 1.6734...  0.0560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11489...  Training loss: 1.7233...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11490...  Training loss: 1.6774...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11491...  Training loss: 1.7030...  0.0609 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11492...  Training loss: 1.7007...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11493...  Training loss: 1.7029...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11494...  Training loss: 1.6956...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11495...  Training loss: 1.6964...  0.0524 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11496...  Training loss: 1.7098...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11497...  Training loss: 1.6552...  0.0543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11498...  Training loss: 1.6771...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11499...  Training loss: 1.6665...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11500...  Training loss: 1.7085...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11501...  Training loss: 1.6982...  0.0522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11502...  Training loss: 1.7037...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11503...  Training loss: 1.6687...  0.0555 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11504...  Training loss: 1.6901...  0.0543 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 11505...  Training loss: 1.6839...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11506...  Training loss: 1.6995...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11507...  Training loss: 1.7332...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11508...  Training loss: 1.7318...  0.0577 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11509...  Training loss: 1.7007...  0.0593 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11510...  Training loss: 1.6871...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11511...  Training loss: 1.7459...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11512...  Training loss: 1.7036...  0.0574 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11513...  Training loss: 1.7014...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11514...  Training loss: 1.7016...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11515...  Training loss: 1.6718...  0.0558 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11516...  Training loss: 1.7745...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11517...  Training loss: 1.8095...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11518...  Training loss: 1.7297...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11519...  Training loss: 1.7128...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11520...  Training loss: 1.6889...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11521...  Training loss: 1.7410...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11522...  Training loss: 1.6661...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11523...  Training loss: 1.6550...  0.0575 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11524...  Training loss: 1.6637...  0.0570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11525...  Training loss: 1.7105...  0.0582 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11526...  Training loss: 1.7135...  0.0522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11527...  Training loss: 1.6843...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11528...  Training loss: 1.7411...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11529...  Training loss: 1.7202...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11530...  Training loss: 1.7038...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11531...  Training loss: 1.6969...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11532...  Training loss: 1.7893...  0.0535 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11533...  Training loss: 1.7082...  0.0582 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11534...  Training loss: 1.7069...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11535...  Training loss: 1.6933...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11536...  Training loss: 1.6943...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11537...  Training loss: 1.6799...  0.0566 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11538...  Training loss: 1.7536...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11539...  Training loss: 1.7442...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11540...  Training loss: 1.6934...  0.0537 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11541...  Training loss: 1.6466...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11542...  Training loss: 1.7604...  0.0543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11543...  Training loss: 1.6655...  0.0625 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11544...  Training loss: 1.7131...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11545...  Training loss: 1.7037...  0.0595 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11546...  Training loss: 1.6265...  0.0574 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11547...  Training loss: 1.6341...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11548...  Training loss: 1.7120...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11549...  Training loss: 1.6644...  0.0533 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11550...  Training loss: 1.6836...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11551...  Training loss: 1.7290...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11552...  Training loss: 1.6692...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11553...  Training loss: 1.7021...  0.0543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11554...  Training loss: 1.7363...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11555...  Training loss: 1.6661...  0.0577 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11556...  Training loss: 1.6952...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11557...  Training loss: 1.6944...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11558...  Training loss: 1.7124...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11559...  Training loss: 1.6874...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11560...  Training loss: 1.7345...  0.0557 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11561...  Training loss: 1.7473...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11562...  Training loss: 1.6800...  0.0522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11563...  Training loss: 1.6479...  0.0621 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11564...  Training loss: 1.7285...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11565...  Training loss: 1.7469...  0.0561 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11566...  Training loss: 1.7463...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11567...  Training loss: 1.7736...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11568...  Training loss: 1.7087...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11569...  Training loss: 1.7579...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11570...  Training loss: 1.7511...  0.0524 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11571...  Training loss: 1.6891...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11572...  Training loss: 1.7144...  0.0591 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11573...  Training loss: 1.7327...  0.0575 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11574...  Training loss: 1.7059...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11575...  Training loss: 1.6826...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11576...  Training loss: 1.6577...  0.0534 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11577...  Training loss: 1.6920...  0.0521 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11578...  Training loss: 1.7209...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11579...  Training loss: 1.7374...  0.0567 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11580...  Training loss: 1.6976...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11581...  Training loss: 1.6651...  0.0563 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11582...  Training loss: 1.6801...  0.0559 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11583...  Training loss: 1.7269...  0.0561 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11584...  Training loss: 1.6731...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11585...  Training loss: 1.7392...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11586...  Training loss: 1.7321...  0.0577 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11587...  Training loss: 1.6748...  0.0585 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11588...  Training loss: 1.7304...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11589...  Training loss: 1.6898...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11590...  Training loss: 1.6502...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11591...  Training loss: 1.7038...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11592...  Training loss: 1.7546...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11593...  Training loss: 1.7214...  0.0555 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11594...  Training loss: 1.6763...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11595...  Training loss: 1.6754...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11596...  Training loss: 1.6859...  0.0565 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11597...  Training loss: 1.6578...  0.0574 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11598...  Training loss: 1.6695...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11599...  Training loss: 1.6795...  0.0559 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11600...  Training loss: 1.6670...  0.0587 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11601...  Training loss: 1.6827...  0.0581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11602...  Training loss: 1.7047...  0.0541 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11603...  Training loss: 1.7176...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11604...  Training loss: 1.7229...  0.0531 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 11605...  Training loss: 1.6041...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11606...  Training loss: 1.6996...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11607...  Training loss: 1.6616...  0.0569 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11608...  Training loss: 1.6541...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11609...  Training loss: 1.7148...  0.0539 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11610...  Training loss: 1.7419...  0.0521 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11611...  Training loss: 1.7303...  0.0572 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11612...  Training loss: 1.7414...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11613...  Training loss: 1.6569...  0.0536 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11614...  Training loss: 1.7108...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11615...  Training loss: 1.6784...  0.0587 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11616...  Training loss: 1.7032...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11617...  Training loss: 1.7004...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11618...  Training loss: 1.6797...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11619...  Training loss: 1.6865...  0.0522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11620...  Training loss: 1.6864...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11621...  Training loss: 1.6488...  0.0558 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11622...  Training loss: 1.6911...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11623...  Training loss: 1.6617...  0.0541 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11624...  Training loss: 1.7051...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11625...  Training loss: 1.7273...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11626...  Training loss: 1.7246...  0.0540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11627...  Training loss: 1.6890...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11628...  Training loss: 1.7188...  0.0538 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11629...  Training loss: 1.7067...  0.0579 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11630...  Training loss: 1.6951...  0.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11631...  Training loss: 1.7179...  0.0555 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11632...  Training loss: 1.7091...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11633...  Training loss: 1.6923...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11634...  Training loss: 1.6413...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11635...  Training loss: 1.7521...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11636...  Training loss: 1.7232...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11637...  Training loss: 1.7182...  0.0577 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11638...  Training loss: 1.7766...  0.0575 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11639...  Training loss: 1.6659...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11640...  Training loss: 1.7628...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11641...  Training loss: 1.7125...  0.0571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11642...  Training loss: 1.6869...  0.0570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11643...  Training loss: 1.7363...  0.0536 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11644...  Training loss: 1.7216...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11645...  Training loss: 1.7810...  0.0560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11646...  Training loss: 1.6903...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11647...  Training loss: 1.7381...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11648...  Training loss: 1.7079...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11649...  Training loss: 1.7133...  0.0566 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11650...  Training loss: 1.7130...  0.0524 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11651...  Training loss: 1.7081...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11652...  Training loss: 1.7254...  0.0571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11653...  Training loss: 1.6983...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11654...  Training loss: 1.6768...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11655...  Training loss: 1.6600...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11656...  Training loss: 1.7025...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11657...  Training loss: 1.6840...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11658...  Training loss: 1.7564...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11659...  Training loss: 1.6328...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11660...  Training loss: 1.7146...  0.0549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11661...  Training loss: 1.6867...  0.0540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11662...  Training loss: 1.7144...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11663...  Training loss: 1.7757...  0.0576 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11664...  Training loss: 1.7221...  0.0539 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11665...  Training loss: 1.6718...  0.0558 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11666...  Training loss: 1.6638...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11667...  Training loss: 1.7304...  0.0591 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11668...  Training loss: 1.6870...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11669...  Training loss: 1.7461...  0.0582 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11670...  Training loss: 1.7085...  0.0573 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11671...  Training loss: 1.7481...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11672...  Training loss: 1.7535...  0.0564 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11673...  Training loss: 1.7526...  0.0584 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11674...  Training loss: 1.7491...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11675...  Training loss: 1.7131...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11676...  Training loss: 1.7004...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11677...  Training loss: 1.6836...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11678...  Training loss: 1.6866...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11679...  Training loss: 1.6764...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11680...  Training loss: 1.6939...  0.0575 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11681...  Training loss: 1.7129...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11682...  Training loss: 1.6715...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11683...  Training loss: 1.7037...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11684...  Training loss: 1.6347...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11685...  Training loss: 1.7209...  0.0597 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11686...  Training loss: 1.7799...  0.0582 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11687...  Training loss: 1.7112...  0.0548 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11688...  Training loss: 1.7181...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11689...  Training loss: 1.7078...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11690...  Training loss: 1.6821...  0.0581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11691...  Training loss: 1.6776...  0.0534 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11692...  Training loss: 1.6827...  0.0589 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11693...  Training loss: 1.6870...  0.0590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11694...  Training loss: 1.7008...  0.0582 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11695...  Training loss: 1.7141...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11696...  Training loss: 1.7123...  0.0540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11697...  Training loss: 1.6446...  0.0558 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11698...  Training loss: 1.6750...  0.0564 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11699...  Training loss: 1.6181...  0.0541 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11700...  Training loss: 1.7104...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11701...  Training loss: 1.7168...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11702...  Training loss: 1.6936...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11703...  Training loss: 1.6843...  0.0608 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11704...  Training loss: 1.6641...  0.0579 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 11705...  Training loss: 1.7203...  0.0533 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11706...  Training loss: 1.7075...  0.0567 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11707...  Training loss: 1.7413...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11708...  Training loss: 1.7554...  0.0577 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11709...  Training loss: 1.7582...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11710...  Training loss: 1.6738...  0.0581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11711...  Training loss: 1.7217...  0.0565 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11712...  Training loss: 1.6589...  0.0576 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11713...  Training loss: 1.7187...  0.0564 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11714...  Training loss: 1.7187...  0.0580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11715...  Training loss: 1.6904...  0.0521 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11716...  Training loss: 1.6998...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11717...  Training loss: 1.6820...  0.0584 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11718...  Training loss: 1.6603...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11719...  Training loss: 1.7029...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11720...  Training loss: 1.6656...  0.0624 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11721...  Training loss: 1.7621...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11722...  Training loss: 1.7067...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11723...  Training loss: 1.6960...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11724...  Training loss: 1.7637...  0.0539 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11725...  Training loss: 1.8060...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11726...  Training loss: 1.7639...  0.0578 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11727...  Training loss: 1.7027...  0.0564 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11728...  Training loss: 1.7637...  0.0571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11729...  Training loss: 1.6806...  0.0524 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11730...  Training loss: 1.7217...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11731...  Training loss: 1.7206...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11732...  Training loss: 1.7716...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11733...  Training loss: 1.7028...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11734...  Training loss: 1.6988...  0.0606 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11735...  Training loss: 1.7145...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11736...  Training loss: 1.7433...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11737...  Training loss: 1.6605...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11738...  Training loss: 1.7135...  0.0520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11739...  Training loss: 1.7433...  0.0545 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11740...  Training loss: 1.7281...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11741...  Training loss: 1.7368...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11742...  Training loss: 1.6997...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11743...  Training loss: 1.7160...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11744...  Training loss: 1.7506...  0.0586 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11745...  Training loss: 1.7442...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11746...  Training loss: 1.7381...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11747...  Training loss: 1.6681...  0.0562 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11748...  Training loss: 1.7139...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11749...  Training loss: 1.6971...  0.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11750...  Training loss: 1.7375...  0.0537 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11751...  Training loss: 1.6825...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11752...  Training loss: 1.7592...  0.0564 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11753...  Training loss: 1.7225...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11754...  Training loss: 1.6856...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11755...  Training loss: 1.6886...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11756...  Training loss: 1.6667...  0.0524 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11757...  Training loss: 1.6692...  0.0600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11758...  Training loss: 1.7035...  0.0543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11759...  Training loss: 1.7243...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11760...  Training loss: 1.7269...  0.0527 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11761...  Training loss: 1.6720...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11762...  Training loss: 1.6866...  0.0554 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11763...  Training loss: 1.6878...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11764...  Training loss: 1.6838...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11765...  Training loss: 1.6893...  0.0533 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11766...  Training loss: 1.7194...  0.0544 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11767...  Training loss: 1.6707...  0.0546 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11768...  Training loss: 1.6681...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11769...  Training loss: 1.6868...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11770...  Training loss: 1.7385...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11771...  Training loss: 1.7724...  0.0539 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11772...  Training loss: 1.6954...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11773...  Training loss: 1.6382...  0.0523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11774...  Training loss: 1.7159...  0.0532 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11775...  Training loss: 1.6484...  0.0625 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11776...  Training loss: 1.7174...  0.0584 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11777...  Training loss: 1.7394...  0.0553 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11778...  Training loss: 1.6819...  0.0556 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11779...  Training loss: 1.6656...  0.0533 sec/batch\n",
      "Epoch: 19/20...  Training Step: 11780...  Training loss: 1.6610...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11781...  Training loss: 1.7724...  0.0579 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11782...  Training loss: 1.7656...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11783...  Training loss: 1.7636...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11784...  Training loss: 1.6728...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11785...  Training loss: 1.6997...  0.0537 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11786...  Training loss: 1.7201...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11787...  Training loss: 1.6862...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11788...  Training loss: 1.6463...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11789...  Training loss: 1.6416...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11790...  Training loss: 1.6680...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11791...  Training loss: 1.7121...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11792...  Training loss: 1.6696...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11793...  Training loss: 1.7136...  0.0556 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11794...  Training loss: 1.6564...  0.0575 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11795...  Training loss: 1.7255...  0.0521 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11796...  Training loss: 1.7465...  0.0577 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11797...  Training loss: 1.7343...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11798...  Training loss: 1.6957...  0.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11799...  Training loss: 1.6696...  0.0532 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11800...  Training loss: 1.7134...  0.0578 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11801...  Training loss: 1.7534...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11802...  Training loss: 1.6894...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11803...  Training loss: 1.6731...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11804...  Training loss: 1.7090...  0.0545 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 11805...  Training loss: 1.6725...  0.0531 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11806...  Training loss: 1.6607...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11807...  Training loss: 1.6728...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11808...  Training loss: 1.7285...  0.0554 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11809...  Training loss: 1.7119...  0.0565 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11810...  Training loss: 1.6665...  0.0558 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11811...  Training loss: 1.6702...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11812...  Training loss: 1.7139...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11813...  Training loss: 1.6958...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11814...  Training loss: 1.6761...  0.0584 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11815...  Training loss: 1.6808...  0.0560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11816...  Training loss: 1.7087...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11817...  Training loss: 1.7131...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11818...  Training loss: 1.7172...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11819...  Training loss: 1.7361...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11820...  Training loss: 1.6715...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11821...  Training loss: 1.7041...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11822...  Training loss: 1.7281...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11823...  Training loss: 1.7150...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11824...  Training loss: 1.7346...  0.0612 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11825...  Training loss: 1.6969...  0.0578 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11826...  Training loss: 1.6876...  0.0573 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11827...  Training loss: 1.5924...  0.0580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11828...  Training loss: 1.7064...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11829...  Training loss: 1.6611...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11830...  Training loss: 1.7163...  0.0604 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11831...  Training loss: 1.6707...  0.0572 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11832...  Training loss: 1.6767...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11833...  Training loss: 1.6925...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11834...  Training loss: 1.7175...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11835...  Training loss: 1.6995...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11836...  Training loss: 1.7081...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11837...  Training loss: 1.6693...  0.0562 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11838...  Training loss: 1.7094...  0.0531 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11839...  Training loss: 1.6887...  0.0536 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11840...  Training loss: 1.7360...  0.0566 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11841...  Training loss: 1.6812...  0.0597 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11842...  Training loss: 1.6752...  0.0571 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11843...  Training loss: 1.7299...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11844...  Training loss: 1.7026...  0.0550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11845...  Training loss: 1.6586...  0.0580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11846...  Training loss: 1.6222...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11847...  Training loss: 1.6644...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11848...  Training loss: 1.6784...  0.0532 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11849...  Training loss: 1.6930...  0.0571 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11850...  Training loss: 1.6978...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11851...  Training loss: 1.7596...  0.0522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11852...  Training loss: 1.6985...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11853...  Training loss: 1.6312...  0.0589 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11854...  Training loss: 1.6715...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11855...  Training loss: 1.7388...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11856...  Training loss: 1.7239...  0.0562 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11857...  Training loss: 1.7197...  0.0557 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11858...  Training loss: 1.6595...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11859...  Training loss: 1.6972...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11860...  Training loss: 1.7365...  0.0585 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11861...  Training loss: 1.6064...  0.0559 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11862...  Training loss: 1.6835...  0.0519 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11863...  Training loss: 1.6585...  0.0536 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11864...  Training loss: 1.6888...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11865...  Training loss: 1.6885...  0.0535 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11866...  Training loss: 1.7329...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11867...  Training loss: 1.6622...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11868...  Training loss: 1.7370...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11869...  Training loss: 1.6973...  0.0565 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11870...  Training loss: 1.7133...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11871...  Training loss: 1.6499...  0.0547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11872...  Training loss: 1.7546...  0.0535 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11873...  Training loss: 1.6930...  0.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11874...  Training loss: 1.7208...  0.0565 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11875...  Training loss: 1.6988...  0.0547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11876...  Training loss: 1.7161...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11877...  Training loss: 1.7146...  0.0587 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11878...  Training loss: 1.6353...  0.0563 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11879...  Training loss: 1.7216...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11880...  Training loss: 1.6886...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11881...  Training loss: 1.6784...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11882...  Training loss: 1.6654...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11883...  Training loss: 1.7326...  0.0554 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11884...  Training loss: 1.7540...  0.0518 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11885...  Training loss: 1.7168...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11886...  Training loss: 1.6505...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11887...  Training loss: 1.7294...  0.0521 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11888...  Training loss: 1.7190...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11889...  Training loss: 1.7028...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11890...  Training loss: 1.6637...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11891...  Training loss: 1.6531...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11892...  Training loss: 1.6809...  0.0552 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11893...  Training loss: 1.6744...  0.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11894...  Training loss: 1.6764...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11895...  Training loss: 1.7321...  0.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11896...  Training loss: 1.7441...  0.0578 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11897...  Training loss: 1.6627...  0.0531 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11898...  Training loss: 1.7351...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11899...  Training loss: 1.7061...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11900...  Training loss: 1.6930...  0.0523 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11901...  Training loss: 1.6593...  0.0553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11902...  Training loss: 1.6561...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11903...  Training loss: 1.6939...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11904...  Training loss: 1.7104...  0.0554 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 11905...  Training loss: 1.7250...  0.0565 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11906...  Training loss: 1.7386...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11907...  Training loss: 1.7410...  0.0585 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11908...  Training loss: 1.6836...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11909...  Training loss: 1.7034...  0.0580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11910...  Training loss: 1.7405...  0.0531 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11911...  Training loss: 1.7159...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11912...  Training loss: 1.7367...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11913...  Training loss: 1.7584...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11914...  Training loss: 1.7043...  0.0578 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11915...  Training loss: 1.6647...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11916...  Training loss: 1.6928...  0.0522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11917...  Training loss: 1.7108...  0.0554 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11918...  Training loss: 1.7000...  0.0557 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11919...  Training loss: 1.7342...  0.0578 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11920...  Training loss: 1.7303...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11921...  Training loss: 1.7344...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11922...  Training loss: 1.6122...  0.0574 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11923...  Training loss: 1.7212...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11924...  Training loss: 1.6896...  0.0565 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11925...  Training loss: 1.6242...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11926...  Training loss: 1.7467...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11927...  Training loss: 1.7262...  0.0521 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11928...  Training loss: 1.7142...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11929...  Training loss: 1.7063...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11930...  Training loss: 1.7384...  0.0554 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11931...  Training loss: 1.7279...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11932...  Training loss: 1.6826...  0.0623 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11933...  Training loss: 1.7067...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11934...  Training loss: 1.7569...  0.0560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11935...  Training loss: 1.6788...  0.0587 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11936...  Training loss: 1.7067...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11937...  Training loss: 1.6982...  0.0588 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11938...  Training loss: 1.7157...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11939...  Training loss: 1.6928...  0.0561 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11940...  Training loss: 1.6660...  0.0583 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11941...  Training loss: 1.6799...  0.0539 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11942...  Training loss: 1.6668...  0.0555 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11943...  Training loss: 1.7246...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11944...  Training loss: 1.6840...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11945...  Training loss: 1.7255...  0.0534 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11946...  Training loss: 1.7077...  0.0582 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11947...  Training loss: 1.6939...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11948...  Training loss: 1.6829...  0.0550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11949...  Training loss: 1.6862...  0.0590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11950...  Training loss: 1.6643...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11951...  Training loss: 1.6785...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11952...  Training loss: 1.7086...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11953...  Training loss: 1.6955...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11954...  Training loss: 1.6673...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11955...  Training loss: 1.6727...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11956...  Training loss: 1.7019...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11957...  Training loss: 1.6966...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11958...  Training loss: 1.6639...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11959...  Training loss: 1.6561...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11960...  Training loss: 1.6498...  0.0522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11961...  Training loss: 1.6635...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11962...  Training loss: 1.7366...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11963...  Training loss: 1.7184...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11964...  Training loss: 1.6661...  0.0555 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11965...  Training loss: 1.6556...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11966...  Training loss: 1.7013...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11967...  Training loss: 1.7274...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11968...  Training loss: 1.6698...  0.0579 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11969...  Training loss: 1.7075...  0.0569 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11970...  Training loss: 1.7529...  0.0547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11971...  Training loss: 1.7054...  0.0618 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11972...  Training loss: 1.7362...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11973...  Training loss: 1.7077...  0.0563 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11974...  Training loss: 1.6844...  0.0558 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11975...  Training loss: 1.6954...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11976...  Training loss: 1.7733...  0.0539 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11977...  Training loss: 1.7167...  0.0552 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11978...  Training loss: 1.7720...  0.0520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11979...  Training loss: 1.6932...  0.0575 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11980...  Training loss: 1.7355...  0.0550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11981...  Training loss: 1.6759...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11982...  Training loss: 1.6869...  0.0553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11983...  Training loss: 1.7020...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11984...  Training loss: 1.6842...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11985...  Training loss: 1.7028...  0.0523 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11986...  Training loss: 1.6886...  0.0567 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11987...  Training loss: 1.7166...  0.0563 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11988...  Training loss: 1.7189...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11989...  Training loss: 1.6862...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11990...  Training loss: 1.6850...  0.0561 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11991...  Training loss: 1.6928...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11992...  Training loss: 1.7037...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11993...  Training loss: 1.7252...  0.0562 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11994...  Training loss: 1.7424...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11995...  Training loss: 1.7304...  0.0550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11996...  Training loss: 1.7253...  0.0557 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11997...  Training loss: 1.7235...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11998...  Training loss: 1.6603...  0.0522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 11999...  Training loss: 1.7814...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12000...  Training loss: 1.7521...  0.0580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12001...  Training loss: 1.7320...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12002...  Training loss: 1.7335...  0.0523 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12003...  Training loss: 1.7629...  0.0563 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12004...  Training loss: 1.6746...  0.0523 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 12005...  Training loss: 1.6822...  0.0559 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12006...  Training loss: 1.7326...  0.0581 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12007...  Training loss: 1.7421...  0.0522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12008...  Training loss: 1.6749...  0.0585 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12009...  Training loss: 1.7202...  0.0588 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12010...  Training loss: 1.6909...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12011...  Training loss: 1.7724...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12012...  Training loss: 1.6724...  0.0568 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12013...  Training loss: 1.6679...  0.0593 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12014...  Training loss: 1.7043...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12015...  Training loss: 1.6443...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12016...  Training loss: 1.7286...  0.0539 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12017...  Training loss: 1.6884...  0.0600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12018...  Training loss: 1.6443...  0.0584 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12019...  Training loss: 1.6845...  0.0564 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12020...  Training loss: 1.6887...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12021...  Training loss: 1.6475...  0.0590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12022...  Training loss: 1.6540...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12023...  Training loss: 1.6631...  0.0520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12024...  Training loss: 1.6895...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12025...  Training loss: 1.7156...  0.0523 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12026...  Training loss: 1.7140...  0.0531 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12027...  Training loss: 1.7238...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12028...  Training loss: 1.7047...  0.0532 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12029...  Training loss: 1.6654...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12030...  Training loss: 1.6903...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12031...  Training loss: 1.7076...  0.0582 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12032...  Training loss: 1.6693...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12033...  Training loss: 1.7155...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12034...  Training loss: 1.6926...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12035...  Training loss: 1.7618...  0.0555 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12036...  Training loss: 1.6839...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12037...  Training loss: 1.6856...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12038...  Training loss: 1.6692...  0.0575 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12039...  Training loss: 1.6998...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12040...  Training loss: 1.7077...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12041...  Training loss: 1.7253...  0.0584 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12042...  Training loss: 1.6555...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12043...  Training loss: 1.7224...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12044...  Training loss: 1.6895...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12045...  Training loss: 1.7212...  0.0553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12046...  Training loss: 1.6674...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12047...  Training loss: 1.6870...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12048...  Training loss: 1.7067...  0.0547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12049...  Training loss: 1.7006...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12050...  Training loss: 1.6710...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12051...  Training loss: 1.6545...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12052...  Training loss: 1.6967...  0.0554 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12053...  Training loss: 1.6901...  0.0559 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12054...  Training loss: 1.6867...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12055...  Training loss: 1.7093...  0.0552 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12056...  Training loss: 1.7152...  0.0522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12057...  Training loss: 1.8003...  0.0556 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12058...  Training loss: 1.7220...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12059...  Training loss: 1.7499...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12060...  Training loss: 1.7343...  0.0522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12061...  Training loss: 1.7266...  0.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12062...  Training loss: 1.6363...  0.0520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12063...  Training loss: 1.6534...  0.0552 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12064...  Training loss: 1.7243...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12065...  Training loss: 1.6800...  0.0523 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12066...  Training loss: 1.7033...  0.0578 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12067...  Training loss: 1.6876...  0.0522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12068...  Training loss: 1.6940...  0.0552 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12069...  Training loss: 1.7125...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12070...  Training loss: 1.7197...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12071...  Training loss: 1.6721...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12072...  Training loss: 1.7023...  0.0595 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12073...  Training loss: 1.6732...  0.0535 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12074...  Training loss: 1.7116...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12075...  Training loss: 1.6876...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12076...  Training loss: 1.6357...  0.0521 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12077...  Training loss: 1.7213...  0.0565 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12078...  Training loss: 1.7450...  0.0584 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12079...  Training loss: 1.7378...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12080...  Training loss: 1.6735...  0.0610 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12081...  Training loss: 1.7282...  0.0535 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12082...  Training loss: 1.7866...  0.0532 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12083...  Training loss: 1.6601...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12084...  Training loss: 1.7734...  0.0553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12085...  Training loss: 1.6879...  0.0560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12086...  Training loss: 1.6917...  0.0532 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12087...  Training loss: 1.7150...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12088...  Training loss: 1.6820...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12089...  Training loss: 1.6878...  0.0552 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12090...  Training loss: 1.6546...  0.0567 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12091...  Training loss: 1.6427...  0.0579 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12092...  Training loss: 1.6675...  0.0520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12093...  Training loss: 1.6625...  0.0559 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12094...  Training loss: 1.6514...  0.0557 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12095...  Training loss: 1.6661...  0.0585 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12096...  Training loss: 1.7358...  0.0574 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12097...  Training loss: 1.6663...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12098...  Training loss: 1.6481...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12099...  Training loss: 1.6595...  0.0531 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12100...  Training loss: 1.7479...  0.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12101...  Training loss: 1.6960...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12102...  Training loss: 1.6513...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12103...  Training loss: 1.6961...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12104...  Training loss: 1.6962...  0.0528 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 12105...  Training loss: 1.6790...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12106...  Training loss: 1.6916...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12107...  Training loss: 1.6654...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12108...  Training loss: 1.6700...  0.0535 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12109...  Training loss: 1.6985...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12110...  Training loss: 1.6914...  0.0554 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12111...  Training loss: 1.6636...  0.0583 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12112...  Training loss: 1.6842...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12113...  Training loss: 1.6750...  0.0578 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12114...  Training loss: 1.6938...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12115...  Training loss: 1.6806...  0.0572 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12116...  Training loss: 1.6977...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12117...  Training loss: 1.6552...  0.0564 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12118...  Training loss: 1.6697...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12119...  Training loss: 1.6389...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12120...  Training loss: 1.6794...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12121...  Training loss: 1.7102...  0.0593 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12122...  Training loss: 1.6937...  0.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12123...  Training loss: 1.6542...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12124...  Training loss: 1.6717...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12125...  Training loss: 1.6913...  0.0564 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12126...  Training loss: 1.7009...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12127...  Training loss: 1.7266...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12128...  Training loss: 1.6994...  0.0550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12129...  Training loss: 1.6949...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12130...  Training loss: 1.6923...  0.0521 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12131...  Training loss: 1.7241...  0.0577 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12132...  Training loss: 1.7066...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12133...  Training loss: 1.7189...  0.0547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12134...  Training loss: 1.6629...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12135...  Training loss: 1.6702...  0.0531 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12136...  Training loss: 1.7342...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12137...  Training loss: 1.7905...  0.0567 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12138...  Training loss: 1.7437...  0.0552 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12139...  Training loss: 1.6889...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12140...  Training loss: 1.6852...  0.0550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12141...  Training loss: 1.7394...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12142...  Training loss: 1.6574...  0.0553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12143...  Training loss: 1.6571...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12144...  Training loss: 1.6531...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12145...  Training loss: 1.6885...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12146...  Training loss: 1.6750...  0.0580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12147...  Training loss: 1.6774...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12148...  Training loss: 1.7154...  0.0550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12149...  Training loss: 1.7216...  0.0563 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12150...  Training loss: 1.6881...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12151...  Training loss: 1.6952...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12152...  Training loss: 1.7714...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12153...  Training loss: 1.7238...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12154...  Training loss: 1.6908...  0.0573 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12155...  Training loss: 1.6851...  0.0547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12156...  Training loss: 1.6715...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12157...  Training loss: 1.6519...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12158...  Training loss: 1.7652...  0.0521 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12159...  Training loss: 1.7206...  0.0584 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12160...  Training loss: 1.6788...  0.0547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12161...  Training loss: 1.6252...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12162...  Training loss: 1.7640...  0.0580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12163...  Training loss: 1.6709...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12164...  Training loss: 1.6999...  0.0582 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12165...  Training loss: 1.7134...  0.0532 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12166...  Training loss: 1.6021...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12167...  Training loss: 1.6300...  0.0578 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12168...  Training loss: 1.7262...  0.0568 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12169...  Training loss: 1.6742...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12170...  Training loss: 1.6671...  0.0539 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12171...  Training loss: 1.7126...  0.0547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12172...  Training loss: 1.6445...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12173...  Training loss: 1.6696...  0.0555 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12174...  Training loss: 1.7289...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12175...  Training loss: 1.6702...  0.0559 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12176...  Training loss: 1.7210...  0.0559 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12177...  Training loss: 1.6678...  0.0552 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12178...  Training loss: 1.7001...  0.0520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12179...  Training loss: 1.6485...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12180...  Training loss: 1.7270...  0.0609 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12181...  Training loss: 1.7493...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12182...  Training loss: 1.6981...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12183...  Training loss: 1.6714...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12184...  Training loss: 1.7245...  0.0550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12185...  Training loss: 1.7569...  0.0581 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12186...  Training loss: 1.7252...  0.0577 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12187...  Training loss: 1.7360...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12188...  Training loss: 1.7083...  0.0521 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12189...  Training loss: 1.7488...  0.0531 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12190...  Training loss: 1.7334...  0.0596 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12191...  Training loss: 1.6800...  0.0554 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12192...  Training loss: 1.7160...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12193...  Training loss: 1.7383...  0.0547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12194...  Training loss: 1.6896...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12195...  Training loss: 1.6658...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12196...  Training loss: 1.6577...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12197...  Training loss: 1.7015...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12198...  Training loss: 1.7291...  0.0588 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12199...  Training loss: 1.7419...  0.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12200...  Training loss: 1.7018...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12201...  Training loss: 1.6688...  0.0562 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12202...  Training loss: 1.6718...  0.0553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12203...  Training loss: 1.7038...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12204...  Training loss: 1.6495...  0.0526 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 12205...  Training loss: 1.7188...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12206...  Training loss: 1.7313...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12207...  Training loss: 1.6811...  0.0522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12208...  Training loss: 1.7020...  0.0563 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12209...  Training loss: 1.6813...  0.0561 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12210...  Training loss: 1.6398...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12211...  Training loss: 1.6849...  0.0554 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12212...  Training loss: 1.7692...  0.0554 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12213...  Training loss: 1.6929...  0.0576 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12214...  Training loss: 1.6791...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12215...  Training loss: 1.6541...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12216...  Training loss: 1.7045...  0.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12217...  Training loss: 1.6666...  0.0585 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12218...  Training loss: 1.6825...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12219...  Training loss: 1.6854...  0.0559 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12220...  Training loss: 1.6834...  0.0574 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12221...  Training loss: 1.6778...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12222...  Training loss: 1.7015...  0.0523 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12223...  Training loss: 1.7263...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12224...  Training loss: 1.7085...  0.0579 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12225...  Training loss: 1.6108...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12226...  Training loss: 1.6869...  0.0547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12227...  Training loss: 1.6698...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12228...  Training loss: 1.6541...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12229...  Training loss: 1.6915...  0.0587 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12230...  Training loss: 1.7385...  0.0566 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12231...  Training loss: 1.7213...  0.0563 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12232...  Training loss: 1.7119...  0.0618 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12233...  Training loss: 1.6602...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12234...  Training loss: 1.7123...  0.0553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12235...  Training loss: 1.6692...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12236...  Training loss: 1.6814...  0.0560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12237...  Training loss: 1.6972...  0.0581 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12238...  Training loss: 1.6916...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12239...  Training loss: 1.6754...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12240...  Training loss: 1.6983...  0.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12241...  Training loss: 1.6541...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12242...  Training loss: 1.6916...  0.0576 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12243...  Training loss: 1.6520...  0.0580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12244...  Training loss: 1.7004...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12245...  Training loss: 1.7071...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12246...  Training loss: 1.7088...  0.0555 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12247...  Training loss: 1.6917...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12248...  Training loss: 1.6918...  0.0539 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12249...  Training loss: 1.6874...  0.0553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12250...  Training loss: 1.6849...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12251...  Training loss: 1.7022...  0.0579 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12252...  Training loss: 1.7038...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12253...  Training loss: 1.6932...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12254...  Training loss: 1.6446...  0.0562 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12255...  Training loss: 1.7350...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12256...  Training loss: 1.7382...  0.0521 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12257...  Training loss: 1.6885...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12258...  Training loss: 1.7716...  0.0566 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12259...  Training loss: 1.6895...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12260...  Training loss: 1.7798...  0.0579 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12261...  Training loss: 1.6879...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12262...  Training loss: 1.6589...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12263...  Training loss: 1.7295...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12264...  Training loss: 1.7129...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12265...  Training loss: 1.7792...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12266...  Training loss: 1.6878...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12267...  Training loss: 1.7024...  0.0560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12268...  Training loss: 1.7061...  0.0563 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12269...  Training loss: 1.7073...  0.0550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12270...  Training loss: 1.6966...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12271...  Training loss: 1.6805...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12272...  Training loss: 1.7204...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12273...  Training loss: 1.7039...  0.0593 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12274...  Training loss: 1.6720...  0.0519 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12275...  Training loss: 1.6586...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12276...  Training loss: 1.6762...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12277...  Training loss: 1.6832...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12278...  Training loss: 1.7415...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12279...  Training loss: 1.6366...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12280...  Training loss: 1.7099...  0.0550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12281...  Training loss: 1.7020...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12282...  Training loss: 1.7151...  0.0554 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12283...  Training loss: 1.7718...  0.0547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12284...  Training loss: 1.7207...  0.0553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12285...  Training loss: 1.6564...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12286...  Training loss: 1.6604...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12287...  Training loss: 1.7250...  0.0584 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12288...  Training loss: 1.6712...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12289...  Training loss: 1.7501...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12290...  Training loss: 1.7184...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12291...  Training loss: 1.7330...  0.0569 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12292...  Training loss: 1.7173...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12293...  Training loss: 1.7523...  0.0552 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12294...  Training loss: 1.7398...  0.0578 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12295...  Training loss: 1.7138...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12296...  Training loss: 1.6912...  0.0522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12297...  Training loss: 1.7066...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12298...  Training loss: 1.6631...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12299...  Training loss: 1.6684...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12300...  Training loss: 1.6916...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12301...  Training loss: 1.6693...  0.0581 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12302...  Training loss: 1.6468...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12303...  Training loss: 1.6937...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12304...  Training loss: 1.6189...  0.0560 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 12305...  Training loss: 1.7215...  0.0573 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12306...  Training loss: 1.7552...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12307...  Training loss: 1.7166...  0.0555 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12308...  Training loss: 1.7409...  0.0543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12309...  Training loss: 1.7003...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12310...  Training loss: 1.6953...  0.0573 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12311...  Training loss: 1.6668...  0.0547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12312...  Training loss: 1.6954...  0.0571 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12313...  Training loss: 1.6740...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12314...  Training loss: 1.6803...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12315...  Training loss: 1.6988...  0.0557 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12316...  Training loss: 1.6863...  0.0521 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12317...  Training loss: 1.6574...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12318...  Training loss: 1.6949...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12319...  Training loss: 1.6161...  0.0556 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12320...  Training loss: 1.7114...  0.0578 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12321...  Training loss: 1.6914...  0.0553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12322...  Training loss: 1.6690...  0.0554 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12323...  Training loss: 1.6710...  0.0580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12324...  Training loss: 1.6511...  0.0531 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12325...  Training loss: 1.7204...  0.0583 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12326...  Training loss: 1.7008...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12327...  Training loss: 1.7310...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12328...  Training loss: 1.7569...  0.0564 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12329...  Training loss: 1.7448...  0.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12330...  Training loss: 1.6821...  0.0575 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12331...  Training loss: 1.7094...  0.0571 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12332...  Training loss: 1.6463...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12333...  Training loss: 1.7090...  0.0539 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12334...  Training loss: 1.6899...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12335...  Training loss: 1.6691...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12336...  Training loss: 1.6742...  0.0578 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12337...  Training loss: 1.6903...  0.0612 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12338...  Training loss: 1.6376...  0.0588 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12339...  Training loss: 1.6938...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12340...  Training loss: 1.6707...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12341...  Training loss: 1.7485...  0.0523 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12342...  Training loss: 1.7112...  0.0581 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12343...  Training loss: 1.6859...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12344...  Training loss: 1.7720...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12345...  Training loss: 1.7965...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12346...  Training loss: 1.7539...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12347...  Training loss: 1.6928...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12348...  Training loss: 1.7679...  0.0523 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12349...  Training loss: 1.6979...  0.0531 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12350...  Training loss: 1.7258...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12351...  Training loss: 1.7354...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12352...  Training loss: 1.7484...  0.0522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12353...  Training loss: 1.7088...  0.0557 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12354...  Training loss: 1.7013...  0.0553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12355...  Training loss: 1.6923...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12356...  Training loss: 1.7091...  0.0590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12357...  Training loss: 1.6557...  0.0532 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12358...  Training loss: 1.6970...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12359...  Training loss: 1.7428...  0.0542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12360...  Training loss: 1.7183...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12361...  Training loss: 1.7381...  0.0527 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12362...  Training loss: 1.6925...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12363...  Training loss: 1.6778...  0.0519 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12364...  Training loss: 1.7397...  0.0571 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12365...  Training loss: 1.7403...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12366...  Training loss: 1.7142...  0.0522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12367...  Training loss: 1.6702...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12368...  Training loss: 1.7181...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12369...  Training loss: 1.6855...  0.0575 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12370...  Training loss: 1.7289...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12371...  Training loss: 1.6571...  0.0577 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12372...  Training loss: 1.7489...  0.0521 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12373...  Training loss: 1.7043...  0.0535 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12374...  Training loss: 1.6770...  0.0563 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12375...  Training loss: 1.6918...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12376...  Training loss: 1.6583...  0.0563 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12377...  Training loss: 1.6550...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12378...  Training loss: 1.7050...  0.0556 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12379...  Training loss: 1.7108...  0.0523 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12380...  Training loss: 1.7227...  0.0551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12381...  Training loss: 1.6736...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12382...  Training loss: 1.6739...  0.0567 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12383...  Training loss: 1.7028...  0.0521 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12384...  Training loss: 1.6666...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12385...  Training loss: 1.6926...  0.0532 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12386...  Training loss: 1.7307...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12387...  Training loss: 1.6602...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12388...  Training loss: 1.6478...  0.0524 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12389...  Training loss: 1.7086...  0.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12390...  Training loss: 1.7536...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12391...  Training loss: 1.7516...  0.0544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12392...  Training loss: 1.6915...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12393...  Training loss: 1.6489...  0.0526 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12394...  Training loss: 1.6845...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12395...  Training loss: 1.6501...  0.0548 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12396...  Training loss: 1.7106...  0.0546 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12397...  Training loss: 1.7425...  0.0582 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12398...  Training loss: 1.6676...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12399...  Training loss: 1.6673...  0.0525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 12400...  Training loss: 1.6495...  0.0565 sec/batch\n",
      "\n",
      "Training complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelTrainer = RNNModelTrainer()\n",
    "\n",
    "modelTrainer.train_model(model,\n",
    "                         data,\n",
    "                         epochs,\n",
    "                         keep_prob,\n",
    "                         save_every_n,\n",
    "                         max_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i12400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i5000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i5200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i5400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i5600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i5800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i6000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i6200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i6400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i6600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i6800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i7000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i7200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i7400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i7600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i7800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i8000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i8200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i8400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i8600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i8800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i9000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i9200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i9400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i9600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i9800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i10000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i10200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i10400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i10600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i10800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i11000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i11200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i11400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i11600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i11800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i12000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i12200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i12400_l128.ckpt\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \n",
    "    def sample(self, model, data, checkpoint, n_samples, prime=\"The \"):\n",
    "        \"\"\"\n",
    "        Get sample model outputs from checkpoint.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        : model: CharRNNModel object\n",
    "        : data: Dataset\n",
    "        : checkpoint: Checkpoint from which to get samples\n",
    "        : n_samples: Number of samples\n",
    "        : prime: Word to prime sampling\n",
    "        \"\"\"\n",
    "        \n",
    "        samples = [c for c in prime]\n",
    "        num_chars = len(data.chars)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, checkpoint)\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            \n",
    "            for c in prime:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0,0] = data.chars_to_ints[c]\n",
    "                feed = {model.inputs: x,\n",
    "                        model.keep_prob: 1.,\n",
    "                        model.initial_state: new_state}\n",
    "                preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                             feed_dict=feed)\n",
    "                \n",
    "            c = self.pick_top_n(preds, num_chars)\n",
    "            samples.append(data.ints_to_chars[c])\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                x[0,0] = c\n",
    "                feed = {model.inputs: x,\n",
    "                        model.keep_prob: 1.,\n",
    "                        model.initial_state: new_state}\n",
    "                preds, new_state = sess.run([model.prediction, model.final_state],\n",
    "                                             feed_dict=feed)\n",
    "                \n",
    "                c = self.pick_top_n(preds, num_chars)\n",
    "                samples.append(data.ints_to_chars[c])\n",
    "                \n",
    "        return ''.join(samples)\n",
    "    \n",
    "    \n",
    "    def pick_top_n(self, preds, num_chars, top_n=5):\n",
    "        \"\"\"\n",
    "        Pick random char among top_n chars.\n",
    "        \"\"\"\n",
    "        p = np.squeeze(preds)\n",
    "        p[np.argsort(p)[:-top_n]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        c = np.random.choice(num_chars, 1, p=p)[0]\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building CharRNN model ...\n",
      "\n",
      "Created placeholders\n",
      "\n",
      "Built LSTM cell\n",
      "Built LSTM cell\n",
      "Built LSTM layers\n",
      "\n",
      "Built output layer\n",
      "\n",
      "Added training loss computation\n",
      "\n",
      "Built optimizer\n",
      "\n",
      "Built CharRNN model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CharRNNModel(num_classes,\n",
    "                     batch_size,\n",
    "                     num_steps,\n",
    "                     lstm_size,\n",
    "                     num_layers,\n",
    "                     learning_rate,\n",
    "                     grad_clip,\n",
    "                     True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "prime = \"Far\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i1000_l128.ckpt\n",
      "Farsne the her thom, af has sore the wer on anlend ther the tham whathe wet and\n",
      "of that we the sar hor ald on a wers to are wathed wo sansind tore anderitt him ansist as thited and.\n",
      "\n",
      "\"Te the sere sin withe alled and ald, withing to he\n",
      "mase woult and this thor witing as to to and had and he thare the he with ot\n",
      "the wart ot\n",
      "thitt hith thite at houss on\n",
      "hit anden hid, the sead to her woud, the thang, hin womt ware we wans..\n",
      "\n",
      "\n",
      "I thore sore the to we he wers, wan her ald hat seed was he the har sand this, and on he sonsing of of ald ond anl hor, shere to she her and the the sans, alt her. An he tint he sons, thor has wis thor ho hat so couthing thould hererisgiliding and ant and hit heard, thar wis him,\" and\n",
      "and ard on has was and alerer thoung wam an had sind on winh her some he and tarer of woun tho han her shat he wert thar antithing ande theull.\n",
      "\n",
      "\"The that\n",
      "was the this her to his here wo lese he andented he\n",
      "thimhing tare he ther he wound ansed ta wate te shes ther hit watte and the ceentang\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i1000_l128.ckpt\"\n",
    "\n",
    "samp = sampler.sample(model, data, checkpoint, n_samples, prime)\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i10000_l128.ckpt\n",
      "Farr to\n",
      "the sould of\n",
      "her\n",
      "and shillent, at her houre to the persor and has been was to thing and this take her home and a minner.\n",
      "\n",
      "\"Tant the mernate, as you and would so sere and a lart of that all the more is a passe that's not to make.\"\n",
      "\n",
      "\"They's be he would not stall to see, as you husd all her to see.\"\n",
      "\n",
      "She was that the propers hears the ware wonesting of this, and stand that why say and thing, and\n",
      "stor the peare which withohe that so what see his feeting of an and the cauner of time in the compines of\n",
      "the carriathel of him of the some tried of him.\n",
      "\n",
      "\"Yes, he can there and and well. I have some and the thaps on the the peeper,\" she was a sease,\n",
      "though\n",
      "they stond where had\n",
      "house to how with the standing him, when the where her with a pact he said out his forte the stinging that he had tried\n",
      "intorethed\n",
      "take her\n",
      "hid as to him\n",
      "that which she was she children had\n",
      "that the\n",
      "servine with his forgetter to him one to her, but the cartical on the comple one head, as a late alouch of the terrys\n",
      "had \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i10000_l128.ckpt\"\n",
    "\n",
    "samp = sampler.sample(model, data, checkpoint, n_samples, prime)\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i12400_l128.ckpt\n",
      "Farnane to ast was to the were,\"\n",
      "shad he so and here, and which.\n",
      "\n",
      "\"Tell, and she he saw,\" she was and the taking the corner of their heer tarking out of\n",
      "the man han at the\n",
      "still with at the solestly highing\n",
      "thoughtself-to sat, the sairs and ansuneds of the past, and his thought of the mather of the princess of the to attart to herself. \"Whos the sens of meaning to at the conversation would be sented how,\" and that would now so her and to step of an the hompering\n",
      "of the most son of his sent, and her stalls, she without his cancest that she had been said, stronged, he were bettingt and short of the pair. And then the consress off and something\n",
      "that was\n",
      "strought on, what she cannot his collecting of at a corricons, so said, andry. \"You don't be as staying out at the stranger windors in the count oversore women that,\" she said, but the warts, and was to see. Though whith he had\n",
      "taken were aledery that they was not went in his for a constoning. The\n",
      "creckened and\n",
      "ansonering her and and was not s\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "\n",
    "samp = sampler.sample(model, data, checkpoint, n_samples, prime)\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
