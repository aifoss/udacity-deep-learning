{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading, Preprocessing, and Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def load_data(self, data_dir):\n",
    "        \"\"\"\n",
    "        Load data from the data directory.\n",
    "        \"\"\"\n",
    "        input_file = os.path.join(data_dir)\n",
    "        \n",
    "        with open(input_file, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "            data = f.read()\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    " \n",
    "    def extract_character_vocab(self, data):\n",
    "        \"\"\"\n",
    "        Extract vocabulary from the data and create lookup dictionaries.\n",
    "        \"\"\"\n",
    "        special_words = ['<PAD>', '<UNK>', '<GO>', '<EOS>']\n",
    "\n",
    "        word_set = set([character for line in data.split('\\n') for character in line])\n",
    "        int_to_vocab = {word_i: word for word_i, word in enumerate(special_words + list(word_set))}\n",
    "        vocab_to_int = {word: word_i for word_i, word in int_to_vocab.items()}\n",
    "\n",
    "        return (int_to_vocab, vocab_to_int)\n",
    "    \n",
    "    \n",
    "    def convert_characters_to_ids(self, sentences, letter_to_int, addEOS):\n",
    "        \"\"\"\n",
    "        Convert characters in sentences to integers.\n",
    "        \"\"\"\n",
    "        lines = sentences.split('\\n')\n",
    "        \n",
    "        if addEOS == False:\n",
    "            letter_ids = \\\n",
    "                [[letter_to_int.get(letter, letter_to_int['<UNK>']) for letter in line] \\\n",
    "                         for line in lines]\n",
    "        else:\n",
    "            letter_ids = \\\n",
    "                [[letter_to_int.get(letter, letter_to_int['<UNK>']) for letter in line] \\\n",
    "                        + [letter_to_int['<EOS>']] \\\n",
    "                    for line in lines] \n",
    "        \n",
    "        return letter_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataExplorer:\n",
    "    \n",
    "    def explore_sentences(self, sentences):\n",
    "        print(sentences[:50].split('\\n'))\n",
    "        \n",
    "    def explore_letter_ids(self, letter_ids):\n",
    "        print(letter_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PickleHelper:\n",
    "    \n",
    "    def save_preprocessed_data(self, data):\n",
    "        \"\"\"\n",
    "        Save preprocessed training data.\n",
    "        \"\"\"\n",
    "        pickle.dump(data, open('preprocess.p', 'wb'))\n",
    "        \n",
    "    def load_preprocessed_data(self):\n",
    "        \"\"\"\n",
    "        Load the Preprocessed training data and return them in batches of <batch_size> or less.\n",
    "        \"\"\"\n",
    "        return pickle.load(open('preprocess.p', mode='rb'))\n",
    "    \n",
    "    def save_params(self, params):\n",
    "        \"\"\"\n",
    "        Save parameters to file.\n",
    "        \"\"\"\n",
    "        pickle.dump(params, open('params.p', 'wb'))\n",
    "    \n",
    "    def load_params(self):\n",
    "        \"\"\"\n",
    "        Load parameters from file.\n",
    "        \"\"\"\n",
    "        return pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_path = 'data/letters_source.txt'\n",
    "target_path = 'data/letters_target.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataLoader = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_sentences = dataLoader.load_data(source_path)\n",
    "target_sentences = dataLoader.load_data(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataExplorer = DataExplorer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bsaqq', 'npy', 'lbwuj', 'bqv', 'kial', 'tddam', 'edxpjpg', 'nspv', 'huloz', '']\n"
     ]
    }
   ],
   "source": [
    "dataExplorer.explore_sentences(source_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abqqs', 'npy', 'bjluw', 'bqv', 'aikl', 'addmt', 'degjppx', 'npsv', 'hlouz', '']\n"
     ]
    }
   ],
   "source": [
    "dataExplorer.explore_sentences(target_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPreprocessor = DataPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_int_to_letter, source_letter_to_int = \\\n",
    "    dataPreprocessor.extract_character_vocab(source_sentences)\n",
    "\n",
    "target_int_to_letter, target_letter_to_int = \\\n",
    "    dataPreprocessor.extract_character_vocab(target_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_letter_ids = \\\n",
    "    dataPreprocessor.convert_characters_to_ids(source_sentences, source_letter_to_int, False)\n",
    "\n",
    "target_letter_ids = \\\n",
    "    dataPreprocessor.convert_characters_to_ids(target_sentences, target_letter_to_int, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 7, 20, 13, 13], [28, 10, 25], [4, 11, 29, 27, 16]]\n"
     ]
    }
   ],
   "source": [
    "dataExplorer.explore_letter_ids(source_letter_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 11, 13, 13, 7, 3], [28, 10, 25, 3], [11, 17, 4, 27, 29, 3]]\n"
     ]
    }
   ],
   "source": [
    "dataExplorer.explore_letter_ids(target_letter_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickleHelper = PickleHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickleHelper.save_preprocessed_data(\n",
    "    (source_int_to_letter, source_letter_to_int, source_letter_ids,\n",
    "     target_int_to_letter, target_letter_to_int, target_letter_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(source_int_to_letter, source_letter_to_int, source_letter_ids,\n",
    " target_int_to_letter, target_letter_to_int, target_letter_ids) = \\\n",
    "    pickleHelper.load_preprocessed_data()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking TensorFlow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), \\\n",
    "    'Please use TensorFlow version 1.1 or newer'\n",
    "\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Seq2Seq RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.inputs = None\n",
    "        self.targets = None\n",
    "        self.lr = None\n",
    "        \n",
    "        self.source_seq_len = None\n",
    "        self.target_seq_len = None\n",
    "        self.target_max_seq_len = None\n",
    "        \n",
    "        self.encoder_output = None\n",
    "        self.encoder_state = None\n",
    "        \n",
    "        self.decoder_input = None\n",
    "        self.training_decoder_output = None\n",
    "        self.inference_decoder_output = None\n",
    "        \n",
    "        self.cost = None\n",
    "        self.train_op = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNBuilder:\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        \"\"\"\n",
    "        Create placeholders.\n",
    "        \n",
    "        :return: Tuple (inputs, targets, lr, source_seq_len, target_seq_len, target_max_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "        lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        \n",
    "        source_seq_len = tf.placeholder(tf.int32, (None,), name='source_seq_len')\n",
    "        target_seq_len = tf.placeholder(tf.int32, (None,), name='target_seq_len')\n",
    "        target_max_seq_len = tf.reduce_max(target_seq_len, name='target_max_seq_len')\n",
    "        \n",
    "        return (inputs, targets, lr, source_seq_len, target_seq_len, target_max_seq_len)\n",
    "\n",
    "        \n",
    "    def build_encoding_layer(self, \n",
    "                             inputs, \n",
    "                             rnn_size, \n",
    "                             num_layers, \n",
    "                             source_vocab_size, \n",
    "                             source_seq_len,\n",
    "                             enc_embed_size):\n",
    "        \"\"\"\n",
    "        Build the encoding layer.\n",
    "        \n",
    "        :param inputs: Placeholder for inputs\n",
    "        :param rnn_size: RNN size\n",
    "        :param num_layers: Number of RNN layers\n",
    "        :param source_vocab_size: Source vocab size\n",
    "        :param source_seq_len: Source sequence length\n",
    "        :param enc_embed_size: Encoding embedding dimension\n",
    "        :return: Tuple (enc_output, enc_state)\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.variable_scope(\"encode\"):\n",
    "            # Encodder embedding\n",
    "            enc_embed = tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                         source_vocab_size,\n",
    "                                                         enc_embed_size)\n",
    "\n",
    "            # Encoder cell\n",
    "            def make_cell(rnn_size):\n",
    "                initializer = tf.random_uniform_initializer(-0.1, 0.1, seed=2)\n",
    "                dec_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                                   initializer=initializer)\n",
    "                return dec_cell\n",
    "            \n",
    "            enc_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "                [make_cell(rnn_size) for _ in range(num_layers)])\n",
    "\n",
    "            enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, \n",
    "                                                      enc_embed, \n",
    "                                                      sequence_length=source_seq_len,\n",
    "                                                      dtype=tf.float32)\n",
    "\n",
    "            return (enc_output, enc_state)\n",
    " \n",
    "    \n",
    "    def format_decoder_input(self, targets, target_letter_to_int, batch_size):\n",
    "        \"\"\"\n",
    "        Process the input we'll feed to the decoder.\n",
    "        Remove the last word id from each batch \n",
    "        and concatenate <GO> to the beginning of each batch.\n",
    "        \n",
    "        :param targets: Placeholder for targets\n",
    "        :param target_letter_to_int: Mapping of target letters to ints\n",
    "        :param batch_size: Batch size\n",
    "        :return: Input to the decoder\n",
    "        \"\"\"\n",
    "        ending = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "        dec_input = tf.concat([tf.fill([batch_size, 1], target_letter_to_int['<GO>']), ending], 1)        \n",
    "        return dec_input\n",
    "    \n",
    "    \n",
    "    def build_decoding_layer(self,\n",
    "                             rnn_size,\n",
    "                             num_layers,\n",
    "                             batch_size,\n",
    "                             target_vocab_size,\n",
    "                             target_letter_to_int,\n",
    "                             target_seq_len,\n",
    "                             target_max_seq_len,\n",
    "                             enc_state,\n",
    "                             dec_input,\n",
    "                             dec_embed_size):\n",
    "        \"\"\"\n",
    "        Build the decoding layer.\n",
    "        \n",
    "        :param rnn_size: RNN size\n",
    "        :param num_layers: Number of layers\n",
    "        :param batch_size: Batch size\n",
    "        :param target_vocab_size: Target vocabulary size\n",
    "        :param target_letter_to_int: Mapping of target letters to ints\n",
    "        :param target_seq_len: Target sequenge length\n",
    "        :param target_max_seq_len: Max target sequence length\n",
    "        :param enc_state: Encoding layer state\n",
    "        :param dec_input: Input to the decoder\n",
    "        :param dec_embed_size: Decoding embedding dimension\n",
    "        :return Tuple (training_decoder_output, inference_decoder_output)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Decoder embedding\n",
    "        dec_embedding = tf.Variable(\n",
    "            tf.random_uniform([target_vocab_size, dec_embed_size]))\n",
    "        dec_embed = tf.nn.embedding_lookup(dec_embedding, dec_input)\n",
    "\n",
    "        # Decoder cell\n",
    "        def make_cell(rnn_size):\n",
    "            initializer = tf.random_uniform_initializer(-0.1, 0.1, seed=2)\n",
    "            dec_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                               initializer=initializer)\n",
    "            return dec_cell\n",
    "\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "            [make_cell(rnn_size) for _ in range(num_layers)])\n",
    "\n",
    "        # Dense layer to translate the decoder's output at each time step\n",
    "        # into a chocie from the target vocabulary\n",
    "        initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.1)\n",
    "        output_layer = Dense(target_vocab_size,\n",
    "                             kernel_initializer=initializer)\n",
    "\n",
    "        # Training decoder\n",
    "        training_decoder_output = self.build_training_decoder(target_seq_len,\n",
    "                                                              target_max_seq_len,\n",
    "                                                              enc_state, \n",
    "                                                              dec_embed, \n",
    "                                                              dec_cell, \n",
    "                                                              output_layer)\n",
    "\n",
    "        # Inference decoder\n",
    "        inference_decoder_output = self.build_inference_decoder(batch_size,\n",
    "                                                                target_letter_to_int,\n",
    "                                                                target_max_seq_len,\n",
    "                                                                enc_state, \n",
    "                                                                dec_embedding,\n",
    "                                                                dec_cell,\n",
    "                                                                output_layer)\n",
    "\n",
    "        return (training_decoder_output, inference_decoder_output)\n",
    " \n",
    "\n",
    "    def build_training_decoder(self, \n",
    "                               target_seq_len, \n",
    "                               target_max_seq_len,\n",
    "                               enc_state, \n",
    "                               dec_embed, \n",
    "                               dec_cell, \n",
    "                               output_layer):\n",
    "        \"\"\"\n",
    "        Build the training decoder.\n",
    "        \n",
    "        :param target_seq_len: Target sequence length\n",
    "        :param target_max_seq_len: Max target sequence length\n",
    "        :param enc_state: Encoder state\n",
    "        :param dec_embed: Decoder embed input\n",
    "        :param dec_cell: Decoder cell\n",
    "        :param output_layer: Output layer\n",
    "        :return: Output from the training decoder\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.variable_scope(\"decode\"):\n",
    "            helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed,\n",
    "                                                       sequence_length=target_seq_len,\n",
    "                                                       time_major=False)\n",
    "            \n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, enc_state, output_layer)\n",
    "            \n",
    "            return tf.contrib.seq2seq.dynamic_decode(decoder,\n",
    "                                                     impute_finished=True,\n",
    "                                                     maximum_iterations=target_max_seq_len)[0]\n",
    "            \n",
    "       \n",
    "    def build_inference_decoder(self,\n",
    "                                batch_size,\n",
    "                                target_letter_to_int,\n",
    "                                target_max_seq_len,\n",
    "                                enc_state, \n",
    "                                dec_embedding,\n",
    "                                dec_cell,\n",
    "                                output_layer):\n",
    "        \"\"\"\n",
    "        Build the inference decoder.\n",
    "        Reuse the same parameters trained by the training decoder.\n",
    "        \n",
    "        :param batch_size: Batch size\n",
    "        :param target_letter_to_int: Mapping of target letters to ints\n",
    "        :param enc_state: Encoder state\n",
    "        :param dec_embedding: Placeholder for decoder embdding\n",
    "        :param dec_cell: Decoder cell\n",
    "        :param output_layer: Output layer\n",
    "        :return: Output from the inference decoder\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.variable_scope(\"decode\", reuse=True):\n",
    "            start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']], dtype=tf.int32),\n",
    "                                   [batch_size],\n",
    "                                   name='start_tokens')\n",
    "\n",
    "            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embedding,\n",
    "                                                              start_tokens,\n",
    "                                                              target_letter_to_int['<EOS>'])\n",
    "\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, enc_state, output_layer)\n",
    "            \n",
    "            return tf.contrib.seq2seq.dynamic_decode(decoder,\n",
    "                                                     impute_finished=True,\n",
    "                                                     maximum_iterations=target_max_seq_len)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqGraphBuilder:\n",
    "    \n",
    "    def build_train_graph(self, \n",
    "                          rnn_size, \n",
    "                          num_layers,\n",
    "                          batch_size,\n",
    "                          source_vocab_size,\n",
    "                          target_vocab_size,\n",
    "                          target_letter_to_int, \n",
    "                          encoding_embedding_size, \n",
    "                          decoding_embedding_size):\n",
    "        \n",
    "        \"\"\"\n",
    "        Build the training graph hooking up the Seq2Seq model with the optimizer.\n",
    "        \"\"\"\n",
    "        \n",
    "        train_graph = tf.Graph()\n",
    "        \n",
    "        rnn = RNN()\n",
    "        rnnBuilder = RNNBuilder()\n",
    "        optimizerTuner = OptimizerTuner()\n",
    "\n",
    "        # Set the graph to default to ensure that it is ready for training\n",
    "        with train_graph.as_default():\n",
    "            \n",
    "            # Create placeholders\n",
    "            inputs, targets, lr, source_seq_len, target_seq_len, target_max_seq_len = \\\n",
    "                rnnBuilder.create_placeholders()\n",
    "            rnn.inputs, rnn.targets, rnn.lr = inputs, targets, lr\n",
    "            rnn.source_seq_len, rnn.target_seq_len, rnn.target_max_seq_len = \\\n",
    "                source_seq_len, target_seq_len, target_max_seq_len\n",
    "            \n",
    "            # Building the encoding layer\n",
    "            enc_output, enc_state = \\\n",
    "                rnnBuilder.build_encoding_layer(inputs,\n",
    "                                                rnn_size, \n",
    "                                                num_layers, \n",
    "                                                source_vocab_size,\n",
    "                                                source_seq_len,\n",
    "                                                encoding_embedding_size)\n",
    "            rnn.encoder_output, rnn.encoder_state = enc_output, enc_state\n",
    "            \n",
    "            # Format the decoder input\n",
    "            dec_input = rnnBuilder.format_decoder_input(targets, target_letter_to_int, batch_size)\n",
    "            rnn.decoder_input = dec_input\n",
    "            \n",
    "            # Build the decoding layer\n",
    "            training_decoder_output, inference_decoder_output = \\\n",
    "                rnnBuilder.build_decoding_layer(rnn_size,\n",
    "                                                num_layers,\n",
    "                                                batch_size,\n",
    "                                                target_vocab_size,\n",
    "                                                target_letter_to_int,\n",
    "                                                target_seq_len,\n",
    "                                                target_max_seq_len,\n",
    "                                                enc_state,\n",
    "                                                dec_input,\n",
    "                                                decoding_embedding_size)\n",
    "            rnn.training_decoder_output, rnn.inference_decoder_output = \\\n",
    "                training_decoder_output, inference_decoder_output\n",
    "            \n",
    "            # Create tensors for the training logits and inference logits\n",
    "            training_logits = tf.identity(training_decoder_output.rnn_output, name='logits')\n",
    "            inference_logits = tf.identity(inference_decoder_output.sample_id, name='predictions')\n",
    "            \n",
    "            # Create weights for sequence loss\n",
    "            masks = tf.sequence_mask(target_seq_len,\n",
    "                                     target_max_seq_len,\n",
    "                                     dtype=tf.float32,\n",
    "                                     name='masks')\n",
    "\n",
    "            with tf.variable_scope(\"optimization\"):    \n",
    "                # Loss function\n",
    "                cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    " \n",
    "                # Optimizer\n",
    "                optimizer = tf.train.AdamOptimizer(lr)\n",
    "                train_op = optimizerTuner.get_gradient_clipped_optimizer(optimizer, cost)\n",
    "            \n",
    "                rnn.cost, rnn.train_op = cost, train_op\n",
    "                \n",
    "            return rnn, train_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OptimizerTuner:\n",
    "    \n",
    "    def get_gradient_clipped_optimizer(self, optimizer, cost):\n",
    "        \"\"\"\n",
    "        Apply gradient clipping to optimizer.\n",
    "        \n",
    "        :param optimizer: Optimizer to apply gradient clipping to\n",
    "        :param cost: Loss function\n",
    "        :return: Optimizer with gradient clipping\n",
    "        \"\"\"\n",
    "        \n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        \n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) \\\n",
    "                            for grad, var in gradients if grad is not None]\n",
    "        \n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        \n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_size = 50\n",
    "num_layers = 2\n",
    "batch_size = 128\n",
    "source_vocab_size = len(source_letter_to_int)\n",
    "target_vocab_size = len(target_letter_to_int)\n",
    "encoding_embedding_size = 15\n",
    "decoding_embedding_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graphBuilder = Seq2SeqGraphBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn, train_graph = graphBuilder.build_train_graph(rnn_size, \n",
    "                                                  num_layers,\n",
    "                                                  batch_size,\n",
    "                                                  source_vocab_size,\n",
    "                                                  target_vocab_size,\n",
    "                                                  target_letter_to_int, \n",
    "                                                  encoding_embedding_size, \n",
    "                                                  decoding_embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \n",
    "    def train_seq2seq_model(self,\n",
    "                            rnn,\n",
    "                            train_graph, \n",
    "                            epochs, \n",
    "                            learning_rate, \n",
    "                            display_step, \n",
    "                            checkpoint):\n",
    "        \"\"\"\n",
    "        Train the Seq2Seq model.\n",
    "        \n",
    "        :param rnn: Seq2Seq RNN model\n",
    "        :param train_graph: Tensorflow graph\n",
    "        :param epochs: Number of epochs\n",
    "        :param learning_rate: Learning rate\n",
    "        :param display_step: Interval for displaying debug message\n",
    "        :param checkpoint: Location where to save model\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.Session(graph=train_graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            batchGenerator = DataBatchGenerator()\n",
    "            \n",
    "            for epoch_i in range(1, epochs+1):\n",
    "                batches = batchGenerator.get_batches(batch_size,\n",
    "                                                     train_source,\n",
    "                                                     train_target,\n",
    "                                                     source_pad_int,\n",
    "                                                     target_pad_int)\n",
    "                \n",
    "                for batch_i, (source_batch, target_batch, source_lengths, target_lengths) \\\n",
    "                        in enumerate(batches):\n",
    "                    \n",
    "                    # Training step\n",
    "                    feed = {rnn.inputs: source_batch,\n",
    "                            rnn.targets: target_batch,\n",
    "                            rnn.lr: learning_rate,\n",
    "                            rnn.source_seq_len: source_lengths,\n",
    "                            rnn.target_seq_len: target_lengths}\n",
    "                    \n",
    "                    loss, _ = sess.run([rnn.cost, rnn.train_op],\n",
    "                                       feed_dict=feed)\n",
    "                    \n",
    "                    # Debug message\n",
    "                    if batch_i % display_step == 0 and batch_i > 0:\n",
    "                        \n",
    "                        # Calculate validation cost\n",
    "                        feed = {rnn.inputs: valid_source_batch,\n",
    "                                rnn.targets: valid_target_batch,\n",
    "                                rnn.lr: learning_rate,\n",
    "                                rnn.source_seq_len: valid_source_lengths,\n",
    "                                rnn.target_seq_len: valid_target_lengths}\n",
    "                        \n",
    "                        validation_loss = sess.run([rnn.cost],\n",
    "                                                    feed_dict=feed)\n",
    "                        \n",
    "                        print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(train_source) // batch_size, \n",
    "                                  loss, \n",
    "                                  validation_loss[0]))\n",
    "                        \n",
    "            # Save model\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, checkpoint)\n",
    "            \n",
    "            print('\\nModel Trained and Saved\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingValidationSetCreator:\n",
    "    \n",
    "    def create_train_val_sets(self, batch_size, source_letter_ids, target_letter_ids):\n",
    "        \"\"\"\n",
    "        Create training and validation sets.\n",
    "        \n",
    "        :param batch_size: Batch size\n",
    "        :param source_letter_ids: Mapping of source text letters to ints\n",
    "        :param target_letter_ids: Mapping of target text letters to ints\n",
    "        :return Tuple (train_source, train_target, valid_source, valid_target)\n",
    "        \"\"\"\n",
    "        \n",
    "        train_source = source_letter_ids[batch_size:]\n",
    "        train_target = target_letter_ids[batch_size:]\n",
    "        valid_source = source_letter_ids[:batch_size]\n",
    "        valid_target = target_letter_ids[:batch_size]\n",
    "\n",
    "        return (train_source, train_target, valid_source, valid_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValidationSetBatchCreator:\n",
    "    \n",
    "    def get_val_set_batches(self, \n",
    "                            batch_size, \n",
    "                            valid_source, \n",
    "                            valid_target, \n",
    "                            source_pad_int, \n",
    "                            target_pad_int):\n",
    "        \"\"\"\n",
    "        Get batches from validation datasets.\n",
    "\n",
    "        :param batch_size: Batch size\n",
    "        :param valid_source: Validation source dataset\n",
    "        :param valid_target: Validation target dataset\n",
    "        :param source_pad_int: Int ID for <PAD> in source\n",
    "        :param target_pad_int: Int ID for <PAD> in target\n",
    "        :return: Tuple (valid_source_batch, valid_target_batch, \\\n",
    "                        valid_source_lengths, valid_target_lengths)\n",
    "        \"\"\"\n",
    "\n",
    "        dataBatchGenerator = DataBatchGenerator()\n",
    "        \n",
    "        (valid_source_batch, valid_target_batch, \\\n",
    "         valid_source_lengths, valid_target_lengths) = \\\n",
    "            next(dataBatchGenerator.get_batches(batch_size, \n",
    "                                                valid_source, \n",
    "                                                valid_target,\n",
    "                                                source_pad_int,\n",
    "                                                target_pad_int))\n",
    "        \n",
    "        return (valid_source_batch, valid_target_batch, \n",
    "                valid_source_lengths, valid_target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataBatchGenerator:\n",
    "    \n",
    "    def get_batches(self, \n",
    "                    batch_size, \n",
    "                    sources, \n",
    "                    targets, \n",
    "                    source_pad_int, \n",
    "                    target_pad_int):\n",
    "        \"\"\"\n",
    "        Batch targets, sources, and the lengths of their sentences together.\n",
    "        \n",
    "        :param batch_size: Batch size\n",
    "        :param sources: Source dataset\n",
    "        :param targets: Target datasest\n",
    "        :param source_pad_int: Int ID for <PAD> in source\n",
    "        :param target_pad_int: Int ID for <PAD> in target\n",
    "        :return: Batch generator to yield (pad_source_batch, pad_target_batch, \\\n",
    "                                           pad_source_lengths, pad_target_lengths)\n",
    "        \"\"\"\n",
    "        \n",
    "        for batch_i in range(0, len(sources)//batch_size):\n",
    "            start_i = batch_i * batch_size\n",
    "            \n",
    "            source_batch = sources[start_i:start_i + batch_size]\n",
    "            target_batch = targets[start_i:start_i + batch_size]\n",
    "            \n",
    "            pad_source_batch = np.array(\n",
    "                self.pad_sentence_batch(source_batch, source_pad_int))\n",
    "            pad_target_batch = np.array(\n",
    "                self.pad_sentence_batch(target_batch, target_pad_int))\n",
    "\n",
    "            # Need the lengths for the _lengths parameters\n",
    "            pad_source_lengths = []\n",
    "            for source in pad_source_batch:\n",
    "                pad_source_lengths.append(len(source))\n",
    "                \n",
    "            pad_target_lengths = []\n",
    "            for target in pad_target_batch:\n",
    "                pad_target_lengths.append(len(target))\n",
    "\n",
    "            yield pad_source_batch, pad_target_batch, \\\n",
    "                  pad_source_lengths, pad_target_lengths\n",
    "            \n",
    "            \n",
    "    def pad_sentence_batch(self, sentence_batch, pad_int):\n",
    "        \"\"\"\n",
    "        Pad sentences with <PAD> so that each sentence of a batch has the same length.\n",
    "        \n",
    "        :param sentence_batch: Batch of sentences\n",
    "        :param pad_int: Int ID for <PAD>\n",
    "        :return: Batch of sentences padded with <PAD>\n",
    "        \"\"\"\n",
    "        max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "        return [sentence + [pad_int] * (max_sentence - len(sentence)) \\\n",
    "                    for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingValidationSetCreator = TrainingValidationSetCreator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_source, train_target, valid_source, valid_target = \\\n",
    "    trainingValidationSetCreator.create_train_val_sets(batch_size, \n",
    "                                                       source_letter_ids, \n",
    "                                                       target_letter_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_pad_int = source_letter_to_int['<PAD>']\n",
    "target_pad_int = target_letter_to_int['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validationSetBatchCreator = ValidationSetBatchCreator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(valid_source_batch, valid_target_batch, \\\n",
    " valid_source_lengths, valid_target_lengths) = \\\n",
    "    validationSetBatchCreator.get_val_set_batches(\n",
    "        batch_size, valid_source, valid_target, source_pad_int, target_pad_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "learning_rate = 0.001\n",
    "display_step = 20\n",
    "checkpoint = \"best_model.ckpt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelTrainer = ModelTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/60 Batch   20/77 - Loss:  2.372  - Validation loss:  2.416\n",
      "Epoch   1/60 Batch   40/77 - Loss:  2.273  - Validation loss:  2.235\n",
      "Epoch   1/60 Batch   60/77 - Loss:  1.941  - Validation loss:  1.987\n",
      "Epoch   2/60 Batch   20/77 - Loss:  1.644  - Validation loss:  1.724\n",
      "Epoch   2/60 Batch   40/77 - Loss:  1.646  - Validation loss:  1.612\n",
      "Epoch   2/60 Batch   60/77 - Loss:  1.488  - Validation loss:  1.524\n",
      "Epoch   3/60 Batch   20/77 - Loss:  1.371  - Validation loss:  1.440\n",
      "Epoch   3/60 Batch   40/77 - Loss:  1.456  - Validation loss:  1.420\n",
      "Epoch   3/60 Batch   60/77 - Loss:  1.370  - Validation loss:  1.401\n",
      "Epoch   4/60 Batch   20/77 - Loss:  1.288  - Validation loss:  1.360\n",
      "Epoch   4/60 Batch   40/77 - Loss:  1.348  - Validation loss:  1.320\n",
      "Epoch   4/60 Batch   60/77 - Loss:  1.221  - Validation loss:  1.267\n",
      "Epoch   5/60 Batch   20/77 - Loss:  1.145  - Validation loss:  1.202\n",
      "Epoch   5/60 Batch   40/77 - Loss:  1.211  - Validation loss:  1.178\n",
      "Epoch   5/60 Batch   60/77 - Loss:  1.108  - Validation loss:  1.154\n",
      "Epoch   6/60 Batch   20/77 - Loss:  1.064  - Validation loss:  1.115\n",
      "Epoch   6/60 Batch   40/77 - Loss:  1.127  - Validation loss:  1.096\n",
      "Epoch   6/60 Batch   60/77 - Loss:  1.030  - Validation loss:  1.073\n",
      "Epoch   7/60 Batch   20/77 - Loss:  0.986  - Validation loss:  1.035\n",
      "Epoch   7/60 Batch   40/77 - Loss:  1.054  - Validation loss:  1.016\n",
      "Epoch   7/60 Batch   60/77 - Loss:  0.950  - Validation loss:  0.988\n",
      "Epoch   8/60 Batch   20/77 - Loss:  0.860  - Validation loss:  0.911\n",
      "Epoch   8/60 Batch   40/77 - Loss:  0.900  - Validation loss:  0.870\n",
      "Epoch   8/60 Batch   60/77 - Loss:  0.817  - Validation loss:  0.839\n",
      "Epoch   9/60 Batch   20/77 - Loss:  0.742  - Validation loss:  0.793\n",
      "Epoch   9/60 Batch   40/77 - Loss:  0.800  - Validation loss:  0.769\n",
      "Epoch   9/60 Batch   60/77 - Loss:  0.724  - Validation loss:  0.744\n",
      "Epoch  10/60 Batch   20/77 - Loss:  0.656  - Validation loss:  0.701\n",
      "Epoch  10/60 Batch   40/77 - Loss:  0.718  - Validation loss:  0.686\n",
      "Epoch  10/60 Batch   60/77 - Loss:  0.652  - Validation loss:  0.677\n",
      "Epoch  11/60 Batch   20/77 - Loss:  0.587  - Validation loss:  0.634\n",
      "Epoch  11/60 Batch   40/77 - Loss:  0.643  - Validation loss:  0.615\n",
      "Epoch  11/60 Batch   60/77 - Loss:  0.586  - Validation loss:  0.595\n",
      "Epoch  12/60 Batch   20/77 - Loss:  0.523  - Validation loss:  0.569\n",
      "Epoch  12/60 Batch   40/77 - Loss:  0.572  - Validation loss:  0.552\n",
      "Epoch  12/60 Batch   60/77 - Loss:  0.523  - Validation loss:  0.537\n",
      "Epoch  13/60 Batch   20/77 - Loss:  0.469  - Validation loss:  0.510\n",
      "Epoch  13/60 Batch   40/77 - Loss:  0.513  - Validation loss:  0.494\n",
      "Epoch  13/60 Batch   60/77 - Loss:  0.475  - Validation loss:  0.478\n",
      "Epoch  14/60 Batch   20/77 - Loss:  0.425  - Validation loss:  0.457\n",
      "Epoch  14/60 Batch   40/77 - Loss:  0.465  - Validation loss:  0.442\n",
      "Epoch  14/60 Batch   60/77 - Loss:  0.432  - Validation loss:  0.425\n",
      "Epoch  15/60 Batch   20/77 - Loss:  0.378  - Validation loss:  0.407\n",
      "Epoch  15/60 Batch   40/77 - Loss:  0.418  - Validation loss:  0.389\n",
      "Epoch  15/60 Batch   60/77 - Loss:  0.409  - Validation loss:  0.382\n",
      "Epoch  16/60 Batch   20/77 - Loss:  0.330  - Validation loss:  0.354\n",
      "Epoch  16/60 Batch   40/77 - Loss:  0.372  - Validation loss:  0.339\n",
      "Epoch  16/60 Batch   60/77 - Loss:  0.341  - Validation loss:  0.326\n",
      "Epoch  17/60 Batch   20/77 - Loss:  0.285  - Validation loss:  0.305\n",
      "Epoch  17/60 Batch   40/77 - Loss:  0.330  - Validation loss:  0.297\n",
      "Epoch  17/60 Batch   60/77 - Loss:  0.298  - Validation loss:  0.287\n",
      "Epoch  18/60 Batch   20/77 - Loss:  0.246  - Validation loss:  0.266\n",
      "Epoch  18/60 Batch   40/77 - Loss:  0.279  - Validation loss:  0.259\n",
      "Epoch  18/60 Batch   60/77 - Loss:  0.259  - Validation loss:  0.250\n",
      "Epoch  19/60 Batch   20/77 - Loss:  0.207  - Validation loss:  0.231\n",
      "Epoch  19/60 Batch   40/77 - Loss:  0.241  - Validation loss:  0.225\n",
      "Epoch  19/60 Batch   60/77 - Loss:  0.215  - Validation loss:  0.217\n",
      "Epoch  20/60 Batch   20/77 - Loss:  0.182  - Validation loss:  0.205\n",
      "Epoch  20/60 Batch   40/77 - Loss:  0.212  - Validation loss:  0.200\n",
      "Epoch  20/60 Batch   60/77 - Loss:  0.188  - Validation loss:  0.187\n",
      "Epoch  21/60 Batch   20/77 - Loss:  0.154  - Validation loss:  0.176\n",
      "Epoch  21/60 Batch   40/77 - Loss:  0.180  - Validation loss:  0.173\n",
      "Epoch  21/60 Batch   60/77 - Loss:  0.165  - Validation loss:  0.163\n",
      "Epoch  22/60 Batch   20/77 - Loss:  0.137  - Validation loss:  0.157\n",
      "Epoch  22/60 Batch   40/77 - Loss:  0.157  - Validation loss:  0.154\n",
      "Epoch  22/60 Batch   60/77 - Loss:  0.145  - Validation loss:  0.145\n",
      "Epoch  23/60 Batch   20/77 - Loss:  0.122  - Validation loss:  0.141\n",
      "Epoch  23/60 Batch   40/77 - Loss:  0.139  - Validation loss:  0.138\n",
      "Epoch  23/60 Batch   60/77 - Loss:  0.129  - Validation loss:  0.129\n",
      "Epoch  24/60 Batch   20/77 - Loss:  0.107  - Validation loss:  0.125\n",
      "Epoch  24/60 Batch   40/77 - Loss:  0.123  - Validation loss:  0.127\n",
      "Epoch  24/60 Batch   60/77 - Loss:  0.121  - Validation loss:  0.138\n",
      "Epoch  25/60 Batch   20/77 - Loss:  0.112  - Validation loss:  0.130\n",
      "Epoch  25/60 Batch   40/77 - Loss:  0.116  - Validation loss:  0.114\n",
      "Epoch  25/60 Batch   60/77 - Loss:  0.107  - Validation loss:  0.109\n",
      "Epoch  26/60 Batch   20/77 - Loss:  0.088  - Validation loss:  0.104\n",
      "Epoch  26/60 Batch   40/77 - Loss:  0.100  - Validation loss:  0.102\n",
      "Epoch  26/60 Batch   60/77 - Loss:  0.095  - Validation loss:  0.099\n",
      "Epoch  27/60 Batch   20/77 - Loss:  0.078  - Validation loss:  0.095\n",
      "Epoch  27/60 Batch   40/77 - Loss:  0.091  - Validation loss:  0.093\n",
      "Epoch  27/60 Batch   60/77 - Loss:  0.086  - Validation loss:  0.090\n",
      "Epoch  28/60 Batch   20/77 - Loss:  0.071  - Validation loss:  0.087\n",
      "Epoch  28/60 Batch   40/77 - Loss:  0.082  - Validation loss:  0.085\n",
      "Epoch  28/60 Batch   60/77 - Loss:  0.078  - Validation loss:  0.083\n",
      "Epoch  29/60 Batch   20/77 - Loss:  0.064  - Validation loss:  0.080\n",
      "Epoch  29/60 Batch   40/77 - Loss:  0.075  - Validation loss:  0.079\n",
      "Epoch  29/60 Batch   60/77 - Loss:  0.071  - Validation loss:  0.077\n",
      "Epoch  30/60 Batch   20/77 - Loss:  0.059  - Validation loss:  0.074\n",
      "Epoch  30/60 Batch   40/77 - Loss:  0.068  - Validation loss:  0.073\n",
      "Epoch  30/60 Batch   60/77 - Loss:  0.065  - Validation loss:  0.071\n",
      "Epoch  31/60 Batch   20/77 - Loss:  0.053  - Validation loss:  0.069\n",
      "Epoch  31/60 Batch   40/77 - Loss:  0.062  - Validation loss:  0.068\n",
      "Epoch  31/60 Batch   60/77 - Loss:  0.060  - Validation loss:  0.066\n",
      "Epoch  32/60 Batch   20/77 - Loss:  0.049  - Validation loss:  0.064\n",
      "Epoch  32/60 Batch   40/77 - Loss:  0.057  - Validation loss:  0.063\n",
      "Epoch  32/60 Batch   60/77 - Loss:  0.055  - Validation loss:  0.062\n",
      "Epoch  33/60 Batch   20/77 - Loss:  0.044  - Validation loss:  0.060\n",
      "Epoch  33/60 Batch   40/77 - Loss:  0.052  - Validation loss:  0.059\n",
      "Epoch  33/60 Batch   60/77 - Loss:  0.051  - Validation loss:  0.058\n",
      "Epoch  34/60 Batch   20/77 - Loss:  0.040  - Validation loss:  0.056\n",
      "Epoch  34/60 Batch   40/77 - Loss:  0.047  - Validation loss:  0.055\n",
      "Epoch  34/60 Batch   60/77 - Loss:  0.047  - Validation loss:  0.054\n",
      "Epoch  35/60 Batch   20/77 - Loss:  0.037  - Validation loss:  0.052\n",
      "Epoch  35/60 Batch   40/77 - Loss:  0.043  - Validation loss:  0.051\n",
      "Epoch  35/60 Batch   60/77 - Loss:  0.044  - Validation loss:  0.051\n",
      "Epoch  36/60 Batch   20/77 - Loss:  0.034  - Validation loss:  0.049\n",
      "Epoch  36/60 Batch   40/77 - Loss:  0.039  - Validation loss:  0.048\n",
      "Epoch  36/60 Batch   60/77 - Loss:  0.040  - Validation loss:  0.048\n",
      "Epoch  37/60 Batch   20/77 - Loss:  0.031  - Validation loss:  0.046\n",
      "Epoch  37/60 Batch   40/77 - Loss:  0.036  - Validation loss:  0.045\n",
      "Epoch  37/60 Batch   60/77 - Loss:  0.037  - Validation loss:  0.045\n",
      "Epoch  38/60 Batch   20/77 - Loss:  0.028  - Validation loss:  0.043\n",
      "Epoch  38/60 Batch   40/77 - Loss:  0.033  - Validation loss:  0.042\n",
      "Epoch  38/60 Batch   60/77 - Loss:  0.034  - Validation loss:  0.042\n",
      "Epoch  39/60 Batch   20/77 - Loss:  0.026  - Validation loss:  0.041\n",
      "Epoch  39/60 Batch   40/77 - Loss:  0.030  - Validation loss:  0.039\n",
      "Epoch  39/60 Batch   60/77 - Loss:  0.032  - Validation loss:  0.040\n",
      "Epoch  40/60 Batch   20/77 - Loss:  0.023  - Validation loss:  0.039\n",
      "Epoch  40/60 Batch   40/77 - Loss:  0.027  - Validation loss:  0.037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40/60 Batch   60/77 - Loss:  0.029  - Validation loss:  0.037\n",
      "Epoch  41/60 Batch   20/77 - Loss:  0.022  - Validation loss:  0.036\n",
      "Epoch  41/60 Batch   40/77 - Loss:  0.025  - Validation loss:  0.035\n",
      "Epoch  41/60 Batch   60/77 - Loss:  0.027  - Validation loss:  0.035\n",
      "Epoch  42/60 Batch   20/77 - Loss:  0.020  - Validation loss:  0.035\n",
      "Epoch  42/60 Batch   40/77 - Loss:  0.023  - Validation loss:  0.033\n",
      "Epoch  42/60 Batch   60/77 - Loss:  0.025  - Validation loss:  0.033\n",
      "Epoch  43/60 Batch   20/77 - Loss:  0.018  - Validation loss:  0.033\n",
      "Epoch  43/60 Batch   40/77 - Loss:  0.021  - Validation loss:  0.031\n",
      "Epoch  43/60 Batch   60/77 - Loss:  0.023  - Validation loss:  0.031\n",
      "Epoch  44/60 Batch   20/77 - Loss:  0.017  - Validation loss:  0.031\n",
      "Epoch  44/60 Batch   40/77 - Loss:  0.019  - Validation loss:  0.029\n",
      "Epoch  44/60 Batch   60/77 - Loss:  0.022  - Validation loss:  0.029\n",
      "Epoch  45/60 Batch   20/77 - Loss:  0.015  - Validation loss:  0.030\n",
      "Epoch  45/60 Batch   40/77 - Loss:  0.017  - Validation loss:  0.027\n",
      "Epoch  45/60 Batch   60/77 - Loss:  0.020  - Validation loss:  0.027\n",
      "Epoch  46/60 Batch   20/77 - Loss:  0.014  - Validation loss:  0.028\n",
      "Epoch  46/60 Batch   40/77 - Loss:  0.016  - Validation loss:  0.026\n",
      "Epoch  46/60 Batch   60/77 - Loss:  0.019  - Validation loss:  0.025\n",
      "Epoch  47/60 Batch   20/77 - Loss:  0.013  - Validation loss:  0.027\n",
      "Epoch  47/60 Batch   40/77 - Loss:  0.015  - Validation loss:  0.024\n",
      "Epoch  47/60 Batch   60/77 - Loss:  0.018  - Validation loss:  0.024\n",
      "Epoch  48/60 Batch   20/77 - Loss:  0.012  - Validation loss:  0.026\n",
      "Epoch  48/60 Batch   40/77 - Loss:  0.014  - Validation loss:  0.023\n",
      "Epoch  48/60 Batch   60/77 - Loss:  0.016  - Validation loss:  0.022\n",
      "Epoch  49/60 Batch   20/77 - Loss:  0.011  - Validation loss:  0.024\n",
      "Epoch  49/60 Batch   40/77 - Loss:  0.013  - Validation loss:  0.022\n",
      "Epoch  49/60 Batch   60/77 - Loss:  0.015  - Validation loss:  0.021\n",
      "Epoch  50/60 Batch   20/77 - Loss:  0.010  - Validation loss:  0.023\n",
      "Epoch  50/60 Batch   40/77 - Loss:  0.012  - Validation loss:  0.020\n",
      "Epoch  50/60 Batch   60/77 - Loss:  0.014  - Validation loss:  0.020\n",
      "Epoch  51/60 Batch   20/77 - Loss:  0.009  - Validation loss:  0.022\n",
      "Epoch  51/60 Batch   40/77 - Loss:  0.011  - Validation loss:  0.019\n",
      "Epoch  51/60 Batch   60/77 - Loss:  0.013  - Validation loss:  0.019\n",
      "Epoch  52/60 Batch   20/77 - Loss:  0.009  - Validation loss:  0.021\n",
      "Epoch  52/60 Batch   40/77 - Loss:  0.011  - Validation loss:  0.018\n",
      "Epoch  52/60 Batch   60/77 - Loss:  0.012  - Validation loss:  0.018\n",
      "Epoch  53/60 Batch   20/77 - Loss:  0.008  - Validation loss:  0.019\n",
      "Epoch  53/60 Batch   40/77 - Loss:  0.010  - Validation loss:  0.017\n",
      "Epoch  53/60 Batch   60/77 - Loss:  0.011  - Validation loss:  0.018\n",
      "Epoch  54/60 Batch   20/77 - Loss:  0.008  - Validation loss:  0.018\n",
      "Epoch  54/60 Batch   40/77 - Loss:  0.009  - Validation loss:  0.017\n",
      "Epoch  54/60 Batch   60/77 - Loss:  0.010  - Validation loss:  0.018\n",
      "Epoch  55/60 Batch   20/77 - Loss:  0.007  - Validation loss:  0.018\n",
      "Epoch  55/60 Batch   40/77 - Loss:  0.008  - Validation loss:  0.016\n",
      "Epoch  55/60 Batch   60/77 - Loss:  0.010  - Validation loss:  0.017\n",
      "Epoch  56/60 Batch   20/77 - Loss:  0.006  - Validation loss:  0.018\n",
      "Epoch  56/60 Batch   40/77 - Loss:  0.008  - Validation loss:  0.015\n",
      "Epoch  56/60 Batch   60/77 - Loss:  0.009  - Validation loss:  0.016\n",
      "Epoch  57/60 Batch   20/77 - Loss:  0.006  - Validation loss:  0.016\n",
      "Epoch  57/60 Batch   40/77 - Loss:  0.007  - Validation loss:  0.014\n",
      "Epoch  57/60 Batch   60/77 - Loss:  0.008  - Validation loss:  0.015\n",
      "Epoch  58/60 Batch   20/77 - Loss:  0.005  - Validation loss:  0.015\n",
      "Epoch  58/60 Batch   40/77 - Loss:  0.007  - Validation loss:  0.013\n",
      "Epoch  58/60 Batch   60/77 - Loss:  0.008  - Validation loss:  0.014\n",
      "Epoch  59/60 Batch   20/77 - Loss:  0.005  - Validation loss:  0.014\n",
      "Epoch  59/60 Batch   40/77 - Loss:  0.007  - Validation loss:  0.013\n",
      "Epoch  59/60 Batch   60/77 - Loss:  0.007  - Validation loss:  0.013\n",
      "Epoch  60/60 Batch   20/77 - Loss:  0.005  - Validation loss:  0.014\n",
      "Epoch  60/60 Batch   40/77 - Loss:  0.006  - Validation loss:  0.012\n",
      "Epoch  60/60 Batch   60/77 - Loss:  0.007  - Validation loss:  0.012\n",
      "\n",
      "Model Trained and Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelTrainer.train_seq2seq_model(rnn,\n",
    "                                 train_graph, \n",
    "                                 epochs, \n",
    "                                 learning_rate, \n",
    "                                 display_step, \n",
    "                                 checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PredictionChecker:\n",
    "    \n",
    "    def check_prediction(self, \n",
    "                         checkpoint, \n",
    "                         text,\n",
    "                         source_letter_to_int,\n",
    "                         target_letter_to_int):\n",
    "        \"\"\"\n",
    "        Check sample predictions from the trained model.\n",
    "        \n",
    "        :param checkpoint: Checkpoint where the model is saved\n",
    "        :param text: Input text\n",
    "        :param source_letter_to_int: Mapping of source letters to ints\n",
    "        :param target_letter_to_int: mapping of target letters to ints\n",
    "        \"\"\"\n",
    "        \n",
    "        loaded_graph = tf.Graph()\n",
    "        \n",
    "        answer_logits = self.get_answer_logits(text, loaded_graph, checkpoint)\n",
    "        \n",
    "        self.print_prediction(answer_logits, text, source_letter_to_int, target_letter_to_int)\n",
    "        \n",
    "    \n",
    "    def get_answer_logits(self, text, loaded_graph, checkpoint):\n",
    "        \"\"\"\n",
    "        Run trainded model with given input text.\n",
    "        \n",
    "        :param text: Input text\n",
    "        :param loaded_graph: Loaded TensorFlow graph of the trained model\n",
    "        :param checkpoint: Checkpoint name\n",
    "        :return: Logits from running the model\n",
    "        \"\"\"\n",
    "    \n",
    "        with tf.Session(graph=loaded_graph) as sess:\n",
    "            # Load saved model\n",
    "            loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "            loader.restore(sess, checkpoint)\n",
    "            \n",
    "            # Load tensors\n",
    "            inputs = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "            logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "            source_seq_len = loaded_graph.get_tensor_by_name('source_seq_len:0')\n",
    "            target_seq_len = loaded_graph.get_tensor_by_name('target_seq_len:0')\n",
    "            \n",
    "            # Multiply by batch_size to match the model's input parameters\n",
    "            feed = {inputs: [text]*batch_size,\n",
    "                    source_seq_len: [len(text)]*batch_size,\n",
    "                    target_seq_len: [len(text)]*batch_size}\n",
    "            \n",
    "            answer_logits = sess.run(logits,\n",
    "                                     feed_dict=feed)[0]\n",
    "            \n",
    "            return answer_logits\n",
    "\n",
    "    \n",
    "    def print_prediction(self, answer_logits, text, source_letter_to_int, target_letter_to_int):\n",
    "        \"\"\"\n",
    "        Print prediction.\n",
    "        \n",
    "        :param answer_logits: Logits from the model\n",
    "        :param text: Input text\n",
    "        :param source_letter_to_int: Mapping of source letters to ints\n",
    "        :param target_letter_to_int: mapping of target letters to ints\n",
    "        \"\"\"\n",
    "    \n",
    "        pad = source_letter_to_int[\"<PAD>\"] \n",
    "\n",
    "        print('Original Text:', input_sentence)\n",
    "\n",
    "        print('\\nSource')\n",
    "        print('  Word Ids:    {}'.format([i for i in text]))\n",
    "        print('  Input Words: {}'.format(\n",
    "            \" \".join([source_int_to_letter[i] for i in text])))\n",
    "\n",
    "        print('\\nTarget')\n",
    "        print('  Word Ids:       {}'.format(\n",
    "            [i for i in answer_logits if i != pad]))\n",
    "        print('  Response Words: {}'.format(\n",
    "            \" \".join([target_int_to_letter[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InputSentencePreparer:\n",
    "\n",
    "    def source_to_seq(self, text, source_letter_to_int):\n",
    "        \"\"\"\n",
    "        Prepare the text for the model.\n",
    "        \n",
    "        :param: Input text\n",
    "        :param source_letter_to_int: Mapping of source letters to ints\n",
    "        :return: Prepared input sentence to feed to the model\n",
    "        \"\"\"\n",
    "        seq_len = 7\n",
    "        return [source_letter_to_int.get(word, source_letter_to_int['<UNK>']) \\\n",
    "                for word in text]+ [source_letter_to_int['<PAD>']]*(seq_len-len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sentence = 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputSentencePreparer = InputSentencePreparer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = inputSentencePreparer.source_to_seq(input_sentence, source_letter_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionChecker = PredictionChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from best_model.ckpt\n",
      "Original Text: hello\n",
      "\n",
      "Source\n",
      "  Word Ids:    [24, 21, 4, 4, 14, 0, 0]\n",
      "  Input Words: h e l l o <PAD> <PAD>\n",
      "\n",
      "Target\n",
      "  Word Ids:       [21, 24, 4, 4, 14, 3]\n",
      "  Response Words: e h l l o <EOS>\n"
     ]
    }
   ],
   "source": [
    "predictionChecker.check_prediction(checkpoint, \n",
    "                                   text,\n",
    "                                   source_letter_to_int,\n",
    "                                   target_letter_to_int)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
